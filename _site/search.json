[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Ong Zhi Rong Jordan",
    "section": "",
    "text": "Currently a full-time servicemen, assuming an appointment in Training & Doctrine Command (TRADOC)."
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "Ong Zhi Rong Jordan",
    "section": "Education",
    "text": "Education\nNanyang Technological University - NTU | Singapore Bachelor of Engineering in Computer Engineering | Aug 2013 - Aug 2016\nSingapore Management University - SMU | Singapore Master of IT in Business (MITB)| Jan 2022 - Nov 2022 (Ongoing)"
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "Ong Zhi Rong Jordan",
    "section": "Experience",
    "text": "Experience\nNil"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Bringing Data to Life",
    "section": "",
    "text": "Computation of spatial weights using R. Understanding the spatial relationships that exist among the features in the dataset.\n\n\n\ngeospatial\n\n\nsf\n\n\nspdep\n\n\ntmap\n\n\n\n\nOng Zhi Rong Jordan\n\n\nNovember 24, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUtilising the different libraries such as ggplot, tmap and leaflet to visualise geographical data.\n\n\n\ngeospatial\n\n\nsf\n\n\nggplot\n\n\ntmap\n\n\nleaflet\n\n\n\n\nOng Zhi Rong Jordan\n\n\nNovember 23, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUtilising the sf and tidyverse packages to tidy geospatial data.\n\n\n\ngeospatial\n\n\nsf\n\n\n\n\nOng Zhi Rong Jordan\n\n\nNovember 19, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLeveraging on simple data wrangling techniques to create a RFM model. Subsequently, leveraging on unsupervised classification to conduct customer segmentation for targeted marketing.\n\n\n\ntibble\n\n\nclustering\n\n\nunsupervised\n\n\n\n\nOng Zhi Rong Jordan\n\n\nJune 25, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUsing Static, Interactive and Statistical (SIS) Graphs to reveal the demographics and relationships of a city.\n\n\n\ntibble\n\n\nggstatsplot\n\n\n\n\nOng Zhi Rong Jordan\n\n\nJune 8, 2022\n\n\n\n\n\n\n\n\nNo matching items\n\n\n  \n\n\n See all"
  },
  {
    "objectID": "index.html#posts-of-my-own-works",
    "href": "index.html#posts-of-my-own-works",
    "title": "Bringing Data to Life",
    "section": "Posts of my own works",
    "text": "Posts of my own works\n\n\n\n\n  \n\n\n\n\nRFM Model\n\n\nLeveraging on simple data wrangling techniques to create a RFM model. Subsequently, leveraging on unsupervised classification to conduct customer segmentation for targeted marketing.\n\n\n\n\ntibble\n\n\nclustering\n\n\nunsupervised\n\n\n \n\n\n\n\nJune 25, 2022\n\n\nOng Zhi Rong Jordan\n\n\n\n\n\n\n  \n\n\n\n\nSIS Visual Representation\n\n\nUsing Static, Interactive and Statistical (SIS) Graphs to reveal the demographics and relationships of a city.\n\n\n\n\ntibble\n\n\nggstatsplot\n\n\n \n\n\n\n\nJune 8, 2022\n\n\nOng Zhi Rong Jordan\n\n\n\n\n\n\nNo matching items\n\n\n See all posts of my work"
  },
  {
    "objectID": "index.html#posts-from-geospatial-analytics",
    "href": "index.html#posts-from-geospatial-analytics",
    "title": "Bringing Data to Life",
    "section": "Posts from Geospatial Analytics",
    "text": "Posts from Geospatial Analytics\n\n\n\n\n  \n\n\n\n\nIntroduction to Spatial Weights and Application\n\n\nComputation of spatial weights using R. Understanding the spatial relationships that exist among the features in the dataset.\n\n\n\n\ngeospatial\n\n\nsf\n\n\nspdep\n\n\ntmap\n\n\n \n\n\n\n\nNovember 24, 2022\n\n\nOng Zhi Rong Jordan\n\n\n\n\n\n\n  \n\n\n\n\nIntroduction to Choropleth Mapping\n\n\nUtilising the different libraries such as ggplot, tmap and leaflet to visualise geographical data.\n\n\n\n\ngeospatial\n\n\nsf\n\n\nggplot\n\n\ntmap\n\n\nleaflet\n\n\n \n\n\n\n\nNovember 23, 2022\n\n\nOng Zhi Rong Jordan\n\n\n\n\n\n\n  \n\n\n\n\nData Wrangling of Geospatial Data\n\n\nUtilising the sf and tidyverse packages to tidy geospatial data.\n\n\n\n\ngeospatial\n\n\nsf\n\n\n \n\n\n\n\nNovember 19, 2022\n\n\nOng Zhi Rong Jordan\n\n\n\n\n\n\nNo matching items\n\n\n See all posts of Geospatial"
  },
  {
    "objectID": "index.html#posts-from-visual-analytics",
    "href": "index.html#posts-from-visual-analytics",
    "title": "Bringing Data to Life",
    "section": "Posts from Visual Analytics",
    "text": "Posts from Visual Analytics\n\n\n\n\n\nNo matching items\n\n\n See all posts of Visual"
  },
  {
    "objectID": "posts/Geo/Geospatial_HOE1/index.html",
    "href": "posts/Geo/Geospatial_HOE1/index.html",
    "title": "Introduction to Choropleth Mapping",
    "section": "",
    "text": "For this analysis, we will use the following packages from CRAN.\nsf - Support for simple features, a standardized way to encode spatial vector data. Binds to ‘GDAL’ for reading and writing data, to ‘GEOS’ for geometrical operations, and to ‘PROJ’ for projection conversions and datum transformations. Uses by default the ‘s2’ package for spherical geometry operations on ellipsoidal (long/lat) coordinates. tidyverse - Loading the core tidyverse packages which will be used for data wrangling and visualisation. tmap - Thematic maps are geographical maps in which spatial data distributions are visualized. This package offers a flexible, layer-based, and easy to use approach to create thematic maps, such as choropleths and bubble maps.\n\npacman::p_load(sf, tidyverse, tmap)"
  },
  {
    "objectID": "posts/Geo/Geospatial_HOE1/index.html#dataset",
    "href": "posts/Geo/Geospatial_HOE1/index.html#dataset",
    "title": "Introduction to Choropleth Mapping",
    "section": "Dataset",
    "text": "Dataset\nFor this analysis, we will extract data that is available from the web.\n\nMaster Plan 2014 Subzone Boundary (Web) from data.gov.sg\nPre-Schools Location from data.gov.sg\nCycling Path from LTADataMall\nLatest version of Singapore Airbnb listing data from Inside Airbnb\n\nExtracting geographical information from Dataset\nWe will leverage on the st_read function to retrieve polygon, line and point feature in both ESRI shapefile and KML format.\n\nmpsz = st_read(dsn = \"data/geospatial\", \n                  layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `C:\\jordanong09\\ISSS624_Geospatial\\posts\\Geo\\Geospatial_HOE1\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\ncyclingpath = st_read(dsn = \"data/geospatial\", \n                      layer = \"CyclingPath\")\n\nReading layer `CyclingPath' from data source \n  `C:\\jordanong09\\ISSS624_Geospatial\\posts\\Geo\\Geospatial_HOE1\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 1625 features and 2 fields\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: 12711.19 ymin: 28711.33 xmax: 42626.09 ymax: 48948.15\nProjected CRS: SVY21\n\npreschool = st_read(\"data/geospatial/pre-schools-location-kml.kml\")\n\nReading layer `PRESCHOOLS_LOCATION' from data source \n  `C:\\jordanong09\\ISSS624_Geospatial\\posts\\Geo\\Geospatial_HOE1\\data\\geospatial\\pre-schools-location-kml.kml' \n  using driver `KML'\nSimple feature collection with 1359 features and 2 fields\nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 103.6824 ymin: 1.248403 xmax: 103.9897 ymax: 1.462134\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84\n\nst_geometry(mpsz)\n\nGeometry set for 323 features \nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\nFirst 5 geometries:\n\nglimpse(mpsz)\n\nRows: 323\nColumns: 16\n$ OBJECTID   <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, …\n$ SUBZONE_NO <int> 1, 1, 3, 8, 3, 7, 9, 2, 13, 7, 12, 6, 1, 5, 1, 1, 3, 2, 2, …\n$ SUBZONE_N  <chr> \"MARINA SOUTH\", \"PEARL'S HILL\", \"BOAT QUAY\", \"HENDERSON HIL…\n$ SUBZONE_C  <chr> \"MSSZ01\", \"OTSZ01\", \"SRSZ03\", \"BMSZ08\", \"BMSZ03\", \"BMSZ07\",…\n$ CA_IND     <chr> \"Y\", \"Y\", \"Y\", \"N\", \"N\", \"N\", \"N\", \"Y\", \"N\", \"N\", \"N\", \"N\",…\n$ PLN_AREA_N <chr> \"MARINA SOUTH\", \"OUTRAM\", \"SINGAPORE RIVER\", \"BUKIT MERAH\",…\n$ PLN_AREA_C <chr> \"MS\", \"OT\", \"SR\", \"BM\", \"BM\", \"BM\", \"BM\", \"SR\", \"QT\", \"QT\",…\n$ REGION_N   <chr> \"CENTRAL REGION\", \"CENTRAL REGION\", \"CENTRAL REGION\", \"CENT…\n$ REGION_C   <chr> \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\",…\n$ INC_CRC    <chr> \"5ED7EB253F99252E\", \"8C7149B9EB32EEFC\", \"C35FEFF02B13E0E5\",…\n$ FMEL_UPD_D <date> 2014-12-05, 2014-12-05, 2014-12-05, 2014-12-05, 2014-12-05…\n$ X_ADDR     <dbl> 31595.84, 28679.06, 29654.96, 26782.83, 26201.96, 25358.82,…\n$ Y_ADDR     <dbl> 29220.19, 29782.05, 29974.66, 29933.77, 30005.70, 29991.38,…\n$ SHAPE_Leng <dbl> 5267.381, 3506.107, 1740.926, 3313.625, 2825.594, 4428.913,…\n$ SHAPE_Area <dbl> 1630379.27, 559816.25, 160807.50, 595428.89, 387429.44, 103…\n$ geometry   <MULTIPOLYGON [m]> MULTIPOLYGON (((31495.56 30..., MULTIPOLYGON (…\n\nhead(mpsz, n=5)  \n\nSimple feature collection with 5 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 25867.68 ymin: 28369.47 xmax: 32362.39 ymax: 30435.54\nProjected CRS: SVY21\n  OBJECTID SUBZONE_NO      SUBZONE_N SUBZONE_C CA_IND      PLN_AREA_N\n1        1          1   MARINA SOUTH    MSSZ01      Y    MARINA SOUTH\n2        2          1   PEARL'S HILL    OTSZ01      Y          OUTRAM\n3        3          3      BOAT QUAY    SRSZ03      Y SINGAPORE RIVER\n4        4          8 HENDERSON HILL    BMSZ08      N     BUKIT MERAH\n5        5          3        REDHILL    BMSZ03      N     BUKIT MERAH\n  PLN_AREA_C       REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR\n1         MS CENTRAL REGION       CR 5ED7EB253F99252E 2014-12-05 31595.84\n2         OT CENTRAL REGION       CR 8C7149B9EB32EEFC 2014-12-05 28679.06\n3         SR CENTRAL REGION       CR C35FEFF02B13E0E5 2014-12-05 29654.96\n4         BM CENTRAL REGION       CR 3775D82C5DDBEFBD 2014-12-05 26782.83\n5         BM CENTRAL REGION       CR 85D9ABEF0A40678F 2014-12-05 26201.96\n    Y_ADDR SHAPE_Leng SHAPE_Area                       geometry\n1 29220.19   5267.381  1630379.3 MULTIPOLYGON (((31495.56 30...\n2 29782.05   3506.107   559816.2 MULTIPOLYGON (((29092.28 30...\n3 29974.66   1740.926   160807.5 MULTIPOLYGON (((29932.33 29...\n4 29933.77   3313.625   595428.9 MULTIPOLYGON (((27131.28 30...\n5 30005.70   2825.594   387429.4 MULTIPOLYGON (((26451.03 30..."
  },
  {
    "objectID": "posts/Geo/Geospatial_HOE1/index.html#plotting-of-map",
    "href": "posts/Geo/Geospatial_HOE1/index.html#plotting-of-map",
    "title": "Introduction to Choropleth Mapping",
    "section": "Plotting of map",
    "text": "Plotting of map\n\nplot(mpsz)\n\n\n\nplot(st_geometry(mpsz))\n\n\n\nplot(mpsz[\"PLN_AREA_N\"])"
  },
  {
    "objectID": "posts/Geo/Geospatial_HOE1/index.html#changing-of-projection",
    "href": "posts/Geo/Geospatial_HOE1/index.html#changing-of-projection",
    "title": "Introduction to Choropleth Mapping",
    "section": "Changing of Projection",
    "text": "Changing of Projection\n\nmpsz3414 <- st_set_crs(mpsz, 3414)\n\npreschool3414 <- st_transform(preschool, \n                              crs = 3414)"
  },
  {
    "objectID": "posts/Geo/Geospatial_HOE1/index.html#importing-and-converting-an-aspatial-data",
    "href": "posts/Geo/Geospatial_HOE1/index.html#importing-and-converting-an-aspatial-data",
    "title": "Introduction to Choropleth Mapping",
    "section": "Importing and Converting An Aspatial Data",
    "text": "Importing and Converting An Aspatial Data\n\nlistings <- read_csv(\"data/aspatial/listings.csv\")\nlist(listings) \n\n[[1]]\n# A tibble: 4,252 × 16\n       id name         host_id host_name neighbourhood_g… neighbourhood latitude\n    <dbl> <chr>          <dbl> <chr>     <chr>            <chr>            <dbl>\n 1  50646 Pleasant Ro…  227796 Sujatha   Central Region   Bukit Timah       1.33\n 2  71609 Ensuite Roo…  367042 Belinda   East Region      Tampines          1.35\n 3  71896 B&B  Room 1…  367042 Belinda   East Region      Tampines          1.35\n 4  71903 Room 2-near…  367042 Belinda   East Region      Tampines          1.35\n 5 275343 Convenientl… 1439258 Joyce     Central Region   Bukit Merah       1.29\n 6 275344 15 mins to … 1439258 Joyce     Central Region   Bukit Merah       1.29\n 7 294281 5 mins walk… 1521514 Elizabeth Central Region   Newton            1.31\n 8 301247 Nice room w… 1552002 Rahul     Central Region   Geylang           1.32\n 9 324945 20 Mins to … 1439258 Joyce     Central Region   Bukit Merah       1.29\n10 330089 Accomo@ RED… 1439258 Joyce     Central Region   Bukit Merah       1.29\n# … with 4,242 more rows, and 9 more variables: longitude <dbl>,\n#   room_type <chr>, price <dbl>, minimum_nights <dbl>,\n#   number_of_reviews <dbl>, last_review <date>, reviews_per_month <dbl>,\n#   calculated_host_listings_count <dbl>, availability_365 <dbl>\n\n\n\nlistings_sf <- st_as_sf(listings, \n                       coords = c(\"longitude\", \"latitude\"),\n                       crs=4326) %>%\n  st_transform(crs = 3414)\n\nglimpse(listings_sf)\n\nRows: 4,252\nColumns: 15\n$ id                             <dbl> 50646, 71609, 71896, 71903, 275343, 275…\n$ name                           <chr> \"Pleasant Room along Bukit Timah\", \"Ens…\n$ host_id                        <dbl> 227796, 367042, 367042, 367042, 1439258…\n$ host_name                      <chr> \"Sujatha\", \"Belinda\", \"Belinda\", \"Belin…\n$ neighbourhood_group            <chr> \"Central Region\", \"East Region\", \"East …\n$ neighbourhood                  <chr> \"Bukit Timah\", \"Tampines\", \"Tampines\", …\n$ room_type                      <chr> \"Private room\", \"Private room\", \"Privat…\n$ price                          <dbl> 80, 178, 81, 81, 52, 40, 72, 41, 49, 49…\n$ minimum_nights                 <dbl> 90, 90, 90, 90, 14, 14, 90, 8, 14, 14, …\n$ number_of_reviews              <dbl> 18, 20, 24, 48, 20, 13, 133, 105, 14, 1…\n$ last_review                    <date> 2014-07-08, 2019-12-28, 2014-12-10, 20…\n$ reviews_per_month              <dbl> 0.22, 0.28, 0.33, 0.67, 0.20, 0.16, 1.2…\n$ calculated_host_listings_count <dbl> 1, 4, 4, 4, 50, 50, 7, 1, 50, 50, 50, 4…\n$ availability_365               <dbl> 365, 365, 365, 365, 353, 364, 365, 90, …\n$ geometry                       <POINT [m]> POINT (22646.02 35167.9), POINT (…\n\n\n\nbuffer_cycling <- st_buffer(cyclingpath, \n                               dist=5, nQuadSegs = 30)\n\nbuffer_cycling$AREA <- st_area(buffer_cycling)\n\nsum(buffer_cycling$AREA)\n\n773143.9 [m^2]\n\n\n\nmpsz3414$`PreSch Count`<- lengths(st_intersects(mpsz3414, preschool3414))\n\nsummary(mpsz3414$`PreSch Count`)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  0.000   0.000   2.000   4.207   6.000  37.000 \n\ntop_n(mpsz3414, 1, `PreSch Count`)\n\nSimple feature collection with 1 feature and 16 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 23449.05 ymin: 46001.23 xmax: 25594.22 ymax: 47996.47\nProjected CRS: SVY21 / Singapore TM\n  OBJECTID SUBZONE_NO      SUBZONE_N SUBZONE_C CA_IND PLN_AREA_N PLN_AREA_C\n1      290          3 WOODLANDS EAST    WDSZ03      N  WOODLANDS         WD\n      REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR   Y_ADDR\n1 NORTH REGION       NR C90769E43EE6B0F2 2014-12-05 24506.64 46991.63\n  SHAPE_Leng SHAPE_Area                       geometry PreSch Count\n1   6603.608    2553464 MULTIPOLYGON (((24786.75 46...           37"
  },
  {
    "objectID": "posts/Geo/Geospatial_HOE1/index.html#plotting-of-geographical-data",
    "href": "posts/Geo/Geospatial_HOE1/index.html#plotting-of-geographical-data",
    "title": "Introduction to Choropleth Mapping",
    "section": "Plotting of geographical data",
    "text": "Plotting of geographical data\n\nmpsz3414$Area <- mpsz3414 %>%\n  st_area()\n\nmpsz3414 <- mpsz3414 %>%\n  mutate(`PreSch Density` = `PreSch Count`/Area * 1000000)\n\nhist(mpsz3414$`PreSch Density`)"
  },
  {
    "objectID": "posts/Geo/Geospatial_HOE1/index.html#visualising-of-geographical-data-using-ggplot2",
    "href": "posts/Geo/Geospatial_HOE1/index.html#visualising-of-geographical-data-using-ggplot2",
    "title": "Introduction to Choropleth Mapping",
    "section": "Visualising of geographical data using ggplot2",
    "text": "Visualising of geographical data using ggplot2\n\nggplot(data=mpsz3414, \n       aes(x= as.numeric(`PreSch Density`)))+\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"light blue\") +\n  labs(title = \"Are pre-school even distributed in Singapore?\",\n       subtitle= \"There are many planning sub-zones with a single pre-school, on the other hand, \\nthere are two planning sub-zones with at least 20 pre-schools\",\n      x = \"Pre-school density (per km sq)\",\n      y = \"Frequency\")\n\n\n\n\n\nggplot(data=mpsz3414, \n       aes(x= as.numeric(`PreSch Density`), y = as.numeric(`PreSch Count`)))+\n  geom_point() +\n  labs(title = \"Are pre-school even distributed in Singapore?\",\n      x = \"Pre-school density (per km sq)\",\n      y = \"Pre-school Count\")"
  },
  {
    "objectID": "posts/Geo/Geospatial_HOE1/index.html#choropleth-mapping-with-r",
    "href": "posts/Geo/Geospatial_HOE1/index.html#choropleth-mapping-with-r",
    "title": "Introduction to Choropleth Mapping",
    "section": "Choropleth Mapping with R",
    "text": "Choropleth Mapping with R\n\npopdata <- read_csv(\"data/aspatial/respopagesextod2011to2020.csv\")\n\n\npopdata2020 <- popdata %>%\n  filter(Time == 2020) %>%\n  group_by(PA, SZ, AG) %>%\n  summarise(`POP` = sum(`Pop`)) %>%\n  ungroup()%>%\n  pivot_wider(names_from=AG, \n              values_from=POP) %>%\n  mutate(YOUNG = rowSums(.[3:6])\n         +rowSums(.[12])) %>%\nmutate(`ECONOMY ACTIVE` = rowSums(.[7:11])+\nrowSums(.[13:15]))%>%\nmutate(`AGED`=rowSums(.[16:21])) %>%\nmutate(`TOTAL`=rowSums(.[3:21])) %>%  \nmutate(`DEPENDENCY` = (`YOUNG` + `AGED`)\n/`ECONOMY ACTIVE`) %>%\n  select(`PA`, `SZ`, `YOUNG`, \n       `ECONOMY ACTIVE`, `AGED`, \n       `TOTAL`, `DEPENDENCY`)\n\n\npopdata2020 <- popdata2020 %>%\n  mutate_at(.vars = vars(PA, SZ), \n          .funs = funs(toupper)) %>%\n  filter(`ECONOMY ACTIVE` > 0)\n\nmpsz_pop2020 <- left_join(mpsz, popdata2020,\n                          by = c(\"SUBZONE_N\" = \"SZ\"))\n\nwrite_rds(mpsz_pop2020, \"data/rds/mpszpop2020.rds\")\n\n\ntmap_mode(\"plot\")\nqtm(mpsz_pop2020, \n    fill = \"DEPENDENCY\")\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"Dependency ratio\") +\n  tm_layout(main.title = \"Distribution of Dependency Ratio by planning subzone\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar() +\n  tm_grid(alpha =0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\\n and Population data from Department of Statistics DOS\", \n             position = c(\"left\", \"bottom\"))\n\n\n\n\n\ntm_shape(mpsz_pop2020) +\n  tm_polygons()\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\")\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\") +\n  tm_borders(lwd = 0.1,  alpha = 1)\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"jenks\") +\n  tm_borders(alpha = 0.5)\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"equal\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\nDIY using quantile and 4 classes.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 4,\n          style = \"quantile\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\nsummary(mpsz_pop2020$DEPENDENCY)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n 0.0000  0.6519  0.7025  0.7742  0.7645 19.0000      92 \n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          breaks = c(0, 0.60, 0.70, 0.80, 0.90, 1.00)) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 6,\n          style = \"quantile\",\n          palette = \"Blues\") +\n  tm_borders(alpha = 0.5)\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          style = \"quantile\",\n          palette = \"-Greens\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"jenks\", \n          palette = \"Blues\", \n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(main.title = \"Distribution of Dependency Ratio by planning subzone \\n(Jenks classification)\",\n            main.title.position = \"center\",\n            main.title.size = 1,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            legend.outside = FALSE,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = FALSE) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"-Greens\") +\n  tm_borders(alpha = 0.5) +\n  tmap_style(\"classic\")\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"No. of persons\") +\n  tm_layout(main.title = \"Distribution of Dependency Ratio \\nby planning subzone\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar(width = 0.15) +\n  tm_grid(lwd = 0.1, alpha = 0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\\n and Population data from Department of Statistics DOS\", \n             position = c(\"left\", \"bottom\")) +\n  tmap_style(\"white\")\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(c(\"YOUNG\", \"AGED\"),\n          style = \"equal\", \n          palette = \"Blues\") +\n  tm_layout(legend.position = c(\"right\", \"bottom\")) +\n  tm_borders(alpha = 0.5) +\n  tmap_style(\"white\")\n\n\n\ntm_shape(mpsz_pop2020)+ \n  tm_polygons(c(\"DEPENDENCY\",\"AGED\"),\n          style = c(\"equal\", \"quantile\"), \n          palette = list(\"Blues\",\"Greens\")) +\n  tm_layout(legend.position = c(\"right\", \"bottom\"))\n\n\n\n\n\ntm_shape(mpsz_pop2020) +\n  tm_fill(\"DEPENDENCY\",\n          style = \"quantile\",\n          palette = \"Blues\",\n          thres.poly = 0) + \n  tm_facets(by=\"REGION_N\", \n            free.coords=TRUE, \n            drop.shapes=TRUE) +\n  tm_layout(legend.show = FALSE,\n            title.position = c(\"center\", \"center\"), \n            title.size = 20) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\nyoungmap <- tm_shape(mpsz_pop2020)+ \n  tm_polygons(\"YOUNG\", \n              style = \"quantile\", \n              palette = \"Blues\")\n\nagedmap <- tm_shape(mpsz_pop2020)+ \n  tm_polygons(\"AGED\", \n              style = \"quantile\", \n              palette = \"Blues\")\n\ntmap_arrange(youngmap, agedmap, asp=1, ncol=2)\n\n\n\n\n\ntm_shape(mpsz_pop2020[mpsz_pop2020$REGION_N==\"CENTRAL REGION\", ])+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\", \n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(legend.outside = TRUE,\n            legend.height = 0.45, \n            legend.width = 5.0,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = FALSE) +\n  tm_borders(alpha = 0.5)"
  },
  {
    "objectID": "posts/geo.html",
    "href": "posts/geo.html",
    "title": "Jordan OZR",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\n\nComputation of spatial weights using R. Understanding the spatial relationships that exist among the features in the dataset.\n\n\n\n\ngeospatial\n\n\nsf\n\n\nspdep\n\n\ntmap\n\n\n \n\n\n\n\nNovember 24, 2022\n\n\nOng Zhi Rong Jordan\n\n\n\n\n\n\n  \n\n\n\n\n\nUtilising the different libraries such as ggplot, tmap and leaflet to visualise geographical data.\n\n\n\n\ngeospatial\n\n\nsf\n\n\nggplot\n\n\ntmap\n\n\nleaflet\n\n\n \n\n\n\n\nNovember 23, 2022\n\n\nOng Zhi Rong Jordan\n\n\n\n\n\n\n  \n\n\n\n\n\nUtilising the sf and tidyverse packages to tidy geospatial data.\n\n\n\n\ngeospatial\n\n\nsf\n\n\n \n\n\n\n\nNovember 19, 2022\n\n\nOng Zhi Rong Jordan\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/index.html",
    "href": "posts/index.html",
    "title": "Jordan OZR",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n\nComputation of spatial weights using R. Understanding the spatial relationships that exist among the features in the dataset.\n\n\n\ngeospatial\n\n\nsf\n\n\nspdep\n\n\ntmap\n\n\n\n\nOng Zhi Rong Jordan\n\n\nNovember 24, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUtilising the different libraries such as ggplot, tmap and leaflet to visualise geographical data.\n\n\n\ngeospatial\n\n\nsf\n\n\nggplot\n\n\ntmap\n\n\nleaflet\n\n\n\n\nOng Zhi Rong Jordan\n\n\nNovember 23, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUtilising the sf and tidyverse packages to tidy geospatial data.\n\n\n\ngeospatial\n\n\nsf\n\n\n\n\nOng Zhi Rong Jordan\n\n\nNovember 19, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLeveraging on simple data wrangling techniques to create a RFM model. Subsequently, leveraging on unsupervised classification to conduct customer segmentation for targeted marketing.\n\n\n\ntibble\n\n\nclustering\n\n\nunsupervised\n\n\n\n\nOng Zhi Rong Jordan\n\n\nJune 25, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUsing Static, Interactive and Statistical (SIS) Graphs to reveal the demographics and relationships of a city.\n\n\n\ntibble\n\n\nggstatsplot\n\n\n\n\nOng Zhi Rong Jordan\n\n\nJune 8, 2022\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Me/RFM/index.html",
    "href": "posts/Me/RFM/index.html",
    "title": "RFM Model",
    "section": "",
    "text": "The RFM model has become essential for businesses to identify high value customers and possible churn customers to conduct targeted marketing. Businesses have leverage RFM model to better understand customer behaviours and also calculate Customer Life Time Value (LTV). This could also translate to better budgeting for marketing cost using the (3:1) ratio of LTV:CAC. In this article, I will demonstrate how we can leverage on existing libraries to conduct unsupervised classification and lastly potential future works to enhance the model.\n\nknitr::include_graphics(\"RFM.png\")"
  },
  {
    "objectID": "posts/Me/RFM/index.html#libraries",
    "href": "posts/Me/RFM/index.html#libraries",
    "title": "RFM Model",
    "section": "Libraries",
    "text": "Libraries\nFor this analysis, we will use the following packages from CRAN.\ncluster - Methods for Cluster analysis. Much extended the original from Peter Rousseeuw, Anja Struyf and Mia Hubert, based on Kaufman and Rousseeuw (1990) “Finding Groups in Data”.tidyverse - Loading the core tidyverse packages which will be used for data wrangling and visualisation.factoextra - Extract and Visualize the Results of Multivariate Data Analyses. GGally Extension of ggplot2 by adding several functions to reduce the complexity of combining geometric objects with transformed data.\n\npacman::p_load(cluster, tidyverse, factoextra,lubridate,patchwork, GGally, moments,bestNormalize) #refer to 1st post to understand the usage of pacman"
  },
  {
    "objectID": "posts/Me/RFM/index.html#data-set",
    "href": "posts/Me/RFM/index.html#data-set",
    "title": "RFM Model",
    "section": "Data Set",
    "text": "Data Set\n\nWe will use a customer data set that consist of 6 columns.\n\nCustomer_ID: Identification of Customer\nCategoryGroup: Category group of the item purchased\nCategory: Category of the item purchased\nInvoiceDate: The date of purchased\nQuantity: The number of items purchased\nTotalPrice: The total amount spend on that item"
  },
  {
    "objectID": "posts/Me/RFM/index.html#data-wrangling",
    "href": "posts/Me/RFM/index.html#data-wrangling",
    "title": "RFM Model",
    "section": "Data Wrangling",
    "text": "Data Wrangling\n\ncustomer <- readRDS(\"data/customer.rds\")\n\nLet’s examine the data!\nFrom the summary, we can identify a few potential problems!\n\nCustomer_ID is in numeric not character. # I prefer IDs to be in character form since it is for representation of customer instead of number of customers.\n\nInvoiceDate is not in date time format!\n\nTotalPrice is not in numeric (the symbol was attached to the number)\n\n\nsummary (customer)\n\n  Customer_ID    CategoryGroup        Category         InvoiceDate       \n Min.   :12348   Length:395888      Length:395888      Length:395888     \n 1st Qu.:14132   Class :character   Class :character   Class :character  \n Median :15535   Mode  :character   Mode  :character   Mode  :character  \n Mean   :15462                                                           \n 3rd Qu.:16841                                                           \n Max.   :18287                                                           \n    Quantity        TotalPrice       \n Min.   :   1.00   Length:395888     \n 1st Qu.:   2.00   Class :character  \n Median :   4.00   Mode  :character  \n Mean   :   8.29                     \n 3rd Qu.:  12.00                     \n Max.   :1500.00                     \n\nhead(customer$TotalPrice)\n\n[1] \"£8\" \"£2\" \"£1\" \"£3\" \"£3\" \"£5\"\n\n\nChange of data class\nFor the date time format, we will leverage on lubridate functions to convert our exisiting date to date time format. Since the format is Month/Day/Year, we will use the function mdy. For TotalPrice, there are two symbols found, £ and ,. We will use the gsub function and replace all symbols to an empty space. Lastly, using as.numeric to convert it to a numeric class. For CustomerID, simply use as.character to convert it to character class.\n\ncustomer$InvoiceDate <- mdy(customer$InvoiceDate)\ncustomer$TotalPrice <- as.numeric(gsub(\"[£]|[,]\",\"\",customer$TotalPrice, perl=TRUE))\ncustomer$Customer_ID <- as.character(customer$Customer_ID)\n\nExtracting Recency, Frequency and Monetary\nRecency\nTo extract how recent the customer purchase an item from the store, we will use the last InvoiceDate to substract all the dates a customer purchase from the store and retrieve the minimum number. Since the format of Recency will be in datetime format, we will convert it using the as.numeric function.\n\ncustomer_recency <- customer %>%\n  mutate(recency = (max(InvoiceDate) + 1) - InvoiceDate) %>%\n  group_by(Customer_ID) %>%\n  summarise (Recency = as.numeric(min(recency)))\n\nFrequency\nTo extract how frequent the customer purchase an item from the store, we will use the n() function to find out how many different dates the customer visited the store.\n\ncustomer_frequency <- customer %>%\n  group_by(Customer_ID,InvoiceDate) %>%\n  summarise (count = n()) %>%\n  ungroup() %>%\n  group_by (Customer_ID) %>%\n  summarise (Frequency = n()) %>%\n  ungroup()\n\n\ncustomer_monetary <- customer %>%\n  group_by(Customer_ID) %>%\n  summarise (Monetary = sum(TotalPrice))\n\n\ncustomer_RFM <- customer_recency %>%\n  left_join (customer_frequency, by = \"Customer_ID\") %>%\n  left_join (customer_monetary, by = \"Customer_ID\")\n\nExamining the distribution of the RFM model\n\n# Histogram overlaid with kernel density curve\nrdplot <- ggplot(customer_RFM, aes(x=Recency)) + \n    geom_histogram(aes(y=..density..),      # Histogram with density instead of count on y-axis\n                   binwidth=10,\n                   colour=\"black\", fill=\"white\") +\n    geom_density(alpha=.2, fill=\"#ff9285\") + \n  ylab(\"Density\") +\n  theme_classic() +\n  theme(axis.title.y= element_text(angle=0), axis.ticks.x= element_blank(),\n        axis.line= element_line(color= 'grey'))\n\nfdplot <- ggplot(customer_RFM, aes(x=Frequency)) + \n    geom_histogram(aes(y=..density..),      # Histogram with density instead of count on y-axis\n                   binwidth=2,\n                   colour=\"black\", fill=\"white\") +\n    geom_density(alpha=.2, fill=\"#906efa\") + \n  ylab(\"Density\") +\n  theme_classic() +\n  theme(axis.title.y= element_text(angle=0), axis.ticks.x= element_blank(),\n        axis.line= element_line(color= 'grey'))\n\nmdplot <- ggplot(customer_RFM, aes(x=Monetary)) + \n    geom_histogram(aes(y=..density..),      # Histogram with density instead of count on y-axis\n                   binwidth=250,\n                   colour=\"black\", fill=\"white\") +\n    geom_density(alpha=.2, fill=\"#d18500\") + \n  ylab(\"Density\") +\n  theme_classic() +\n  theme(axis.title.y= element_text(angle=0), axis.ticks.x= element_blank(),\n        axis.line= element_line(color= 'grey'))\n\n\nrdplot + fdplot + mdplot\n\n\n\n\nThrough the skewness and the histogrm, we can conclude that the attributes does not conform to normal distribution. Since all three attributes does not conform to a normal distribution and K-means would perform better with a normal distributed data, we will conduct data transformation. Utilizing the bestNormalise library, we can identify which normalization techniques best suits each attributes based on their distribution.\n\nskewness(customer_RFM$Recency)\n\n[1] 0.7240294\n\nskewness(customer_RFM$Frequency)\n\n[1] 3.781094\n\nskewness(customer_RFM$Monetary)\n\n[1] 1.435198\n\n\nWe can\n\nbestNormalize(customer_RFM$Recency)\n\nBest Normalizing transformation with 5010 Observations\n Estimated Normality Statistics (Pearson P / df, lower => more normal):\n - arcsinh(x): 11.1308\n - Box-Cox: 8.203\n - Center+scale: 28.4903\n - Exp(x): 22.7727\n - Log_b(x+a): 11.1712\n - orderNorm (ORQ): 1.1137\n - sqrt(x + a): 10.3142\n - Yeo-Johnson: 8.3977\nEstimation method: Out-of-sample via CV with 10 folds and 5 repeats\n \nBased off these, bestNormalize chose:\norderNorm Transformation with 5010 nonmissing obs and ties\n - 552 unique values \n - Original quantiles:\n  0%  25%  50%  75% 100% \n   1   32  121  382  676 \n\nbestNormalize(customer_RFM$Frequency)\n\nBest Normalizing transformation with 5010 Observations\n Estimated Normality Statistics (Pearson P / df, lower => more normal):\n - arcsinh(x): 82.7576\n - Box-Cox: 83.2959\n - Center+scale: 82.3867\n - Exp(x): 74.932\n - Log_b(x+a): 82.7547\n - orderNorm (ORQ): 82.2905\n - sqrt(x + a): 82.732\n - Yeo-Johnson: 83.5991\nEstimation method: Out-of-sample via CV with 10 folds and 5 repeats\n \nBased off these, bestNormalize chose:\nStandardized exp(x) Transformation with 5010 nonmissing obs.:\n Relevant statistics:\n - mean (before standardization) = 2.741283e+28 \n - sd (before standardization) = 1.940317e+30 \n\nbestNormalize(customer_RFM$Monetary)\n\nBest Normalizing transformation with 5010 Observations\n Estimated Normality Statistics (Pearson P / df, lower => more normal):\n - arcsinh(x): 2.4536\n - Box-Cox: 1.7519\n - Center+scale: 20.535\n - Log_b(x+a): 2.4542\n - orderNorm (ORQ): 1.1424\n - sqrt(x + a): 5.4938\n - Yeo-Johnson: 1.7558\nEstimation method: Out-of-sample via CV with 10 folds and 5 repeats\n \nBased off these, bestNormalize chose:\norderNorm Transformation with 5010 nonmissing obs and ties\n - 2312 unique values \n - Original quantiles:\n     0%     25%     50%     75%    100% \n   4.00  313.25  713.50 1555.00 5004.00 \n\n\n\ncustomer_RFM_dt <- customer_RFM %>%\n  select(Recency, Frequency, Monetary)\n\nRecency <- orderNorm(customer_RFM_dt$Recency)\nFrequency <- boxcox (customer_RFM_dt$Frequency)\nMonetary <- orderNorm(customer_RFM_dt$Monetary)\n\ncustomer_RFM_dt$Recency <- Recency$x.t\ncustomer_RFM_dt$Frequency <- Frequency$x.t\ncustomer_RFM_dt$Monetary <- Monetary$x.t\n\nskewness(customer_RFM_dt$Recency)\n\n[1] 0.01507482\n\nskewness(customer_RFM_dt$Frequency)\n\n[1] 0.08897177\n\nskewness(customer_RFM_dt$Monetary)\n\n[1] 0.0006851529\n\n\n\n# Histogram overlaid with kernel density curve\nrdplot_dt <- ggplot(customer_RFM_dt, aes(x=Recency)) + \n    geom_histogram(aes(y=..density..),      # Histogram with density instead of count on y-axis\n                   colour=\"black\", fill=\"white\") +\n    geom_density(alpha=.2, fill=\"#ff9285\") + \n  ylab(\"Density\") +\n  theme_classic() +\n  theme(axis.title.y= element_text(angle=0), axis.ticks.x= element_blank(),\n        axis.line= element_line(color= 'grey'))\n\nfdplot_dt <- ggplot(customer_RFM_dt, aes(x=Frequency)) + \n    geom_histogram(aes(y=..density..),      # Histogram with density instead of count on y-axis\n                   colour=\"black\", fill=\"white\") +\n    geom_density(alpha=.2, fill=\"#906efa\") + \n  ylab(\"Density\") +\n  theme_classic() +\n  theme(axis.title.y= element_text(angle=0), axis.ticks.x= element_blank(),\n        axis.line= element_line(color= 'grey'))\n\nmdplot_dt <- ggplot(customer_RFM_dt, aes(x=Monetary)) + \n    geom_histogram(aes(y=..density..),      # Histogram with density instead of count on y-axis\n                   colour=\"black\", fill=\"white\") +\n    geom_density(alpha=.2, fill=\"#d18500\") + \n  ylab(\"Density\") +\n  theme_classic() +\n  theme(axis.title.y= element_text(angle=0), axis.ticks.x= element_blank(),\n        axis.line= element_line(color= 'grey'))\n\n\nrdplot_dt + fdplot_dt + mdplot_dt\n\n\n\n\n\ncustomer_RFM_cluster <- customer_RFM_dt %>%\n  select(Recency, Frequency, Monetary)\n\nK-means Clustering\nTo identify the optimal number of clusters using K means clustering, we will use the fviz_nbclust function and the silhouette and wss. Based on the silhouette score, the optimal cluster is 2 while the WSS score shows either 2 or 3. We will now explore both cluster size.\n\nset.seed(1234)\n\nfviz_nbclust(customer_RFM_cluster, kmeans, method = \"silhouette\")\n\n\n\nfviz_nbclust(customer_RFM_cluster, kmeans, method = \"wss\")\n\n\n\n\nInsights from Cluster\n\nkm_cluster2 <- kmeans(customer_RFM_cluster, \n                     2, \n                     nstart = 25)\n\n\n\nkm_cluster3 <- kmeans(customer_RFM_cluster, \n                     3, \n                     nstart = 25)\n\ncustomer_RFM$km_cluster2 <- as.character(km_cluster2$cluster)\n\ncustomer_RFM$km_cluster3 <- as.character(km_cluster3$cluster)\n\nFrom the table, we can identify that cluster 1 consist of customers on average made a purchase within 94 days, frequent the store 5 times and spend 1.7k. Whereas for cluster 2, the customers recency period on average is about 325 days, frequent on average 1 time and spend about $392. We can say that cluster 1 consist of our high value customers and cluster 2 consist of potential churn customers.\n\ncustomer_RFM %>%\n  group_by(km_cluster2) %>%\n  summarise(mean_recency = mean(Recency),\n            mean_frequency = mean(Frequency),\n            mean_monetary = mean(Monetary),\n            members = n()) \n\n# A tibble: 2 × 5\n  km_cluster2 mean_recency mean_frequency mean_monetary members\n  <chr>              <dbl>          <dbl>         <dbl>   <int>\n1 1                   94.1           5.45         1774.    2595\n2 2                  326.            1.39          393.    2415\n\ncustomer_RFM %>%\n  group_by(km_cluster3) %>%\n  summarise(mean_recency = mean(Recency),\n            mean_frequency = mean(Frequency),\n            mean_monetary = mean(Monetary),\n            members = n()) \n\n# A tibble: 3 × 5\n  km_cluster3 mean_recency mean_frequency mean_monetary members\n  <chr>              <dbl>          <dbl>         <dbl>   <int>\n1 1                  364.            1.11          295.    1681\n2 2                   54.6           7.32         2431.    1391\n3 3                  177.            2.80          865.    1938\n\n\nTo better visualise the distribution of our customers based on their cluster, we will leverage on the ggparcoord to visualise the distribution using a parallel coordinates plot.\n\n# Plot\nggparcoord(customer_RFM,\n    columns = 2:4, groupColumn = 5,\n    showPoints = TRUE,\n    scale=\"uniminmax\",\n    title = \"Parallel Coordinate Plot for the Customer Data\",\n    alphaLines = 0.3\n    ) + \n  theme_classic()+\n  theme(\n    plot.title = element_text(size=10)\n  )  + scale_color_brewer(palette = \"Set2\") + \n  guides(color=guide_legend(title=\"Cluster\"))"
  },
  {
    "objectID": "posts/Me/SIS Representation/index.html",
    "href": "posts/Me/SIS Representation/index.html",
    "title": "SIS Visual Representation",
    "section": "",
    "text": "In this article, I will share how we can leverage on Static, Interactive and Statistical (SIS) graphs to conduct appropriate data visualisation and draw statistical conclusion from the data set. In this article, we will explore varios libraries such as parsetR, ggstatsplot and ggplot."
  },
  {
    "objectID": "posts/Me/SIS Representation/index.html#libraries",
    "href": "posts/Me/SIS Representation/index.html#libraries",
    "title": "SIS Visual Representation",
    "section": "Libraries",
    "text": "Libraries\nInstead of using the base R function such as library() or install.packages(),we will use the p_load function from the pacman package that combine these functions together. Before using the package, you will need to install the package from CRAN.\n\ninstall.packages(\"pacman\")\n\nFor this analysis, we will use the following packages from CRAN.\nparsetR - Visualize your data with interactive d3.js parallel sets with the power and convenience of an htmlwidget.tidyverse - Loading the core tidyverse packages which will be used for data wrangling and visualisation.ggstatsplot - An extension of ggplot2 package for creating graphics with details from statistical tests included in the information-rich plots themselves.patchwork - Combine separate ggplots into the same graphic.\n\npacman::p_load(parsetR, tidyverse, ggstatsplot, patchwork, hrbrthemes)"
  },
  {
    "objectID": "posts/Me/SIS Representation/index.html#data-set",
    "href": "posts/Me/SIS Representation/index.html#data-set",
    "title": "SIS Visual Representation",
    "section": "Data Set",
    "text": "Data Set\n\nTwo different data set for this analysis:\n\n\nParticipants.csv - Information of all participants.\n\nFinancialJournal.csv- Input of the participant’s wages and expenses."
  },
  {
    "objectID": "posts/Me/SIS Representation/index.html#data-wrangling",
    "href": "posts/Me/SIS Representation/index.html#data-wrangling",
    "title": "SIS Visual Representation",
    "section": "Data Wrangling",
    "text": "Data Wrangling\n\nknitr::include_graphics(\"qn1_concept.png\")\n\n\n\n\n\n\n\n\nparticipants <- read_csv(\"rawdata/Participants.csv\")\nfinance <- read_csv(\"rawdata/FinancialJournal.csv\")\n\nReducing of File Size uploading to Git\nTo reduce the requirement to upload the original data set, I will use the saveRDS function to convert my working tibble dataframe to a R data format namely .rds. We will subsequently use the readRDS function to read the data files in R.\n\nsaveRDS(participants, \"participants.rds\")\nsaveRDS(finance, \"finance.rds\")\n\n\nparticipants <- readRDS(\"data/participants.rds\")\nfinance <- readRDS(\"data/finance.rds\")"
  },
  {
    "objectID": "posts/Me/SIS Representation/index.html#data-preparation",
    "href": "posts/Me/SIS Representation/index.html#data-preparation",
    "title": "SIS Visual Representation",
    "section": "Data Preparation",
    "text": "Data Preparation\nThrough the data from the participants, we can identify a total of 1011 participants ad 6 different attributes. The finance data shows the timestamp of the participants log and a category column. It seems like the data is in the long format and therefore we will subsequently pivot the data table to a wide format. We can also see that household size should be a categorical data rather than a numerical data. We address these issues using the dplyr package.\n\nsummary(participants)\n\n participantId    householdSize    haveKids            age       \n Min.   :   0.0   Min.   :1.000   Mode :logical   Min.   :18.00  \n 1st Qu.: 252.5   1st Qu.:1.000   FALSE:710       1st Qu.:29.00  \n Median : 505.0   Median :2.000   TRUE :301       Median :39.00  \n Mean   : 505.0   Mean   :1.964                   Mean   :39.07  \n 3rd Qu.: 757.5   3rd Qu.:3.000                   3rd Qu.:50.00  \n Max.   :1010.0   Max.   :3.000                   Max.   :60.00  \n educationLevel     interestGroup        joviality       \n Length:1011        Length:1011        Min.   :0.000204  \n Class :character   Class :character   1st Qu.:0.240074  \n Mode  :character   Mode  :character   Median :0.477539  \n                                       Mean   :0.493794  \n                                       3rd Qu.:0.746819  \n                                       Max.   :0.999234  \n\nsummary(finance)\n\n participantId      timestamp                        category        \n Min.   :   0.0   Min.   :2022-03-01 00:00:00.00   Length:1856330    \n 1st Qu.: 222.0   1st Qu.:2022-06-14 12:30:00.00   Class :character  \n Median : 464.0   Median :2022-10-06 16:20:00.00   Mode  :character  \n Mean   : 480.8   Mean   :2022-10-07 12:36:41.13                     \n 3rd Qu.: 726.0   3rd Qu.:2023-01-29 19:10:00.00                     \n Max.   :1010.0   Max.   :2023-05-25 00:05:00.00                     \n     amount         \n Min.   :-1562.726  \n 1st Qu.:   -5.594  \n Median :   -4.000  \n Mean   :   19.922  \n 3rd Qu.:   22.856  \n Max.   : 4096.526  \n\n\nAs part of Data Preparation, I prefer to ensure my columns are well worded. This would reduce the need to reword the X and Y axis subsequently for all the plots.\n\nparticipants <- participants %>%\n  rename('Participant Id' = 'participantId', \n         'Household Size' = 'householdSize', \n         'Have Kids' = 'haveKids', \n         'Age' = 'age', \n         'Education Level' = 'educationLevel', \n         'Interest Group' = 'interestGroup', \n         'Joviality' = 'joviality')\n\n\ncolnames(participants) # verify if the columns have been renamed correctly \n\n[1] \"Participant Id\"  \"Household Size\"  \"Have Kids\"       \"Age\"            \n[5] \"Education Level\" \"Interest Group\"  \"Joviality\"      \n\n#rename value \nparticipants$`Education Level` <- sub('HighSchoolOrCollege', \n                                    'High School or College',\n                                    participants$`Education Level`)\n\nparticipants$`Household Size` <- as.factor(participants$`Household Size`)\nparticipants$`Education Level` <- factor(participants$`Education Level`, levels = c(\n  \"Low\", \"High School or College\", \"Bachelors\", \"Graduate\"), ordered = TRUE) #create factor data object to categorise the Education Level by levels.\n\nWe will now examine how many different input categories are there. There are 6 different categories and 1011 participants throughout the period of 1 year and 2 months based on the timestamp. There should be a total of 2,547,720 financial records but the total recorded data was only 1,856,330. This shows some participants might not have recorded their finance throughout the period. We will now identify participants that are not consistent in their input.\n\nunique(finance$category)\n\n[1] \"Wage\"           \"Shelter\"        \"Education\"      \"RentAdjustment\"\n[5] \"Food\"           \"Recreation\"    \n\n\nBased on our analysis of the data, there are 131 participants who have only logged in less than 12 times throughout the period of analysis. We will identify these participants as inactive and will exclude them during our analysis of the the population demographics.\n\nincome <- finance %>% \n  filter(category == 'Wage') %>% # extract only wage data\n  select(participantId, amount) %>%\n  group_by(participantId) %>%\n  summarise(count = n()) %>%\n  ungroup()\n\ninactive <- finance %>% \n  filter(category == 'Wage') %>% # extract only wage data\n  select(participantId, amount) %>%\n  group_by(participantId) %>%\n  summarise(count = n()) %>%\n  filter (count < 13) %>%\n  ungroup()\n\n\ninactivepart <- inactive$participantId\n\nactive_participants <- subset(participants, !(`Participant Id` %in% inactivepart))\n\nSince the period of study is 15 months, we will extract the average monthly wage of each active participants using the summarise function and rounding the answer to 2 decimal place.\n\nactive_finance <- subset(finance, !(participantId %in% inactivepart))\n\nactive_finance <- active_finance %>% \n  filter(category == 'Wage') %>% # extract only wage data\n  select(participantId, amount) %>%\n  group_by(participantId) %>%\n  summarise (Income = round(sum(amount)/15,2)) %>%\n  ungroup()\n\n\nactive_participants <- active_participants %>%\n  left_join (active_finance, by = c(\"Participant Id\" = \"participantId\")) %>%\n  mutate(Joviality = Joviality * 100)"
  },
  {
    "objectID": "posts/Me/SIS Representation/index.html#visualisation-and-insights",
    "href": "posts/Me/SIS Representation/index.html#visualisation-and-insights",
    "title": "SIS Visual Representation",
    "section": "Visualisation and Insights",
    "text": "Visualisation and Insights\nVisualising using Static Graph\nWe will first visualise the distribution of the different attributes.\n\n\ngeom_text() is used to add annotations of the count and % values for geom_bar()\n\nGrids and background color are removed for a cleaner look as annotations are included.\nTo choose the different colours for the graph, I use medialab to decide on the Hue colors based on the number of graphs.\n\n\nage <- ggplot (active_participants, aes (x=Age)) +\n  geom_histogram(binwidth=5, fill=\"#c96d44\", color=\"#e9ecef\", alpha=0.9) +\n    labs(title = \"Age Distribution of Active Participants\", subtitle = \"Bin Size 5\") +\n    theme_ipsum() +\n    theme(\n      plot.title = element_text(size=15), axis.title.y= element_text(angle=0)\n    )\n\nhKids <- active_participants %>%\n  ggplot(aes(x = `Have Kids`)) +\n  geom_bar(fill= '#777acd') +\n  geom_text(stat = 'count',\n           aes(label= paste0(stat(count), ', ', \n                             round(stat(count)/sum(stat(count))*100, \n                             1), '%')), vjust= -0.5, size= 3) +\n  labs(y= 'No. of\\nParticipants', title = \"Distribution of Participants \\nwith/without Kids\") +\n  theme(axis.title.y= element_text(angle=0), axis.ticks.x= element_blank(),\n        panel.background= element_blank(), axis.line= element_line(color= 'grey'))\n\n\nhousehold <- active_participants %>%\n  ggplot(aes(x = `Household Size`)) +\n   geom_bar(fill= '#7aa456') +\n  geom_text(stat = 'count',\n           aes(label= paste0(stat(count), ', ', \n                             round(stat(count)/sum(stat(count))*100, \n                             1), '%')), vjust= -0.5, size= 3) +\n  labs(y= 'No. of\\nParticipants', title = \"Distribution of Participants \\nbased on Household Size\") +\n  theme(axis.title.y= element_text(angle=0), axis.ticks.x= element_blank(),\n        panel.background= element_blank(), axis.line= element_line(color= 'grey'))\n\neducation <- active_participants %>%\n  ggplot(aes(x = `Education Level`)) +\n   geom_bar(fill= '#c65999') +\n  geom_text(stat = 'count',\n           aes(label= paste0(stat(count), ', ', \n                             round(stat(count)/sum(stat(count))*100, \n                             1), '%')), vjust= -0.5, size= 3) +\n  labs(y= 'No. of\\nParticipants', title = \"Distribution of Participants \\nbased on Education Level\") +\n  theme(axis.title.y= element_text(angle=0), axis.ticks.x= element_blank(),\n        panel.background= element_blank(), axis.line= element_line(color= 'grey'), title = element_text(size = 10))\n\n\n(age + hKids)/(household + education) #using patchwork to stitch the different graphs together\n\n\n\n\n\njoy <- ggplot (active_participants, aes (x=Joviality)) +\n  geom_histogram(binwidth=5, fill=\"#9c954d\", color=\"#e9ecef\", alpha=0.9) +\n    labs(title = \"Joviality Distribution of Active Participants\", subtitle = \"Bin Size 5\") +\n    theme_ipsum() +\n    theme(\n      plot.title = element_text(size=15), axis.title.y= element_text(angle=0)\n    )\n\nincome <- ggplot (active_participants, aes (x=Income)) +\n  geom_histogram(binwidth=1000, fill=\"#b067a3\", color=\"#e9ecef\", alpha=0.9) +\n    labs(title = \"Income Distribution of Active Participants\", subtitle = \"Bin Size 1000\") +\n    theme_ipsum() +\n    theme(\n      plot.title = element_text(size=15), axis.title.y= element_text(angle=0)\n    )\n\nincome + joy\n\n\n\n\nWe will conduct binning on our numerical data such as Age, Income and Joviality. We use the ntile function to break the values and case_when() to change the group labels accordingly.\n\nactive_participants_grouped <- active_participants %>%\n  mutate (Income_group = ntile(Income, 4)) %>%\n  mutate (Joviality_group = ntile(Joviality, 4)) %>%\n  mutate (Income_group = case_when(\n    Income_group == 1 ~ \"Low Income\",\n    Income_group == 2 ~ \"Mid-Low Income\",\n    Income_group == 3 ~ \"Mid-High Income\",\n    Income_group == 4 ~ \"High Income\"\n  )) %>%\n  mutate (Joviality_group = case_when(\n    Joviality_group == 1 ~ \"Low Joy\",\n    Joviality_group == 2 ~ \"Mid-Low Joy\",\n    Joviality_group == 3 ~ \"Mid-High Joy\",\n    Joviality_group == 4 ~ \"High Joy\"\n  ))\n\nVisualising using Interactive Graph\nWe will now analyse the data using interactive graphs such as parallel set plot. We will leverage on the parset library to provide interactive function. The interesting feature about the parset function is that it allows the user to dynamically shift the levels of the attributes (top-bottom and left-right), providing the user a more interactive visualisation of the data set.\n\nactive_participants_parset <- active_participants_grouped %>%\n  select (`Household Size`, `Have Kids`, `Education Level`, `Interest Group`, Income_group, Joviality_group)\n\nparset(active_participants_parset)\n\n\n\n\n\nVisualising using Statistical Graph\nFrom the Parset plot, we identify a few probable relationship such as Education Level to Income Level etc. We will now use statistical plot to verify our claim. The ggstatsplot library provides a suite of statistical plot to allow user to choose the plot based on its data set. For this study, since our attributes are in categorical form, I will leverage on the ggbarstats.\nInsights\nPearson’s \\(x^2\\)-test of independence revealed that, across 880 participants,there was a significant association between Income Level, Education Level and Joviality Level. (p-value below alpha value of 0.05). The Bayes Factor for the left analysis revealed that the data were \\(8e^{66}\\) times more probable under the alternative hypothesis as compared to the null hypothesis. This can be considered extreme evidence (Sandra Andraszewicz, 2015) in favor of the alternative hypothesis. The Bayes Factor for the right analysis revealed that the data were 23968348874 times more probable under the alternative hypothesis as compared to the null hypothesis. This can also be considered extreme evidence in favor of the alternative hypothesis.\n\nactive_participants_parset$Income_group <- factor(active_participants_parset$Income_group, levels = c(\n  \"Low Income\", \"Mid-Low Income\", \"Mid-High Income\", \"High Income\"), ordered = TRUE) #create factor data object to segment the Education Level by levels.\n\nincome <- ggbarstats(\n  data = active_participants_parset,\n  x = `Education Level`,\n  y = Income_group,\n  type = \"np\",\n  xlab = \"Income Group\"\n)\n\njoy <- ggbarstats(\n  data = active_participants_parset,\n  x = Joviality_group,\n  y = Income_group,\n  type = \"np\",\n  xlab = \"Income Group\"\n)\n\n\nincome + joy"
  },
  {
    "objectID": "posts/Me/SIS Representation/index.html#conclusion",
    "href": "posts/Me/SIS Representation/index.html#conclusion",
    "title": "SIS Visual Representation",
    "section": "Conclusion",
    "text": "Conclusion\nIt is important for data analyst to understand the importance of static and interactive graphs, how we should leverage these tools to provide appropriate data visualisation and subsequently use statistical graphs to draw statistical conclusion to support the hypothesis."
  },
  {
    "objectID": "posts/me.html",
    "href": "posts/me.html",
    "title": "Jordan OZR",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\n\nLeveraging on simple data wrangling techniques to create a RFM model. Subsequently, leveraging on unsupervised classification to conduct customer segmentation for targeted marketing.\n\n\n\n\ntibble\n\n\nclustering\n\n\nunsupervised\n\n\n \n\n\n\n\nJune 25, 2022\n\n\nOng Zhi Rong Jordan\n\n\n\n\n\n\n  \n\n\n\n\n\nUsing Static, Interactive and Statistical (SIS) Graphs to reveal the demographics and relationships of a city.\n\n\n\n\ntibble\n\n\nggstatsplot\n\n\n \n\n\n\n\nJune 8, 2022\n\n\nOng Zhi Rong Jordan\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/visual.html",
    "href": "posts/visual.html",
    "title": "Jordan OZR",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Geo/Geospatial_Choropleth/index.html",
    "href": "posts/Geo/Geospatial_Choropleth/index.html",
    "title": "Introduction to Choropleth Mapping",
    "section": "",
    "text": "For this analysis, we will use the following packages from CRAN.\n\n\nsf - Support for simple features, a standardized way to encode spatial vector data. Binds to ‘GDAL’ for reading and writing data, to ‘GEOS’ for geometrical operations, and to ‘PROJ’ for projection conversions and datum transformations. Uses by default the ‘s2’ package for spherical geometry operations on ellipsoidal (long/lat) coordinates.\n\ntidyverse - Loading the core tidyverse packages which will be used for data wrangling and visualisation.\n\ntmap - Thematic maps are geographical maps in which spatial data distributions are visualized. This package offers a flexible, layer-based, and easy to use approach to create thematic maps, such as choropleths and bubble maps.\n\nleaflet - Create and customize interactive maps using the ‘Leaflet’ JavaScript library and the ‘htmlwidgets’ package.\n\nscales - Graphical scales map data to aesthetics, and provide methods for automatically determining breaks and labels for axes and legends.\n\nviridis - color scales in this package to make plots that are pretty, better represent your data, easier to read by those with colorblindness, and print well in gray scale\n\n\npacman::p_load(sf, tidyverse, tmap, leaflet, scales, viridis)"
  },
  {
    "objectID": "posts/Geo/Geospatial_Choropleth/index.html#libraries",
    "href": "posts/Geo/Geospatial_Choropleth/index.html#libraries",
    "title": "Introduction to Choropleth Mapping",
    "section": "Libraries",
    "text": "Libraries\nFor this analysis, we will use the following packages from CRAN.\n\n\nsf - Support for simple features, a standardized way to encode spatial vector data. Binds to ‘GDAL’ for reading and writing data, to ‘GEOS’ for geometrical operations, and to ‘PROJ’ for projection conversions and datum transformations. Uses by default the ‘s2’ package for spherical geometry operations on ellipsoidal (long/lat) coordinates.\n\ntidyverse - Loading the core tidyverse packages which will be used for data wrangling and visualisation.\n\ntmap - Thematic maps are geographical maps in which spatial data distributions are visualized. This package offers a flexible, layer-based, and easy to use approach to create thematic maps, such as choropleths and bubble maps.\n\nleaflet - Create and customize interactive maps using the ‘Leaflet’ JavaScript library and the ‘htmlwidgets’ package.\n\nscales - Graphical scales map data to aesthetics, and provide methods for automatically determining breaks and labels for axes and legends.\n\nviridis - Color scales in this package to make plots that are pretty, better represent your data, easier to read by those with colorblindness, and print well in gray scale.\n\n\npacman::p_load(sf, tidyverse, tmap, leaflet, scales, viridis)"
  },
  {
    "objectID": "posts/Geo/Geospatial_Choropleth/index.html#data-preparation",
    "href": "posts/Geo/Geospatial_Choropleth/index.html#data-preparation",
    "title": "Introduction to Choropleth Mapping",
    "section": "Data Preparation",
    "text": "Data Preparation\nTwo data set will be used to create the choropleth map. They are:\n\nMaster Plan 2014 Subzone Boundary (Web) (i.e. MP14_SUBZONE_WEB_PL) in ESRI shapefile format. It can be downloaded at data.gov.sg This is a geospatial data. It consists of the geographical boundary of Singapore at the planning subzone level. The data is based on URA Master Plan 2014.\nSingapore Residents by Planning Area / Subzone, Age Group, Sex and Type of Dwelling, June 2011-2020 in csv format (i.e. respopagesextod2011to2020.csv). This is an aspatial data fie. It can be downloaded at Department of Statistics, Singapore Although it does not contain any coordinates values, but it’s PA and SZ fields can be used as unique identifiers to geocode to MP14_SUBZONE_WEB_PL shapefile.\n\nImporting of Geospatial Data\nThe code chunk below uses the st_read() function of sf package to import MP14_SUBZONE_WEB_PL shapefile into R as a simple feature data frame called mpsz. To view the tibble data frame, we can simply call the tibble file name mpsz. When you print a tibble, it only shows the first ten rows and all the columns that fit on one screen. It also prints an abbreviated description of the column type.\n\nmpsz <- st_read(dsn = \"data/geospatial\", \n                layer = \"MP14_SUBZONE_WEB_PL\") %>%\n  st_transform(crs = 3414)\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `C:\\jordanong09\\ISSS624_Geospatial\\posts\\Geo\\Geospatial_Choropleth\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\nmpsz\n\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21 / Singapore TM\nFirst 10 features:\n   OBJECTID SUBZONE_NO       SUBZONE_N SUBZONE_C CA_IND      PLN_AREA_N\n1         1          1    MARINA SOUTH    MSSZ01      Y    MARINA SOUTH\n2         2          1    PEARL'S HILL    OTSZ01      Y          OUTRAM\n3         3          3       BOAT QUAY    SRSZ03      Y SINGAPORE RIVER\n4         4          8  HENDERSON HILL    BMSZ08      N     BUKIT MERAH\n5         5          3         REDHILL    BMSZ03      N     BUKIT MERAH\n6         6          7  ALEXANDRA HILL    BMSZ07      N     BUKIT MERAH\n7         7          9   BUKIT HO SWEE    BMSZ09      N     BUKIT MERAH\n8         8          2     CLARKE QUAY    SRSZ02      Y SINGAPORE RIVER\n9         9         13 PASIR PANJANG 1    QTSZ13      N      QUEENSTOWN\n10       10          7       QUEENSWAY    QTSZ07      N      QUEENSTOWN\n   PLN_AREA_C       REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR\n1          MS CENTRAL REGION       CR 5ED7EB253F99252E 2014-12-05 31595.84\n2          OT CENTRAL REGION       CR 8C7149B9EB32EEFC 2014-12-05 28679.06\n3          SR CENTRAL REGION       CR C35FEFF02B13E0E5 2014-12-05 29654.96\n4          BM CENTRAL REGION       CR 3775D82C5DDBEFBD 2014-12-05 26782.83\n5          BM CENTRAL REGION       CR 85D9ABEF0A40678F 2014-12-05 26201.96\n6          BM CENTRAL REGION       CR 9D286521EF5E3B59 2014-12-05 25358.82\n7          BM CENTRAL REGION       CR 7839A8577144EFE2 2014-12-05 27680.06\n8          SR CENTRAL REGION       CR 48661DC0FBA09F7A 2014-12-05 29253.21\n9          QT CENTRAL REGION       CR 1F721290C421BFAB 2014-12-05 22077.34\n10         QT CENTRAL REGION       CR 3580D2AFFBEE914C 2014-12-05 24168.31\n     Y_ADDR SHAPE_Leng SHAPE_Area                       geometry\n1  29220.19   5267.381  1630379.3 MULTIPOLYGON (((31495.56 30...\n2  29782.05   3506.107   559816.2 MULTIPOLYGON (((29092.28 30...\n3  29974.66   1740.926   160807.5 MULTIPOLYGON (((29932.33 29...\n4  29933.77   3313.625   595428.9 MULTIPOLYGON (((27131.28 30...\n5  30005.70   2825.594   387429.4 MULTIPOLYGON (((26451.03 30...\n6  29991.38   4428.913  1030378.8 MULTIPOLYGON (((25899.7 297...\n7  30230.86   3275.312   551732.0 MULTIPOLYGON (((27746.95 30...\n8  30222.86   2208.619   290184.7 MULTIPOLYGON (((29351.26 29...\n9  29893.78   6571.323  1084792.3 MULTIPOLYGON (((20996.49 30...\n10 30104.18   3454.239   631644.3 MULTIPOLYGON (((24472.11 29...\n\n\nImporting Attribute Data into R\nNext, we will import respopagsex2000to2018.csv file into RStudio and save the file into an R dataframe called popdata.\nThe task will be performed by using read_csv() function of readr package as shown in the code chunk below.\n\npopdata <- read_csv(\"data/respopagesextod2011to2020.csv\")\n\nData Wrangling\nThe following data wrangling and transformation functions will be used:\n\npivot_wider() of tidyr package, and\nmutate(), filter(), group_by() and select() of dplyr package\n\n\npopdata2020 <- popdata %>%\n  filter(Time == 2020) %>%\n  group_by(PA, SZ, AG) %>%\n  summarise(`POP` = sum(`Pop`)) %>%\n  ungroup()%>%\n  pivot_wider(names_from=AG, \n              values_from=POP) %>%\n  mutate(YOUNG = rowSums(.[3:6])\n         +rowSums(.[12])) %>%\nmutate(`ECONOMY ACTIVE` = rowSums(.[7:11])+\nrowSums(.[13:15]))%>%\nmutate(`AGED`=rowSums(.[16:21])) %>%\nmutate(`TOTAL`=rowSums(.[3:21])) %>%  \nmutate(`DEPENDENCY` = (`YOUNG` + `AGED`)\n/`ECONOMY ACTIVE`) %>%\n  select(`PA`, `SZ`, `YOUNG`, \n       `ECONOMY ACTIVE`, `AGED`, \n       `TOTAL`, `DEPENDENCY`)\n\nJoining of attribute and geospatial data frame\nBefore we can perform the georelational join, one extra step is required to convert the values in PA and SZ fields to uppercase. This is because the values of PA and SZ fields are made up of upper- and lowercase. On the other, hand the SUBZONE_N and PLN_AREA_N are in uppercase.\nNext, left_join() of dplyr is used to join the geographical data and attribute table using planning subzone name e.g. SUBZONE_N and SZ as the common identifier.\n\npopdata2020 <- popdata2020 %>%\n  mutate_at(.vars = vars(PA, SZ), \n          .funs = funs(toupper)) %>%\n  filter(`ECONOMY ACTIVE` > 0)\n\nmpsz_pop2020 <- left_join(mpsz, popdata2020,\n                          by = c(\"SUBZONE_N\" = \"SZ\"))"
  },
  {
    "objectID": "posts/Geo/Geospatial_Choropleth/index.html#choropleth-mapping-of-geospatial-data",
    "href": "posts/Geo/Geospatial_Choropleth/index.html#choropleth-mapping-of-geospatial-data",
    "title": "Introduction to Choropleth Mapping",
    "section": "Choropleth Mapping of Geospatial Data",
    "text": "Choropleth Mapping of Geospatial Data\nPlotting using TMap\ntmap has similar syntax to the popular ggplot2 but will also produce a reasonable map with only a couple of lines of code. A default colour scheme will be given where this is not specified and passed to tm_polygons and a legend will also be created by default.\ntmap also offer the user two views, static (plot) or interactive (view).\nTwo approaches can be used to prepare thematic map using tmap, they are:\n\nPlotting a thematic map quickly by using qtm().\nPlotting highly customisable thematic map by using tmap elements.\n\nPlotting a choropleth map quickly by using qtm()\nqtm is termed as quick thematic mode allow users to quickly draw a choropleth with a single line of code. It is concise and provides a good default visualisation in many cases. We will explore the different view that tmap provides.\nThe code chunk below will draw an interactive cartographic standard choropleth map as shown below. The fill argument is used to map the attribute. (i.e. DEPENDENCY)\nThe interactive mode uses the leaflet library. Since the leaflet library require the sf object to be in WGS84, we need to set the tmap_options to true to allow our data set which is SVY21 to be plotted on the leaflet map.\n\ntmap_mode(\"view\")\ntmap_options(check.and.fix = TRUE)\nqtm(mpsz_pop2020, \n    fill = \"DEPENDENCY\")\n\n\n\n\n\ntmap_mode(\"plot\")\n\nThe code chunk below will draw a static cartographic standard choropleth map as shown below.\n\nqtm(mpsz_pop2020, \n    fill = \"DEPENDENCY\")\n\n\n\n\n\nstatic map using qtm\n\n\n\n\nCreating a choropleth map by using tmap’s elements\nDespite its usefulness of drawing a choropleth map quickly and easily, the disadvantge of qtm() is that it makes aesthetics of individual layers harder to control. To draw a high quality cartographic choropleth map as shown in the figure below, tmap’s drawing elements should be used. In the following sub-section, we will share with you tmap functions that used to plot these elements.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"Dependency ratio\") +\n  tm_layout(main.title = \"Distribution of Dependency Ratio by planning subzone\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar() +\n  tm_grid(alpha =0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\\n and Population data from Department of Statistics DOS\", \n             position = c(\"left\", \"bottom\"))\n\n\n\n\n\nstatic tmap with tmap elements\n\n\n\n\nDrawing a base map\nThe basic building block of tmap is tm_shape() followed by one or more layer elemments such as tm_fill() and tm_polygons().\nIn the code chunk below, tm_shape() is used to define the input data (i.e mpsz_pop2020) and tm_polygons() is used to draw the planning subzone polygons\n\ntm_shape(mpsz_pop2020) +\n  tm_polygons()\n\n\n\n\n\nbase map without elements\n\n\n\n\nDrawing a choropleth map using tm_polygons()\nTo draw a choropleth map showing the geographical distribution of a selected variable by planning subzone, we just need to assign the target variable such as Dependency to tm_polygons(). This is similar to the qtm drawn earlier.\n\nThe default interval binning used to draw the choropleth map is called “pretty”.\nThe default colour scheme used is YlOrRd of ColorBrewer\nBy default, Missing value will be shaded in grey.\n\n\ntm_shape(mpsz_pop2020)+\n  tm_polygons(\"DEPENDENCY\")\n\n\n\n\n\nmap with polygons\n\n\n\n\nDrawing a choropleth map using tm_fill() and *tm_border()**\nActually, tm_polygons() is a wraper of tm_fill() and tm_border(). tm_fill() shades the polygons by using the default colour scheme and tm_borders() adds the borders of the shapefile onto the choropleth map.\nThe code chunk below draws a choropleth map by using tm_fill() alone.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\")\n\n\n\n\n\nmap with fill only\n\n\n\n\nTo add the boundary of the planning subzones, tm_borders will be used as shown in the code chunk below.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\") +\n  tm_borders(lwd = 0.1,  alpha = 1)\n\n\n\n\n\nmap with fill and borders\n\n\n\n\nData classification methods of tmap\nMost choropleth maps employ some methods of data classification. The point of classification is to take a large number of observations and group them into data ranges or classes.\ntmap provides a total ten data classification methods, namely: fixed, sd, equal, pretty (default), quantile, kmeans, hclust, bclust, fisher, and jenks.\nThe code chunk below shows multiple data classification methods and classes to illustrate the difference. tmap_arrange is used to display the consolidated maps in grid form.\n\ntmap1 <-  tm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 4,\n          style = \"quantile\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(title = \"Quantile - 4 Class\", title.size = 0.7, legend.text.size = 0.4)\n\ntmap2 <-  tm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 4,\n          style = \"jenks\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(title = \"Jenks - 4 Class\", title.size = 0.7, legend.text.size = 0.4)\n\ntmap3 <-  tm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 4,\n          style = \"equal\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(title = \"Equal - 5 Class\", title.size = 0.7, legend.text.size = 0.4)\n\ntmap4 <-  tm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"hclust\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(title = \"Hclust - 5 Class\", title.size = 0.7, legend.text.size = 0.4)\n\ntmap5 <-  tm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"sd\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(title = \"Sd - 5 Class\", title.size = 0.7, legend.text.size = 0.4)\n\n\ntmap6 <-  tm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"kmeans\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(title = \"Kmeans - 5 Class\", title.size = 0.7, legend.text.size = 0.4)\n\ntmap_arrange(tmap1, tmap2, tmap3, tmap4, tmap5, tmap6, ncol = 3)\n\n\n\n\n\ntmap with multiple data classification methods and classes\n\n\n\n\n\nsummary(mpsz_pop2020$DEPENDENCY)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n 0.0000  0.6519  0.7025  0.7742  0.7645 19.0000      92 \n\ntmap_mode(\"plot\")\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          breaks = c(0, 0.60, 0.70, 0.80, 0.90, 1.00)) +\n  tm_borders(alpha = 0.5) \n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 6,\n          style = \"quantile\",\n          palette = \"Blues\") +\n  tm_borders(alpha = 0.5)\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          style = \"quantile\",\n          palette = \"-Greens\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"jenks\", \n          palette = \"Blues\", \n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(main.title = \"Distribution of Dependency Ratio by planning subzone \\n(Jenks classification)\",\n            main.title.position = \"center\",\n            main.title.size = 1,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            legend.outside = FALSE,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = FALSE) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"-Greens\") +\n  tm_borders(alpha = 0.5) +\n  tmap_style(\"classic\")\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"No. of persons\") +\n  tm_layout(main.title = \"Distribution of Dependency Ratio \\nby planning subzone\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar(width = 0.15) +\n  tm_grid(lwd = 0.1, alpha = 0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\\n and Population data from Department of Statistics DOS\", \n             position = c(\"left\", \"bottom\")) +\n  tmap_style(\"white\")\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(c(\"YOUNG\", \"AGED\"),\n          style = \"equal\", \n          palette = \"Blues\") +\n  tm_layout(legend.position = c(\"right\", \"bottom\")) +\n  tm_borders(alpha = 0.5) +\n  tmap_style(\"white\")\n\n\n\ntm_shape(mpsz_pop2020)+ \n  tm_polygons(c(\"DEPENDENCY\",\"AGED\"),\n          style = c(\"equal\", \"quantile\"), \n          palette = list(\"Blues\",\"Greens\")) +\n  tm_layout(legend.position = c(\"right\", \"bottom\"))\n\n\n\n\n\ntm_shape(mpsz_pop2020) +\n  tm_fill(\"DEPENDENCY\",\n          style = \"quantile\",\n          palette = \"Blues\",\n          thres.poly = 0) + \n  tm_facets(by=\"REGION_N\", \n            free.coords=TRUE, \n            drop.shapes=TRUE) +\n  tm_layout(legend.show = FALSE,\n            title.position = c(\"center\", \"center\"), \n            title.size = 20) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\nyoungmap <- tm_shape(mpsz_pop2020)+ \n  tm_polygons(\"YOUNG\", \n              style = \"quantile\", \n              palette = \"Blues\")\n\nagedmap <- tm_shape(mpsz_pop2020)+ \n  tm_polygons(\"AGED\", \n              style = \"quantile\", \n              palette = \"Blues\")\n\ntmap_arrange(youngmap, agedmap, asp=1, ncol=2)\n\n\n\n\n\ntm_shape(mpsz_pop2020[mpsz_pop2020$REGION_N==\"CENTRAL REGION\", ])+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\", \n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(legend.outside = TRUE,\n            legend.height = 0.45, \n            legend.width = 5.0,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = FALSE) +\n  tm_borders(alpha = 0.5)\n\n\n\n\nPlotting using ggplot\nggplot provides the user much more flexibility in the layers required on the map. Since our object is an sf object, we will use geom_sf which will automatically detect a geometry column and map it. coord_sf is also used to govern the map projection.\n\nggmap <- ggplot(data = mpsz_pop2020) +\n  geom_sf(aes(fill = YOUNG)) +\n  geom_text(aes(x = X_ADDR, y = Y_ADDR, label = PLN_AREA_C), size = 1) + #input the planning area labels\n  xlab(\"Longitude\") + ylab(\"Latitude\") + #x and y axis name\n  ggtitle(\"Dependency level across Planning Area\") + #title\n  theme_bw() + #theme chosen\n  theme(panel.grid.major = element_line(color = gray(.5), linetype = \"dashed\", size = 0.5),\n        panel.background = element_rect(fill = \"aliceblue\")) + \n  coord_sf(crs = st_crs(3414)) \n\nggmap\n\n\n\n\nThe viridis package also allow the user to improve the colour scaling on the plot. Since we use fill to fill the map with the Young attribute, we will use scale_fill_viridis to scale the variable based on the viridis palette.\n\nggmap + scale_fill_viridis(option = \"magma\", direction = -1)\n\n\n\n\nPlotting using leaflet\nLeaflet is one of the most popular open-source JavaScript libraries for interactive maps. This package has grown significantly in popularity in recent years and has fast become common currency amongst companies wishing to dynamically visualize its data. It is an excellent option to consider where the patterns in your data are large and complex and where you have constituent polygons of varying sizes.\nFeatures\n\nInteractive panning/zooming\n\nCompose maps using arbitrary combinations of:\n\nMap tiles\nMarkers\nPolygons\nLines\nPopups\nGeoJSON\n\n\nCreate maps right from the R console or RStudio\nEmbed maps in knitr/R Markdown documents and Shiny apps\nEasily render spatial objects from the sp or sf packages, or data frames with latitude/longitude columns\nUse map bounds and mouse events to drive Shiny logic\nDisplay maps in non spherical mercator projections\nAugment map features using chosen plugins from leaflet plugins repository\n\nData Preparation for leaflet mapping\nFirstly, we create a new column and scale the Young attribute from 0 - 100. We use the colorBin function to maps numeric input data to a fixed number of output colors using the bin created. We then create the interactive labels using the sprintf function.\n\nmpsz_pop2020_leaflet <- mpsz_pop2020 %>%\n  mutate (youngpct = rescale(YOUNG, to = c(0,100)))\n\nmpsz_pop2020_leaflet$youngpct[is.nan(mpsz_pop2020_leaflet$youngpct)]<-0\n\nbins <- c(0, 20, 30, 40, 50, 60, 70, 80, 90, Inf)\npal <- colorBin(\"YlOrRd\", domain = mpsz_pop2020_leaflet$youngpct, bins = bins)\n\nlabels <- sprintf(\n  \"<strong>%s</strong><br/>%g Young Pct\",\n  mpsz_pop2020_leaflet$PLN_AREA_N, mpsz_pop2020_leaflet$youngpct\n) %>% lapply(htmltools::HTML)\n\nCRS projection for leaflet mapping\nThe Leaflet package expects all point, line, and shape data to be specified in latitude and longitude using WGS 84 (a.k.a. EPSG:4326). By default, when displaying this data it projects everything to EPSG:3857 and expects that any map tiles are also displayed in EPSG:3857.\nTherefore, we will need to transform our sf object to the correct crs using st_transform.\n\nmpsz_pop2020_leaflet <- mpsz_pop2020_leaflet %>%\n  st_transform(crs = 4326)\n\nPlotting of leaflet map\nThe easiest way to add tiles is by calling addTiles() with no arguments; by default, OpenStreetMap tiles are used. But many popular free third-party basemaps can be added using the addProviderTiles() function, which is implemented using the leaflet-providers plugin.\nAs a convenience, leaflet also provides a named list of all the third-party tile providers that are supported by the plugin. This enables you to use auto-completion feature of your favorite R IDE (like RStudio) and not have to remember or look up supported tile providers; just type providers$ and choose from one of the options. You can also use names(providers) to view all of the options. For this visualisation, I will use the CartoDB.Positron tiles.\n\nleaflet(mpsz_pop2020_leaflet) %>%\n  addProviderTiles(providers$CartoDB.Positron) %>%\n  addPolygons(\n  fillColor = ~pal(youngpct),\n  weight = 1,\n  opacity = 1,\n  color = \"white\",\n  dashArray = \"3\",\n  fillOpacity = 0.7,\n  highlight = highlightOptions(\n    weight = 5,\n    color = \"#666\",\n    dashArray = \"\",\n    fillOpacity = 0.7,\n    bringToFront = TRUE),\n  label = labels,\n  labelOptions = labelOptions(\n    style = list(\"font-weight\" = \"normal\", padding = \"3px 8px\"),\n    textsize = \"15px\",\n    direction = \"auto\")) %>%\naddLegend(pal = pal, values = ~youngpct, opacity = 0.7, title = NULL,\n                position = \"bottomright\")"
  },
  {
    "objectID": "posts/Geo/Geospatial_Datawrangling/index.html",
    "href": "posts/Geo/Geospatial_Datawrangling/index.html",
    "title": "Data Wrangling of Geospatial Data",
    "section": "",
    "text": "For this analysis, we will use the following packages from CRAN.\n\n\nsf - Support for simple features, a standardized way to encode spatial vector data. Binds to ‘GDAL’ for reading and writing data, to ‘GEOS’ for geometrical operations, and to ‘PROJ’ for projection conversions and datum transformations. Uses by default the ‘s2’ package for spherical geometry operations on ellipsoidal (long/lat) coordinates.\n\ntidyverse - Loading the core tidyverse packages which will be used for data wrangling and visualisation.\n\n\npacman::p_load(sf, tidyverse)"
  },
  {
    "objectID": "posts/Geo/Geospatial_Datawrangling/index.html#dataset",
    "href": "posts/Geo/Geospatial_Datawrangling/index.html#dataset",
    "title": "Data Wrangling of Geospatial Data",
    "section": "Dataset",
    "text": "Dataset\nFor this analysis, we will extract data that is available from the web.\n\nMaster Plan 2014 Subzone Boundary (Web) from data.gov.sg\nPre-Schools Location from data.gov.sg\nCycling Path from LTADataMall\nLatest version of Singapore Airbnb listing data from Inside Airbnb\n\nExtracting geographical information from Dataset\nWe will leverage on the st_read function to retrieve polygon, line and point feature in both ESRI shapefile and KML format.\nThe st_geometry function returns an object of class sfc whereas the glimpse function from dplyr act as a transposed version of the print function that shows the values of the different columns.\nTo check all the classes within the dataset, we use the sapply function to run the class function through all the columns within the data set and return their classes.\n\nmpsz <- st_read(dsn = \"data/geospatial\", \n                  layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `C:\\jordanong09\\ISSS624_Geospatial\\posts\\Geo\\Geospatial_Datawrangling\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\ncyclingpath <- st_read(dsn = \"data/geospatial\", \n                      layer = \"CyclingPath\")\n\nReading layer `CyclingPath' from data source \n  `C:\\jordanong09\\ISSS624_Geospatial\\posts\\Geo\\Geospatial_Datawrangling\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 1625 features and 2 fields\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: 12711.19 ymin: 28711.33 xmax: 42626.09 ymax: 48948.15\nProjected CRS: SVY21\n\npreschool <- st_read(\"data/geospatial/pre-schools-location-kml.kml\")\n\nReading layer `PRESCHOOLS_LOCATION' from data source \n  `C:\\jordanong09\\ISSS624_Geospatial\\posts\\Geo\\Geospatial_Datawrangling\\data\\geospatial\\pre-schools-location-kml.kml' \n  using driver `KML'\nSimple feature collection with 1359 features and 2 fields\nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 103.6824 ymin: 1.248403 xmax: 103.9897 ymax: 1.462134\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84\n\nst_geometry(mpsz)\n\nGeometry set for 323 features \nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\nFirst 5 geometries:\n\nglimpse(mpsz)\n\nRows: 323\nColumns: 16\n$ OBJECTID   <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, …\n$ SUBZONE_NO <int> 1, 1, 3, 8, 3, 7, 9, 2, 13, 7, 12, 6, 1, 5, 1, 1, 3, 2, 2, …\n$ SUBZONE_N  <chr> \"MARINA SOUTH\", \"PEARL'S HILL\", \"BOAT QUAY\", \"HENDERSON HIL…\n$ SUBZONE_C  <chr> \"MSSZ01\", \"OTSZ01\", \"SRSZ03\", \"BMSZ08\", \"BMSZ03\", \"BMSZ07\",…\n$ CA_IND     <chr> \"Y\", \"Y\", \"Y\", \"N\", \"N\", \"N\", \"N\", \"Y\", \"N\", \"N\", \"N\", \"N\",…\n$ PLN_AREA_N <chr> \"MARINA SOUTH\", \"OUTRAM\", \"SINGAPORE RIVER\", \"BUKIT MERAH\",…\n$ PLN_AREA_C <chr> \"MS\", \"OT\", \"SR\", \"BM\", \"BM\", \"BM\", \"BM\", \"SR\", \"QT\", \"QT\",…\n$ REGION_N   <chr> \"CENTRAL REGION\", \"CENTRAL REGION\", \"CENTRAL REGION\", \"CENT…\n$ REGION_C   <chr> \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\",…\n$ INC_CRC    <chr> \"5ED7EB253F99252E\", \"8C7149B9EB32EEFC\", \"C35FEFF02B13E0E5\",…\n$ FMEL_UPD_D <date> 2014-12-05, 2014-12-05, 2014-12-05, 2014-12-05, 2014-12-05…\n$ X_ADDR     <dbl> 31595.84, 28679.06, 29654.96, 26782.83, 26201.96, 25358.82,…\n$ Y_ADDR     <dbl> 29220.19, 29782.05, 29974.66, 29933.77, 30005.70, 29991.38,…\n$ SHAPE_Leng <dbl> 5267.381, 3506.107, 1740.926, 3313.625, 2825.594, 4428.913,…\n$ SHAPE_Area <dbl> 1630379.27, 559816.25, 160807.50, 595428.89, 387429.44, 103…\n$ geometry   <MULTIPOLYGON [m]> MULTIPOLYGON (((31495.56 30..., MULTIPOLYGON (…\n\nsapply(mpsz, class)\n\n$OBJECTID\n[1] \"integer\"\n\n$SUBZONE_NO\n[1] \"integer\"\n\n$SUBZONE_N\n[1] \"character\"\n\n$SUBZONE_C\n[1] \"character\"\n\n$CA_IND\n[1] \"character\"\n\n$PLN_AREA_N\n[1] \"character\"\n\n$PLN_AREA_C\n[1] \"character\"\n\n$REGION_N\n[1] \"character\"\n\n$REGION_C\n[1] \"character\"\n\n$INC_CRC\n[1] \"character\"\n\n$FMEL_UPD_D\n[1] \"Date\"\n\n$X_ADDR\n[1] \"numeric\"\n\n$Y_ADDR\n[1] \"numeric\"\n\n$SHAPE_Leng\n[1] \"numeric\"\n\n$SHAPE_Area\n[1] \"numeric\"\n\n$geometry\n[1] \"sfc_MULTIPOLYGON\" \"sfc\""
  },
  {
    "objectID": "posts/Geo/Geospatial_Datawrangling/index.html#plotting-of-geospatial-data",
    "href": "posts/Geo/Geospatial_Datawrangling/index.html#plotting-of-geospatial-data",
    "title": "Data Wrangling of Geospatial Data",
    "section": "Plotting of geospatial data",
    "text": "Plotting of geospatial data\nUnlike non-geospatial dataset where we plot the data using charts, we will leverage on map-based visualisation to draw insights from our geospatial data. The plot function uses the geometry data, contained primarily in the polygons slot. plot is one of the most useful functions in R, as it changes its behaviour depending on the input data. From the example below, we can see how we manipulate the plot based on how we subset the dataset.\n\nplot(mpsz) #plot based on the different column attributes\n\n\n\nplot(mpsz[\"PLN_AREA_N\"]) #colour plot based on column `PLN_AREA_N`\n\n\n\nplot(st_geometry(mpsz)) #only plot the basic geometry of the polygon data\ncondition <- mpsz$SUBZONE_NO > 5 #set a condition\nplot(mpsz[condition, ], col = \"turquoise\", add = TRUE) #layer the condition above the initial plot"
  },
  {
    "objectID": "posts/Geo/Geospatial_Datawrangling/index.html#changing-of-projection",
    "href": "posts/Geo/Geospatial_Datawrangling/index.html#changing-of-projection",
    "title": "Data Wrangling of Geospatial Data",
    "section": "Changing of Projection",
    "text": "Changing of Projection\nThe Coordinate Reference System (CRS) of spatial objects defines where they are placed on the Earth’s surface. We need to ensure the CRS of our sf objects are correct. Since Singapore uses EPSG:3414 - SVY21 / Singapore TM and from the above details, we understand that all the sf object does not conform to the correct CRS (WGS 84 or SVY21). We will utilise two different function, st_set_crs or st_transform to manually change the CRS of our sp object to the desired value.\n\nmpsz3414 <- st_set_crs(mpsz, 3414)\n\npreschool3414 <- st_transform(preschool, \n                              crs = 3414)\n\n\nst_geometry(mpsz3414)\n\nGeometry set for 323 features \nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21 / Singapore TM\nFirst 5 geometries:\n\nst_geometry(preschool3414)\n\nGeometry set for 1359 features \nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 11203.01 ymin: 25667.6 xmax: 45404.24 ymax: 49300.88\nz_range:       zmin: 0 zmax: 0\nProjected CRS: SVY21 / Singapore TM\nFirst 5 geometries:"
  },
  {
    "objectID": "posts/Geo/Geospatial_Datawrangling/index.html#importing-and-converting-an-aspatial-data",
    "href": "posts/Geo/Geospatial_Datawrangling/index.html#importing-and-converting-an-aspatial-data",
    "title": "Data Wrangling of Geospatial Data",
    "section": "Importing and Converting an Aspatial Data",
    "text": "Importing and Converting an Aspatial Data\nR provides the function to convert any foreign object to an sf object using the st_as_sf function. This will allow user to provide a data table that consist of the longitude and latitude and select the correct CRS to transform it to the approriate sf object.\nAfter importing the data, we will examine the dataframe using the list function.\n\nlistings <- read_csv(\"data/aspatial/listings.csv\")\nlist(listings) \n\n[[1]]\n# A tibble: 4,252 × 16\n       id name         host_id host_name neighbourhood_g… neighbourhood latitude\n    <dbl> <chr>          <dbl> <chr>     <chr>            <chr>            <dbl>\n 1  50646 Pleasant Ro…  227796 Sujatha   Central Region   Bukit Timah       1.33\n 2  71609 Ensuite Roo…  367042 Belinda   East Region      Tampines          1.35\n 3  71896 B&B  Room 1…  367042 Belinda   East Region      Tampines          1.35\n 4  71903 Room 2-near…  367042 Belinda   East Region      Tampines          1.35\n 5 275343 Convenientl… 1439258 Joyce     Central Region   Bukit Merah       1.29\n 6 275344 15 mins to … 1439258 Joyce     Central Region   Bukit Merah       1.29\n 7 294281 5 mins walk… 1521514 Elizabeth Central Region   Newton            1.31\n 8 301247 Nice room w… 1552002 Rahul     Central Region   Geylang           1.32\n 9 324945 20 Mins to … 1439258 Joyce     Central Region   Bukit Merah       1.29\n10 330089 Accomo@ RED… 1439258 Joyce     Central Region   Bukit Merah       1.29\n# … with 4,242 more rows, and 9 more variables: longitude <dbl>,\n#   room_type <chr>, price <dbl>, minimum_nights <dbl>,\n#   number_of_reviews <dbl>, last_review <date>, reviews_per_month <dbl>,\n#   calculated_host_listings_count <dbl>, availability_365 <dbl>\n\n\nThe output reveals that the data frame consists of 4252 rows and 16 columns. The column longtitude and latitude will be required for to transform this data frame to a sf object.\n\nlistings_sf <- st_as_sf(listings, \n                       coords = c(\"longitude\", \"latitude\"),\n                       crs=4326) %>%\n  st_transform(crs = 3414)\n\nst_crs(listings_sf)\n\nCoordinate Reference System:\n  User input: EPSG:3414 \n  wkt:\nPROJCRS[\"SVY21 / Singapore TM\",\n    BASEGEOGCRS[\"SVY21\",\n        DATUM[\"SVY21\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4757]],\n    CONVERSION[\"Singapore Transverse Mercator\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"northing (N)\",north,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"easting (E)\",east,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Cadastre, engineering survey, topographic mapping.\"],\n        AREA[\"Singapore - onshore and offshore.\"],\n        BBOX[1.13,103.59,1.47,104.07]],\n    ID[\"EPSG\",3414]]"
  },
  {
    "objectID": "posts/Geo/Geospatial_Datawrangling/index.html#joining-sf-and-tibble-dataframe",
    "href": "posts/Geo/Geospatial_Datawrangling/index.html#joining-sf-and-tibble-dataframe",
    "title": "Data Wrangling of Geospatial Data",
    "section": "Joining sf and tibble dataframe",
    "text": "Joining sf and tibble dataframe\nOne way to manipulate a dataframe is to combine two different sets of data frame together to combine the information retrieved. We will now aggregate the room price of the apartment based on the planning area.\n\n\nmutate: Adds new variables and preserves existing ones. If the new column is referencing an exisiting column, it will replace the variable. Since all the planning area are in uppercase in the mpsz data frame, we will use toupper to convert all the variables inside neighbourhood to uppercase.\n\nfilter: To remove irrelevant rows that are not required for the join.\n\nrename: Rename the column. I will be changing the neighbourhood to PLN_AREA_N to allow both data frame to identify the keys for the join.\n\nsummarise: After grouping the variables through the group_by function, we will summarise it to one row with the average price using the mean function.\n\n\nlistings_tidy <- listings %>%\n  mutate (neighbourhood = toupper(neighbourhood)) %>%\n  filter ((neighbourhood %in% unique(mpsz$PLN_AREA_N))) %>%\n  rename(\"PLN_AREA_N\" = \"neighbourhood\") %>%\n  group_by(PLN_AREA_N) %>%\n  summarise (avgprice = mean(price)) %>%\n  ungroup()\n\n\nmpsz3414 <- mpsz3414 %>%\n  left_join(listings_tidy)"
  },
  {
    "objectID": "posts/Geo/Geospatial_Datawrangling/index.html#geoprocessing-with-sf-package",
    "href": "posts/Geo/Geospatial_Datawrangling/index.html#geoprocessing-with-sf-package",
    "title": "Data Wrangling of Geospatial Data",
    "section": "Geoprocessing with sf package",
    "text": "Geoprocessing with sf package\nBuffering\nIn some cases, there is a need to create a buffering zone along the linestring object. An example would be to expand 5m along a road and understanding the total area increased through the expansion. One way we can do this is to use the st_buffer function that computes a buffer around this geometry/each geometry. To find out the overall area, st_area will be used. If the coordinates are in degrees longtitude/latitude, st_geod_area is used for area calculation.\n\nbuffer_cycling <- st_buffer(cyclingpath, \n                               dist=5, nQuadSegs = 30)\n\nbuffer_cycling$AREA <- st_area(buffer_cycling)\n\nsum(buffer_cycling$AREA)\n\n773143.9 [m^2]\n\n\nVisualising of buffering\nFrom the below visualisation, we are able to better understand how the buffer distance is being calculated and the different endCapStyle to be use for the buffer.\n\ncyclingpath_buffer <- cyclingpath[1,] %>%\n  select (-CYL_PATH_1)\n\nop = par(mfrow=c(2,3))\nplot(st_buffer(cyclingpath_buffer, dist = 1, endCapStyle=\"ROUND\"), reset = FALSE, main = \"endCapStyle: ROUND, distance 1\")\nplot(cyclingpath_buffer,col='blue',add=TRUE)\nplot(st_buffer(cyclingpath_buffer, dist = 2, endCapStyle=\"FLAT\"), reset = FALSE, main = \"endCapStyle: FLAT, distance 2\")\nplot(cyclingpath_buffer,col='blue',add=TRUE)\nplot(st_buffer(cyclingpath_buffer, dist = 3, endCapStyle=\"SQUARE\"), reset = FALSE, main = \"endCapStyle: SQUARE, distance 3\")\nplot(cyclingpath_buffer,col='blue',add=TRUE)\n\n\n\n\n\nmpsz3414$`PreSch Count`<- lengths(st_intersects(mpsz3414, preschool3414))\n\nsummary(mpsz3414$`PreSch Count`)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  0.000   0.000   2.000   4.207   6.000  37.000 \n\ntop_n(mpsz3414, 1, `PreSch Count`)\n\nSimple feature collection with 1 feature and 17 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 23449.05 ymin: 46001.23 xmax: 25594.22 ymax: 47996.47\nProjected CRS: SVY21 / Singapore TM\n  OBJECTID SUBZONE_NO      SUBZONE_N SUBZONE_C CA_IND PLN_AREA_N PLN_AREA_C\n1      290          3 WOODLANDS EAST    WDSZ03      N  WOODLANDS         WD\n      REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR   Y_ADDR\n1 NORTH REGION       NR C90769E43EE6B0F2 2014-12-05 24506.64 46991.63\n  SHAPE_Leng SHAPE_Area avgprice                       geometry PreSch Count\n1   6603.608    2553464 74.53191 MULTIPOLYGON (((24786.75 46...           37"
  },
  {
    "objectID": "posts/Geo/Geospatial_Datawrangling/index.html#plotting-of-geographical-data",
    "href": "posts/Geo/Geospatial_Datawrangling/index.html#plotting-of-geographical-data",
    "title": "Data Wrangling of Geospatial Data",
    "section": "Plotting of geographical data",
    "text": "Plotting of geographical data\n\nmpsz3414$Area <- mpsz3414 %>%\n  st_area()\n\nmpsz3414 <- mpsz3414 %>%\n  mutate(`PreSch Density` = `PreSch Count`/Area * 1000000)\n\nhist(mpsz3414$`PreSch Density`)"
  },
  {
    "objectID": "posts/Geo/Geospatial_Datawrangling/index.html#visualising-of-geographical-data-using-ggplot2",
    "href": "posts/Geo/Geospatial_Datawrangling/index.html#visualising-of-geographical-data-using-ggplot2",
    "title": "Data Wrangling of Geospatial Data",
    "section": "Visualising of geographical data using ggplot2",
    "text": "Visualising of geographical data using ggplot2\n\nggplot(data=mpsz3414, \n       aes(x= as.numeric(`PreSch Density`)))+\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"light blue\") +\n  labs(title = \"Are pre-school even distributed in Singapore?\",\n       subtitle= \"There are many planning sub-zones with a single pre-school, on the other hand, \\nthere are two planning sub-zones with at least 20 pre-schools\",\n      x = \"Pre-school density (per km sq)\",\n      y = \"Frequency\")\n\n\n\n\n\nggplot(data=mpsz3414, \n       aes(x= as.numeric(`PreSch Density`), y = as.numeric(`PreSch Count`)))+\n  geom_point() +\n  labs(title = \"Are pre-school even distributed in Singapore?\",\n      x = \"Pre-school density (per km sq)\",\n      y = \"Pre-school Count\")"
  },
  {
    "objectID": "posts/Geo/Geospatial_Choropleth/index.html#conclusion",
    "href": "posts/Geo/Geospatial_Choropleth/index.html#conclusion",
    "title": "Introduction to Choropleth Mapping",
    "section": "Conclusion",
    "text": "Conclusion"
  },
  {
    "objectID": "posts/Geo/Geospatial_SpatialWeight/index.html",
    "href": "posts/Geo/Geospatial_SpatialWeight/index.html",
    "title": "Introduction to Spatial Weights and Application",
    "section": "",
    "text": "For this analysis, we will use the following packages from CRAN.\n\n\nsf - Support for simple features, a standardized way to encode spatial vector data. Binds to ‘GDAL’ for reading and writing data, to ‘GEOS’ for geometrical operations, and to ‘PROJ’ for projection conversions and datum transformations. Uses by default the ‘s2’ package for spherical geometry operations on ellipsoidal (long/lat) coordinates.\n\ntidyverse - Loading the core tidyverse packages which will be used for data wrangling and visualisation.\n\ntmap - Thematic maps are geographical maps in which spatial data distributions are visualized. This package offers a flexible, layer-based, and easy to use approach to create thematic maps, such as choropleths and bubble maps.\n\nspdep - A collection of functions to create spatial weights matrix objects from polygon ‘contiguities’, from point patterns by distance and tessellations, for summarizing these objects, and for permitting their use in spatial data analysis, including regional aggregation by minimum spanning tree; a collection of tests for spatial ‘autocorrelation’\n\n\npacman::p_load(sf, spdep, tmap, tidyverse)"
  },
  {
    "objectID": "posts/Geo/Geospatial_SpatialWeight/index.html#data-preparation",
    "href": "posts/Geo/Geospatial_SpatialWeight/index.html#data-preparation",
    "title": "Introduction to Spatial Weights and Application",
    "section": "Data Preparation",
    "text": "Data Preparation\nTwo dataset will be used for this study:\n\nHunan.shp: A shapefile of the Hunan Province that consist of all the capital\nHunan.csv: A csv file containing multiple attributes of each capital within Hunan\n\nImporting of data\nWe will use the st_read to import the shape file and read_csv to import the aspatial data into the R environment. We will then use a relational join left_join to combine the spatial and aspatial data together.\n\nhunan <- st_read(dsn = \"data/geospatial\", \n                 layer = \"Hunan\")\n\nhunan2012 <- read_csv(\"data/aspatial/Hunan_2012.csv\")\n\n\nhunan <- left_join(hunan,hunan2012)"
  },
  {
    "objectID": "posts/Geo/Geospatial_SpatialWeight/index.html#visualisation-of-spatial-data",
    "href": "posts/Geo/Geospatial_SpatialWeight/index.html#visualisation-of-spatial-data",
    "title": "Introduction to Spatial Weights and Application",
    "section": "Visualisation of spatial data",
    "text": "Visualisation of spatial data\nFor the visualisation, we will only be using tmap. We will prepare a basemap anbd a choropleth map to visualise the distribution of GDP per capita among the capital.\n\nbasemap <- tm_shape(hunan) +\n  tm_polygons() +\n  tm_text(\"NAME_3\", size=0.4)\n\ngdppc <- qtm(hunan, \"GDPPC\")\n\ntmap_arrange(basemap, gdppc, asp=1, ncol=2)\n\n\n\n\n\nbase map and choropleth map"
  },
  {
    "objectID": "posts/Geo/Geospatial_SpatialWeight/index.html#computing-contiguity-spatial-weights",
    "href": "posts/Geo/Geospatial_SpatialWeight/index.html#computing-contiguity-spatial-weights",
    "title": "Introduction to Spatial Weights and Application",
    "section": "Computing Contiguity Spatial Weights",
    "text": "Computing Contiguity Spatial Weights\nContiguity means that two spatial units share a common border of non-zero length. There are multiple criterion of contiguity such as:\n\n\nRook: When only common sides of the polygons are considered to define the neighbor relation (common vertices are ignored).\n\nQueen: The difference between the rook and queen criterion to determine neighbors is that the latter also includes common vertices.\n\nBishop: Is based on the existence of common vertices between two spatial units.\n\n\n\n\n\nContiguity Weights\n\n\n\n\nExcept in the simplest of circumstances, visual examination or manual calculation cannot be used to create the spatial weights from the geometry of the data. It is necessary to utilize explicit spatial data structures to deal with the placement and layout of the polygons in order to determine whether two polygons are contiguous.\nWe will use the poly2nb function to construct neighbours list based on the regions with contiguous boundaries. Based on the documentation, user will be able to pass a queen argument that takes in True or False. The argument the default is set to TRUE, that is, if you don’t specify queen = FALSE this function will return a list of first order neighbours using the Queen criteria.\nComputing (QUEEN) contiguity based neighbour\nThe code chunk below is used to compute Queen contiguity weight matrix.\n\nwm_q <- poly2nb(hunan)\nsummary(wm_q)\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \nLink number distribution:\n\n 1  2  3  4  5  6  7  8  9 11 \n 2  2 12 16 24 14 11  4  2  1 \n2 least connected regions:\n30 65 with 1 link\n1 most connected region:\n85 with 11 links\n\n\nbased on the summary report above,the report shows that there are 88 area units in Hunan. The most connected area unit has 11 neighbours. There are two area units with only one neighbours.\nFor each polygon in our polygon object, wm_q lists all neighboring polygons. For example, to see the neighbors for the first polygon in the object, type:\n\nwm_q[[1]]\n\n[1]  2  3  4 57 85\n\n\nPolygon 1 has 5 neighbors. The numbers represent the polygon IDs as stored in hunan SpatialPolygonsDataFrame class.\nTo reveal the county names of the five neighboring polygons, the code chunk will be used:\n\nhunan$NAME_3[c(2,3,4,57,85)]\n\n[1] \"Hanshou\" \"Jinshi\"  \"Li\"      \"Nan\"     \"Taoyuan\"\n\n\nWe can retrieve the GDPPC of these five countries by using the code chunk below.\n\nhunan$GDPPC[wm_q[[1]]]\n\n[1] 20981 34592 24473 21311 22879\n\n\nThe printed output above shows that the GDPPC of the five nearest neighbours based on Queen’s method are 20981, 34592, 24473, 21311 and 22879 respectively.\nYou can display the complete weight matrix by using str().\n\nstr(wm_q)\n\nList of 88\n $ : int [1:5] 2 3 4 57 85\n $ : int [1:5] 1 57 58 78 85\n $ : int [1:4] 1 4 5 85\n $ : int [1:4] 1 3 5 6\n $ : int [1:4] 3 4 6 85\n $ : int [1:5] 4 5 69 75 85\n $ : int [1:4] 67 71 74 84\n $ : int [1:7] 9 46 47 56 78 80 86\n $ : int [1:6] 8 66 68 78 84 86\n $ : int [1:8] 16 17 19 20 22 70 72 73\n $ : int [1:3] 14 17 72\n $ : int [1:5] 13 60 61 63 83\n $ : int [1:4] 12 15 60 83\n $ : int [1:3] 11 15 17\n $ : int [1:4] 13 14 17 83\n $ : int [1:5] 10 17 22 72 83\n $ : int [1:7] 10 11 14 15 16 72 83\n $ : int [1:5] 20 22 23 77 83\n $ : int [1:6] 10 20 21 73 74 86\n $ : int [1:7] 10 18 19 21 22 23 82\n $ : int [1:5] 19 20 35 82 86\n $ : int [1:5] 10 16 18 20 83\n $ : int [1:7] 18 20 38 41 77 79 82\n $ : int [1:5] 25 28 31 32 54\n $ : int [1:5] 24 28 31 33 81\n $ : int [1:4] 27 33 42 81\n $ : int [1:3] 26 29 42\n $ : int [1:5] 24 25 33 49 54\n $ : int [1:3] 27 37 42\n $ : int 33\n $ : int [1:8] 24 25 32 36 39 40 56 81\n $ : int [1:8] 24 31 50 54 55 56 75 85\n $ : int [1:5] 25 26 28 30 81\n $ : int [1:3] 36 45 80\n $ : int [1:6] 21 41 47 80 82 86\n $ : int [1:6] 31 34 40 45 56 80\n $ : int [1:4] 29 42 43 44\n $ : int [1:4] 23 44 77 79\n $ : int [1:5] 31 40 42 43 81\n $ : int [1:6] 31 36 39 43 45 79\n $ : int [1:6] 23 35 45 79 80 82\n $ : int [1:7] 26 27 29 37 39 43 81\n $ : int [1:6] 37 39 40 42 44 79\n $ : int [1:4] 37 38 43 79\n $ : int [1:6] 34 36 40 41 79 80\n $ : int [1:3] 8 47 86\n $ : int [1:5] 8 35 46 80 86\n $ : int [1:5] 50 51 52 53 55\n $ : int [1:4] 28 51 52 54\n $ : int [1:5] 32 48 52 54 55\n $ : int [1:3] 48 49 52\n $ : int [1:5] 48 49 50 51 54\n $ : int [1:3] 48 55 75\n $ : int [1:6] 24 28 32 49 50 52\n $ : int [1:5] 32 48 50 53 75\n $ : int [1:7] 8 31 32 36 78 80 85\n $ : int [1:6] 1 2 58 64 76 85\n $ : int [1:5] 2 57 68 76 78\n $ : int [1:4] 60 61 87 88\n $ : int [1:4] 12 13 59 61\n $ : int [1:7] 12 59 60 62 63 77 87\n $ : int [1:3] 61 77 87\n $ : int [1:4] 12 61 77 83\n $ : int [1:2] 57 76\n $ : int 76\n $ : int [1:5] 9 67 68 76 84\n $ : int [1:4] 7 66 76 84\n $ : int [1:5] 9 58 66 76 78\n $ : int [1:3] 6 75 85\n $ : int [1:3] 10 72 73\n $ : int [1:3] 7 73 74\n $ : int [1:5] 10 11 16 17 70\n $ : int [1:5] 10 19 70 71 74\n $ : int [1:6] 7 19 71 73 84 86\n $ : int [1:6] 6 32 53 55 69 85\n $ : int [1:7] 57 58 64 65 66 67 68\n $ : int [1:7] 18 23 38 61 62 63 83\n $ : int [1:7] 2 8 9 56 58 68 85\n $ : int [1:7] 23 38 40 41 43 44 45\n $ : int [1:8] 8 34 35 36 41 45 47 56\n $ : int [1:6] 25 26 31 33 39 42\n $ : int [1:5] 20 21 23 35 41\n $ : int [1:9] 12 13 15 16 17 18 22 63 77\n $ : int [1:6] 7 9 66 67 74 86\n $ : int [1:11] 1 2 3 5 6 32 56 57 69 75 ...\n $ : int [1:9] 8 9 19 21 35 46 47 74 84\n $ : int [1:4] 59 61 62 88\n $ : int [1:2] 59 87\n - attr(*, \"class\")= chr \"nb\"\n - attr(*, \"region.id\")= chr [1:88] \"1\" \"2\" \"3\" \"4\" ...\n - attr(*, \"call\")= language poly2nb(pl = hunan)\n - attr(*, \"type\")= chr \"queen\"\n - attr(*, \"sym\")= logi TRUE\n\n\nComputing (ROOK) contiguity based neighbour\nThe code chunk below is used to compute Rook contiguity weight matrix.\n\nwm_r <- poly2nb(hunan, queen=FALSE)\nsummary(wm_r)\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 440 \nPercentage nonzero weights: 5.681818 \nAverage number of links: 5 \nLink number distribution:\n\n 1  2  3  4  5  6  7  8  9 10 \n 2  2 12 20 21 14 11  3  2  1 \n2 least connected regions:\n30 65 with 1 link\n1 most connected region:\n85 with 10 links\n\n\nThe summary report above shows that there are 88 area units in Hunan. The most connect area unit has 10 neighbours. There are two area units with only one heighbours.\nVisualising the weights matrix\nA connectivity graph takes a point and displays a line to each neighboring point. We are working with polygons at the moment, so we will need to get points in order to make our connectivity graphs. The most typically method for this will be polygon centroids. To retrieve the centroid of each area, we will use the st_centroid function.\n\ncoords <- st_centroid(st_geometry(hunan))\n\nPlotting Queen and Rook contiguity based neighbours map\n\npar(mfrow=c(1,2))\n\nplot(st_geometry(hunan), border=\"grey\", main = \"Queen Contiguity\")\nplot(wm_q, coords,pch = 19, cex = 0.6, add = TRUE, col= \"red\")\n\nplot(st_geometry(hunan), border=\"grey\", main = \"Rook Contiguity\")\nplot(wm_r, coords,pch = 19, cex = 0.6, add = TRUE, col= \"blue\")"
  },
  {
    "objectID": "posts/Geo/Geospatial_SpatialWeight/index.html#computing-distance-based-neighbours",
    "href": "posts/Geo/Geospatial_SpatialWeight/index.html#computing-distance-based-neighbours",
    "title": "Introduction to Spatial Weights and Application",
    "section": "Computing distance based neighbours",
    "text": "Computing distance based neighbours\nIn this section, you will learn how to derive distance-based weight matrices by using dnearneigh() of spdep package. The function identifies neighbours of region points by Euclidean distance in the metric of the points between lower (greater than or equal to and upper (less than or equal to) bounds.\nDetermine the cut-off distance\nFirstly, we need to determine the upper limit for distance band by using the steps below:\n\nReturn a matrix with the indices of points belonging to the set of the k nearest neighbours of each other by using knearneigh() of spdep.\nConvert the knn object returned by knearneigh() into a neighbours list of class nb with a list of integer vectors containing neighbour region number ids by using knn2nb().\nReturn the length of neighbour relationship edges by using nbdists() of spdep. The function returns in the units of the coordinates if the coordinates are projected, in km otherwise.\nRemove the list structure of the returned object by using unlist().\n\n\nk1 <- knn2nb(knearneigh(coords))\nk1dists <- unlist(nbdists(k1, coords))\nsummary(k1dists)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  24.80   32.53   38.06   38.91   44.66   58.25 \n\n\nThe summary report shows that the largest first nearest neighbour distance is 58.25 km, so using this as the upper threshold gives certainty that all units will have at least one neighbour.\nComputing fixed distance weight matrix\nNow, we will compute the distance weight matrix by using dnearneigh() as shown in the code chunk below.\n\nwm_d62 <- dnearneigh(coords, 0, 59)\nwm_d62\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 298 \nPercentage nonzero weights: 3.84814 \nAverage number of links: 3.386364 \n\n\nThe report shows that on average, every area should have at least 3 neighbours (links).\nTo display the structure of the weight matrix is to combine table() and card() of spdep.\n\ntable(hunan$County, card(wm_d62))\n\n               \n                1 2 3 4 5 6\n  Anhua         1 0 0 0 0 0\n  Anren         0 0 0 1 0 0\n  Anxiang       0 0 0 0 1 0\n  Baojing       0 0 0 1 0 0\n  Chaling       0 0 1 0 0 0\n  Changning     0 0 1 0 0 0\n  Changsha      0 0 0 1 0 0\n  Chengbu       1 0 0 0 0 0\n  Chenxi        0 0 0 1 0 0\n  Cili          1 0 0 0 0 0\n  Dao           0 0 1 0 0 0\n  Dongan        0 0 1 0 0 0\n  Dongkou       0 0 0 1 0 0\n  Fenghuang     0 0 0 1 0 0\n  Guidong       0 0 1 0 0 0\n  Guiyang       0 0 1 0 0 0\n  Guzhang       0 0 0 0 0 1\n  Hanshou       0 0 0 1 0 0\n  Hengdong      0 0 1 0 0 0\n  Hengnan       0 0 0 0 1 0\n  Hengshan      0 0 0 0 0 1\n  Hengyang      0 0 0 0 1 0\n  Hongjiang     0 0 0 1 0 0\n  Huarong       0 0 1 0 0 0\n  Huayuan       0 0 0 1 0 0\n  Huitong       0 0 0 1 0 0\n  Jiahe         0 0 0 0 1 0\n  Jianghua      0 1 0 0 0 0\n  Jiangyong     0 1 0 0 0 0\n  Jingzhou      0 1 0 0 0 0\n  Jinshi        0 0 0 1 0 0\n  Jishou        0 0 0 0 0 1\n  Lanshan       0 0 0 1 0 0\n  Leiyang       0 0 0 1 0 0\n  Lengshuijiang 0 0 1 0 0 0\n  Li            0 0 1 0 0 0\n  Lianyuan      0 0 0 1 0 0\n  Liling        0 1 0 0 0 0\n  Linli         0 0 1 0 0 0\n  Linwu         0 0 0 1 0 0\n  Linxiang      1 0 0 0 0 0\n  Liuyang       0 1 0 0 0 0\n  Longhui       0 0 1 0 0 0\n  Longshan      1 0 0 0 0 0\n  Luxi          0 0 0 0 1 0\n  Mayang        0 0 0 0 0 1\n  Miluo         0 0 0 0 1 0\n  Nan           0 0 0 0 1 0\n  Ningxiang     0 0 0 1 0 0\n  Ningyuan      0 0 0 0 1 0\n  Pingjiang     0 1 0 0 0 0\n  Qidong        0 0 1 0 0 0\n  Qiyang        0 0 1 0 0 0\n  Rucheng       0 1 0 0 0 0\n  Sangzhi       1 0 0 0 0 0\n  Shaodong      0 0 0 0 1 0\n  Shaoshan      0 0 0 1 0 0\n  Shaoyang      0 0 0 1 0 0\n  Shimen        1 0 0 0 0 0\n  Shuangfeng    0 0 0 0 1 0\n  Shuangpai     0 0 0 1 0 0\n  Suining       0 0 0 1 0 0\n  Taojiang      0 1 0 0 0 0\n  Taoyuan       1 0 0 0 0 0\n  Tongdao       1 0 0 0 0 0\n  Wangcheng     0 0 0 1 0 0\n  Wugang        0 0 1 0 0 0\n  Xiangtan      0 0 0 1 0 0\n  Xiangxiang    0 0 0 0 1 0\n  Xiangyin      0 0 0 1 0 0\n  Xinhua        0 0 0 1 0 0\n  Xinhuang      1 0 0 0 0 0\n  Xinning       0 1 0 0 0 0\n  Xinshao       0 0 0 0 0 1\n  Xintian       0 0 0 0 1 0\n  Xupu          0 1 0 0 0 0\n  Yanling       0 0 1 0 0 0\n  Yizhang       1 0 0 0 0 0\n  Yongshun      0 0 0 1 0 0\n  Yongxing      0 0 1 0 0 0\n  You           0 0 1 0 0 0\n  Yuanjiang     0 0 0 0 1 0\n  Yuanling      1 0 0 0 0 0\n  Yueyang       0 1 0 0 0 0\n  Zhijiang      0 0 0 0 1 0\n  Zhongfang     0 0 0 1 0 0\n  Zhuzhou       0 0 1 0 0 0\n  Zixing        0 0 1 0 0 0\n\n\nVisualising distance weight matrix\nThe left graph with the red lines show the links of 1st nearest neighbours and the right graph with the black lines show the links of neighbours within the cut-off distance of 59km.\n\npar(mfrow=c(1,2))\nplot(hunan$geometry, border=\"lightgrey\", main=\"1st nearest neighbours\")\nplot(k1, coords, add=TRUE, col=\"red\", length=0.08)\nplot(hunan$geometry, border=\"lightgrey\", main=\"Distance link\")\nplot(wm_d62, coords, add=TRUE, pch = 19, cex = 0.6)\n\n\n\n\nAdaptive distance weight matrix\nOther than using distance as a criteria to decide the neighbours, it is possible to control the numbers of neighbours directly using k-nearest neighbours, either accepting asymmetric neighbours or imposing symmetry as shown in the code chunk below:\n\nknn6 <- knn2nb(knearneigh(coords, k=6)) #k refers to the number of neighbours per area\nknn6\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 528 \nPercentage nonzero weights: 6.818182 \nAverage number of links: 6 \nNon-symmetric neighbours list\n\n\nPlotting adaptive distance weight\nWe can plot the adaptive distance weight matrix using the code chunk below:\n\nplot(hunan$geometry, border=\"lightgrey\", main = \"Adaptive Distance Weight\")\nplot(knn6, coords, pch = 19, cex = 0.6, add = TRUE, col = \"red\")\n\n\n\n\nWeights based on Inversed Distance Weighting (IDW)\nIn this section, you will learn how to derive a spatial weight matrix based on Inversed Distance method.\nFirst, we will compute the distances between areas by using nbdists() of spdep.\nWe will use the [lapply()] (https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/lapply) to apply the inverse function through the list.\n\ndist <- nbdists(wm_q, coords, longlat = TRUE)\nids <- lapply(dist, function(x) 1/(x))\nhead(ids,5)\n\n[[1]]\n[1] 1.694446 3.805533 1.847048 2.867007 1.166097\n\n[[2]]\n[1] 1.694446 1.832614 1.889267 2.537233 1.681209\n\n[[3]]\n[1] 3.805533 3.007305 3.588446 1.468997\n\n[[4]]\n[1] 1.847048 3.007305 3.731278 1.490622\n\n[[5]]\n[1] 3.588446 3.731278 1.526472 1.775459\n\n\nNext, we will use the nb2listw to apply the weights list with values given by the coding scheme style chosen. There are multiple style to choose from:\n\nB (Basic Binary Coding)\nW (Row Standardised) - sums over all links to n\nC (Globally Standardised) - sums over all links to n\nU (Globally Standardised / No of neighbours) - sums over all links to unity\nS (Variance-Stabilizing Coding Scheme) - sums over all links to n\nminmax - divides the weights by the minimum of the maximum row sums and maximum column sums of the input weights\n\nFor the simplifed analysis, we will use the W (Row Standardised).\n\nrswm_q <- nb2listw(wm_q, style=\"W\", zero.policy = TRUE)\nrswm_q\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \n\nWeights style: W \nWeights constants summary:\n   n   nn S0       S1       S2\nW 88 7744 88 37.86334 365.9147\n\n\nFrom the earlier example, we know that the first Id has 5 neighbours. We take a look at the weight distribution of these 5 neighours. Since we are using Row Standardised, they should be equal.\n\nrswm_q$weights[1]\n\n[[1]]\n[1] 0.2 0.2 0.2 0.2 0.2\n\n\nEach neighbor is assigned a 0.2 of the total weight. This means that when R computes the average neighboring income values, each neighbor’s income will be multiplied by 0.2 before being tallied."
  },
  {
    "objectID": "posts/Geo/Geospatial_SpatialWeight/index.html#application-of-spatial-weight-matrix",
    "href": "posts/Geo/Geospatial_SpatialWeight/index.html#application-of-spatial-weight-matrix",
    "title": "Introduction to Spatial Weights and Application",
    "section": "Application of Spatial Weight Matrix",
    "text": "Application of Spatial Weight Matrix"
  }
]