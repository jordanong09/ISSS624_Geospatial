[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Ong Zhi Rong Jordan",
    "section": "",
    "text": "Currently a full-time servicemen, assuming an appointment in Training & Doctrine Command (TRADOC)."
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "Ong Zhi Rong Jordan",
    "section": "Education",
    "text": "Education\nNanyang Technological University - NTU | Singapore Bachelor of Engineering in Computer Engineering | Aug 2013 - Aug 2016\nSingapore Management University - SMU | Singapore Master of IT in Business (MITB)| Jan 2022 - Nov 2022 (Ongoing)"
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "Ong Zhi Rong Jordan",
    "section": "Experience",
    "text": "Experience\nNil"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Bringing Data to Life",
    "section": "",
    "text": "Geospatial analytics hold tremendous potential to address complex problems facing society. In this study, you are tasked to apply appropriate global and local measures of spatial Association techniques to reveals the spatial patterns of Not Functional water points. For the purpose of this study, Nigeria will be used as the study country.\n\n\n\ngeospatial\n\n\nsf\n\n\nspdep\n\n\ntmap\n\n\n\n\nOng Zhi Rong Jordan\n\n\nNovember 25, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDescribing the presence of systematic spatial variation in a variable. “The first law of geography Everything is related to everything else, but near things are more related than distant things.” Waldo R. Tobler (Tobler, Waldo R. 1970)\n\n\n\ngeospatial\n\n\nsf\n\n\nspdep\n\n\ntmap\n\n\n\n\nOng Zhi Rong Jordan\n\n\nNovember 24, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nComputation of spatial weights using R. Understanding the spatial relationships that exist among the features in the dataset.\n\n\n\ngeospatial\n\n\nsf\n\n\nspdep\n\n\ntmap\n\n\n\n\nOng Zhi Rong Jordan\n\n\nNovember 24, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUtilising the different libraries such as ggplot, tmap and leaflet to visualise geographical data.\n\n\n\ngeospatial\n\n\nsf\n\n\nggplot\n\n\ntmap\n\n\nleaflet\n\n\n\n\nOng Zhi Rong Jordan\n\n\nNovember 23, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUtilising the sf and tidyverse packages to tidy geospatial data.\n\n\n\ngeospatial\n\n\nsf\n\n\n\n\nOng Zhi Rong Jordan\n\n\nNovember 19, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLeveraging on simple data wrangling techniques to create a RFM model. Subsequently, leveraging on unsupervised classification to conduct customer segmentation for targeted marketing.\n\n\n\ntibble\n\n\nclustering\n\n\nunsupervised\n\n\n\n\nOng Zhi Rong Jordan\n\n\nJune 25, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUsing Static, Interactive and Statistical (SIS) Graphs to reveal the demographics and relationships of a city.\n\n\n\ntibble\n\n\nggstatsplot\n\n\n\n\nOng Zhi Rong Jordan\n\n\nJune 8, 2022\n\n\n\n\n\n\n\n\nNo matching items\n\n\n  \n\n\n See all"
  },
  {
    "objectID": "index.html#posts-of-my-own-works",
    "href": "index.html#posts-of-my-own-works",
    "title": "Bringing Data to Life",
    "section": "Posts of my own works",
    "text": "Posts of my own works\n\n\n\n\n  \n\n\n\n\nRFM Model\n\n\nLeveraging on simple data wrangling techniques to create a RFM model. Subsequently, leveraging on unsupervised classification to conduct customer segmentation for targeted marketing.\n\n\n\n\ntibble\n\n\nclustering\n\n\nunsupervised\n\n\n \n\n\n\n\nJune 25, 2022\n\n\nOng Zhi Rong Jordan\n\n\n\n\n\n\n  \n\n\n\n\nSIS Visual Representation\n\n\nUsing Static, Interactive and Statistical (SIS) Graphs to reveal the demographics and relationships of a city.\n\n\n\n\ntibble\n\n\nggstatsplot\n\n\n \n\n\n\n\nJune 8, 2022\n\n\nOng Zhi Rong Jordan\n\n\n\n\n\n\nNo matching items\n\n\n See all posts of my work"
  },
  {
    "objectID": "index.html#posts-from-geospatial-analytics",
    "href": "index.html#posts-from-geospatial-analytics",
    "title": "Bringing Data to Life",
    "section": "Posts from Geospatial Analytics",
    "text": "Posts from Geospatial Analytics\n\n\n\n\n  \n\n\n\n\nGeospatial Analytics for Social Good - Understanding Nigeria Water functional and non-functional water point rate\n\n\nGeospatial analytics hold tremendous potential to address complex problems facing society. In this study, you are tasked to apply appropriate global and local measures of spatial Association techniques to reveals the spatial patterns of Not Functional water points. For the purpose of this study, Nigeria will be used as the study country.\n\n\n\n\ngeospatial\n\n\nsf\n\n\nspdep\n\n\ntmap\n\n\n \n\n\n\n\nNovember 25, 2022\n\n\nOng Zhi Rong Jordan\n\n\n\n\n\n\n  \n\n\n\n\nIntroduction to Global and Local Measures of Spatial Autocorrelation\n\n\nDescribing the presence of systematic spatial variation in a variable. “The first law of geography Everything is related to everything else, but near things are more related than distant things.” Waldo R. Tobler (Tobler, Waldo R. 1970)\n\n\n\n\ngeospatial\n\n\nsf\n\n\nspdep\n\n\ntmap\n\n\n \n\n\n\n\nNovember 24, 2022\n\n\nOng Zhi Rong Jordan\n\n\n\n\n\n\n  \n\n\n\n\nIntroduction to Spatial Weights and Application\n\n\nComputation of spatial weights using R. Understanding the spatial relationships that exist among the features in the dataset.\n\n\n\n\ngeospatial\n\n\nsf\n\n\nspdep\n\n\ntmap\n\n\n \n\n\n\n\nNovember 24, 2022\n\n\nOng Zhi Rong Jordan\n\n\n\n\n\n\n  \n\n\n\n\nIntroduction to Choropleth Mapping\n\n\nUtilising the different libraries such as ggplot, tmap and leaflet to visualise geographical data.\n\n\n\n\ngeospatial\n\n\nsf\n\n\nggplot\n\n\ntmap\n\n\nleaflet\n\n\n \n\n\n\n\nNovember 23, 2022\n\n\nOng Zhi Rong Jordan\n\n\n\n\n\n\n  \n\n\n\n\nData Wrangling of Geospatial Data\n\n\nUtilising the sf and tidyverse packages to tidy geospatial data.\n\n\n\n\ngeospatial\n\n\nsf\n\n\n \n\n\n\n\nNovember 19, 2022\n\n\nOng Zhi Rong Jordan\n\n\n\n\n\n\nNo matching items\n\n\n  \n\n\n See all posts of Geospatial"
  },
  {
    "objectID": "index.html#posts-from-visual-analytics",
    "href": "index.html#posts-from-visual-analytics",
    "title": "Bringing Data to Life",
    "section": "Posts from Visual Analytics",
    "text": "Posts from Visual Analytics\n\n\n\n\n\nNo matching items\n\n\n See all posts of Visual"
  },
  {
    "objectID": "posts/Geo/Geospatial_HOE1/index.html",
    "href": "posts/Geo/Geospatial_HOE1/index.html",
    "title": "Introduction to Choropleth Mapping",
    "section": "",
    "text": "For this analysis, we will use the following packages from CRAN.\nsf - Support for simple features, a standardized way to encode spatial vector data. Binds to ‘GDAL’ for reading and writing data, to ‘GEOS’ for geometrical operations, and to ‘PROJ’ for projection conversions and datum transformations. Uses by default the ‘s2’ package for spherical geometry operations on ellipsoidal (long/lat) coordinates. tidyverse - Loading the core tidyverse packages which will be used for data wrangling and visualisation. tmap - Thematic maps are geographical maps in which spatial data distributions are visualized. This package offers a flexible, layer-based, and easy to use approach to create thematic maps, such as choropleths and bubble maps.\n\npacman::p_load(sf, tidyverse, tmap)"
  },
  {
    "objectID": "posts/Geo/Geospatial_HOE1/index.html#dataset",
    "href": "posts/Geo/Geospatial_HOE1/index.html#dataset",
    "title": "Introduction to Choropleth Mapping",
    "section": "Dataset",
    "text": "Dataset\nFor this analysis, we will extract data that is available from the web.\n\nMaster Plan 2014 Subzone Boundary (Web) from data.gov.sg\nPre-Schools Location from data.gov.sg\nCycling Path from LTADataMall\nLatest version of Singapore Airbnb listing data from Inside Airbnb\n\nExtracting geographical information from Dataset\nWe will leverage on the st_read function to retrieve polygon, line and point feature in both ESRI shapefile and KML format.\n\nmpsz = st_read(dsn = \"data/geospatial\", \n                  layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `C:\\jordanong09\\ISSS624_Geospatial\\posts\\Geo\\Geospatial_HOE1\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\ncyclingpath = st_read(dsn = \"data/geospatial\", \n                      layer = \"CyclingPath\")\n\nReading layer `CyclingPath' from data source \n  `C:\\jordanong09\\ISSS624_Geospatial\\posts\\Geo\\Geospatial_HOE1\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 1625 features and 2 fields\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: 12711.19 ymin: 28711.33 xmax: 42626.09 ymax: 48948.15\nProjected CRS: SVY21\n\npreschool = st_read(\"data/geospatial/pre-schools-location-kml.kml\")\n\nReading layer `PRESCHOOLS_LOCATION' from data source \n  `C:\\jordanong09\\ISSS624_Geospatial\\posts\\Geo\\Geospatial_HOE1\\data\\geospatial\\pre-schools-location-kml.kml' \n  using driver `KML'\nSimple feature collection with 1359 features and 2 fields\nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 103.6824 ymin: 1.248403 xmax: 103.9897 ymax: 1.462134\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84\n\nst_geometry(mpsz)\n\nGeometry set for 323 features \nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\nFirst 5 geometries:\n\nglimpse(mpsz)\n\nRows: 323\nColumns: 16\n$ OBJECTID   <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, …\n$ SUBZONE_NO <int> 1, 1, 3, 8, 3, 7, 9, 2, 13, 7, 12, 6, 1, 5, 1, 1, 3, 2, 2, …\n$ SUBZONE_N  <chr> \"MARINA SOUTH\", \"PEARL'S HILL\", \"BOAT QUAY\", \"HENDERSON HIL…\n$ SUBZONE_C  <chr> \"MSSZ01\", \"OTSZ01\", \"SRSZ03\", \"BMSZ08\", \"BMSZ03\", \"BMSZ07\",…\n$ CA_IND     <chr> \"Y\", \"Y\", \"Y\", \"N\", \"N\", \"N\", \"N\", \"Y\", \"N\", \"N\", \"N\", \"N\",…\n$ PLN_AREA_N <chr> \"MARINA SOUTH\", \"OUTRAM\", \"SINGAPORE RIVER\", \"BUKIT MERAH\",…\n$ PLN_AREA_C <chr> \"MS\", \"OT\", \"SR\", \"BM\", \"BM\", \"BM\", \"BM\", \"SR\", \"QT\", \"QT\",…\n$ REGION_N   <chr> \"CENTRAL REGION\", \"CENTRAL REGION\", \"CENTRAL REGION\", \"CENT…\n$ REGION_C   <chr> \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\",…\n$ INC_CRC    <chr> \"5ED7EB253F99252E\", \"8C7149B9EB32EEFC\", \"C35FEFF02B13E0E5\",…\n$ FMEL_UPD_D <date> 2014-12-05, 2014-12-05, 2014-12-05, 2014-12-05, 2014-12-05…\n$ X_ADDR     <dbl> 31595.84, 28679.06, 29654.96, 26782.83, 26201.96, 25358.82,…\n$ Y_ADDR     <dbl> 29220.19, 29782.05, 29974.66, 29933.77, 30005.70, 29991.38,…\n$ SHAPE_Leng <dbl> 5267.381, 3506.107, 1740.926, 3313.625, 2825.594, 4428.913,…\n$ SHAPE_Area <dbl> 1630379.27, 559816.25, 160807.50, 595428.89, 387429.44, 103…\n$ geometry   <MULTIPOLYGON [m]> MULTIPOLYGON (((31495.56 30..., MULTIPOLYGON (…\n\nhead(mpsz, n=5)  \n\nSimple feature collection with 5 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 25867.68 ymin: 28369.47 xmax: 32362.39 ymax: 30435.54\nProjected CRS: SVY21\n  OBJECTID SUBZONE_NO      SUBZONE_N SUBZONE_C CA_IND      PLN_AREA_N\n1        1          1   MARINA SOUTH    MSSZ01      Y    MARINA SOUTH\n2        2          1   PEARL'S HILL    OTSZ01      Y          OUTRAM\n3        3          3      BOAT QUAY    SRSZ03      Y SINGAPORE RIVER\n4        4          8 HENDERSON HILL    BMSZ08      N     BUKIT MERAH\n5        5          3        REDHILL    BMSZ03      N     BUKIT MERAH\n  PLN_AREA_C       REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR\n1         MS CENTRAL REGION       CR 5ED7EB253F99252E 2014-12-05 31595.84\n2         OT CENTRAL REGION       CR 8C7149B9EB32EEFC 2014-12-05 28679.06\n3         SR CENTRAL REGION       CR C35FEFF02B13E0E5 2014-12-05 29654.96\n4         BM CENTRAL REGION       CR 3775D82C5DDBEFBD 2014-12-05 26782.83\n5         BM CENTRAL REGION       CR 85D9ABEF0A40678F 2014-12-05 26201.96\n    Y_ADDR SHAPE_Leng SHAPE_Area                       geometry\n1 29220.19   5267.381  1630379.3 MULTIPOLYGON (((31495.56 30...\n2 29782.05   3506.107   559816.2 MULTIPOLYGON (((29092.28 30...\n3 29974.66   1740.926   160807.5 MULTIPOLYGON (((29932.33 29...\n4 29933.77   3313.625   595428.9 MULTIPOLYGON (((27131.28 30...\n5 30005.70   2825.594   387429.4 MULTIPOLYGON (((26451.03 30..."
  },
  {
    "objectID": "posts/Geo/Geospatial_HOE1/index.html#plotting-of-map",
    "href": "posts/Geo/Geospatial_HOE1/index.html#plotting-of-map",
    "title": "Introduction to Choropleth Mapping",
    "section": "Plotting of map",
    "text": "Plotting of map\n\nplot(mpsz)\n\n\n\nplot(st_geometry(mpsz))\n\n\n\nplot(mpsz[\"PLN_AREA_N\"])"
  },
  {
    "objectID": "posts/Geo/Geospatial_HOE1/index.html#changing-of-projection",
    "href": "posts/Geo/Geospatial_HOE1/index.html#changing-of-projection",
    "title": "Introduction to Choropleth Mapping",
    "section": "Changing of Projection",
    "text": "Changing of Projection\n\nmpsz3414 <- st_set_crs(mpsz, 3414)\n\npreschool3414 <- st_transform(preschool, \n                              crs = 3414)"
  },
  {
    "objectID": "posts/Geo/Geospatial_HOE1/index.html#importing-and-converting-an-aspatial-data",
    "href": "posts/Geo/Geospatial_HOE1/index.html#importing-and-converting-an-aspatial-data",
    "title": "Introduction to Choropleth Mapping",
    "section": "Importing and Converting An Aspatial Data",
    "text": "Importing and Converting An Aspatial Data\n\nlistings <- read_csv(\"data/aspatial/listings.csv\")\nlist(listings) \n\n[[1]]\n# A tibble: 4,252 × 16\n       id name         host_id host_name neighbourhood_g… neighbourhood latitude\n    <dbl> <chr>          <dbl> <chr>     <chr>            <chr>            <dbl>\n 1  50646 Pleasant Ro…  227796 Sujatha   Central Region   Bukit Timah       1.33\n 2  71609 Ensuite Roo…  367042 Belinda   East Region      Tampines          1.35\n 3  71896 B&B  Room 1…  367042 Belinda   East Region      Tampines          1.35\n 4  71903 Room 2-near…  367042 Belinda   East Region      Tampines          1.35\n 5 275343 Convenientl… 1439258 Joyce     Central Region   Bukit Merah       1.29\n 6 275344 15 mins to … 1439258 Joyce     Central Region   Bukit Merah       1.29\n 7 294281 5 mins walk… 1521514 Elizabeth Central Region   Newton            1.31\n 8 301247 Nice room w… 1552002 Rahul     Central Region   Geylang           1.32\n 9 324945 20 Mins to … 1439258 Joyce     Central Region   Bukit Merah       1.29\n10 330089 Accomo@ RED… 1439258 Joyce     Central Region   Bukit Merah       1.29\n# … with 4,242 more rows, and 9 more variables: longitude <dbl>,\n#   room_type <chr>, price <dbl>, minimum_nights <dbl>,\n#   number_of_reviews <dbl>, last_review <date>, reviews_per_month <dbl>,\n#   calculated_host_listings_count <dbl>, availability_365 <dbl>\n\n\n\nlistings_sf <- st_as_sf(listings, \n                       coords = c(\"longitude\", \"latitude\"),\n                       crs=4326) %>%\n  st_transform(crs = 3414)\n\nglimpse(listings_sf)\n\nRows: 4,252\nColumns: 15\n$ id                             <dbl> 50646, 71609, 71896, 71903, 275343, 275…\n$ name                           <chr> \"Pleasant Room along Bukit Timah\", \"Ens…\n$ host_id                        <dbl> 227796, 367042, 367042, 367042, 1439258…\n$ host_name                      <chr> \"Sujatha\", \"Belinda\", \"Belinda\", \"Belin…\n$ neighbourhood_group            <chr> \"Central Region\", \"East Region\", \"East …\n$ neighbourhood                  <chr> \"Bukit Timah\", \"Tampines\", \"Tampines\", …\n$ room_type                      <chr> \"Private room\", \"Private room\", \"Privat…\n$ price                          <dbl> 80, 178, 81, 81, 52, 40, 72, 41, 49, 49…\n$ minimum_nights                 <dbl> 90, 90, 90, 90, 14, 14, 90, 8, 14, 14, …\n$ number_of_reviews              <dbl> 18, 20, 24, 48, 20, 13, 133, 105, 14, 1…\n$ last_review                    <date> 2014-07-08, 2019-12-28, 2014-12-10, 20…\n$ reviews_per_month              <dbl> 0.22, 0.28, 0.33, 0.67, 0.20, 0.16, 1.2…\n$ calculated_host_listings_count <dbl> 1, 4, 4, 4, 50, 50, 7, 1, 50, 50, 50, 4…\n$ availability_365               <dbl> 365, 365, 365, 365, 353, 364, 365, 90, …\n$ geometry                       <POINT [m]> POINT (22646.02 35167.9), POINT (…\n\n\n\nbuffer_cycling <- st_buffer(cyclingpath, \n                               dist=5, nQuadSegs = 30)\n\nbuffer_cycling$AREA <- st_area(buffer_cycling)\n\nsum(buffer_cycling$AREA)\n\n773143.9 [m^2]\n\n\n\nmpsz3414$`PreSch Count`<- lengths(st_intersects(mpsz3414, preschool3414))\n\nsummary(mpsz3414$`PreSch Count`)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  0.000   0.000   2.000   4.207   6.000  37.000 \n\ntop_n(mpsz3414, 1, `PreSch Count`)\n\nSimple feature collection with 1 feature and 16 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 23449.05 ymin: 46001.23 xmax: 25594.22 ymax: 47996.47\nProjected CRS: SVY21 / Singapore TM\n  OBJECTID SUBZONE_NO      SUBZONE_N SUBZONE_C CA_IND PLN_AREA_N PLN_AREA_C\n1      290          3 WOODLANDS EAST    WDSZ03      N  WOODLANDS         WD\n      REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR   Y_ADDR\n1 NORTH REGION       NR C90769E43EE6B0F2 2014-12-05 24506.64 46991.63\n  SHAPE_Leng SHAPE_Area                       geometry PreSch Count\n1   6603.608    2553464 MULTIPOLYGON (((24786.75 46...           37"
  },
  {
    "objectID": "posts/Geo/Geospatial_HOE1/index.html#plotting-of-geographical-data",
    "href": "posts/Geo/Geospatial_HOE1/index.html#plotting-of-geographical-data",
    "title": "Introduction to Choropleth Mapping",
    "section": "Plotting of geographical data",
    "text": "Plotting of geographical data\n\nmpsz3414$Area <- mpsz3414 %>%\n  st_area()\n\nmpsz3414 <- mpsz3414 %>%\n  mutate(`PreSch Density` = `PreSch Count`/Area * 1000000)\n\nhist(mpsz3414$`PreSch Density`)"
  },
  {
    "objectID": "posts/Geo/Geospatial_HOE1/index.html#visualising-of-geographical-data-using-ggplot2",
    "href": "posts/Geo/Geospatial_HOE1/index.html#visualising-of-geographical-data-using-ggplot2",
    "title": "Introduction to Choropleth Mapping",
    "section": "Visualising of geographical data using ggplot2",
    "text": "Visualising of geographical data using ggplot2\n\nggplot(data=mpsz3414, \n       aes(x= as.numeric(`PreSch Density`)))+\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"light blue\") +\n  labs(title = \"Are pre-school even distributed in Singapore?\",\n       subtitle= \"There are many planning sub-zones with a single pre-school, on the other hand, \\nthere are two planning sub-zones with at least 20 pre-schools\",\n      x = \"Pre-school density (per km sq)\",\n      y = \"Frequency\")\n\n\n\n\n\nggplot(data=mpsz3414, \n       aes(x= as.numeric(`PreSch Density`), y = as.numeric(`PreSch Count`)))+\n  geom_point() +\n  labs(title = \"Are pre-school even distributed in Singapore?\",\n      x = \"Pre-school density (per km sq)\",\n      y = \"Pre-school Count\")"
  },
  {
    "objectID": "posts/Geo/Geospatial_HOE1/index.html#choropleth-mapping-with-r",
    "href": "posts/Geo/Geospatial_HOE1/index.html#choropleth-mapping-with-r",
    "title": "Introduction to Choropleth Mapping",
    "section": "Choropleth Mapping with R",
    "text": "Choropleth Mapping with R\n\npopdata <- read_csv(\"data/aspatial/respopagesextod2011to2020.csv\")\n\n\npopdata2020 <- popdata %>%\n  filter(Time == 2020) %>%\n  group_by(PA, SZ, AG) %>%\n  summarise(`POP` = sum(`Pop`)) %>%\n  ungroup()%>%\n  pivot_wider(names_from=AG, \n              values_from=POP) %>%\n  mutate(YOUNG = rowSums(.[3:6])\n         +rowSums(.[12])) %>%\nmutate(`ECONOMY ACTIVE` = rowSums(.[7:11])+\nrowSums(.[13:15]))%>%\nmutate(`AGED`=rowSums(.[16:21])) %>%\nmutate(`TOTAL`=rowSums(.[3:21])) %>%  \nmutate(`DEPENDENCY` = (`YOUNG` + `AGED`)\n/`ECONOMY ACTIVE`) %>%\n  select(`PA`, `SZ`, `YOUNG`, \n       `ECONOMY ACTIVE`, `AGED`, \n       `TOTAL`, `DEPENDENCY`)\n\n\npopdata2020 <- popdata2020 %>%\n  mutate_at(.vars = vars(PA, SZ), \n          .funs = funs(toupper)) %>%\n  filter(`ECONOMY ACTIVE` > 0)\n\nmpsz_pop2020 <- left_join(mpsz, popdata2020,\n                          by = c(\"SUBZONE_N\" = \"SZ\"))\n\nwrite_rds(mpsz_pop2020, \"data/rds/mpszpop2020.rds\")\n\n\ntmap_mode(\"plot\")\nqtm(mpsz_pop2020, \n    fill = \"DEPENDENCY\")\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"Dependency ratio\") +\n  tm_layout(main.title = \"Distribution of Dependency Ratio by planning subzone\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar() +\n  tm_grid(alpha =0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\\n and Population data from Department of Statistics DOS\", \n             position = c(\"left\", \"bottom\"))\n\n\n\n\n\ntm_shape(mpsz_pop2020) +\n  tm_polygons()\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\")\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\") +\n  tm_borders(lwd = 0.1,  alpha = 1)\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"jenks\") +\n  tm_borders(alpha = 0.5)\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"equal\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\nDIY using quantile and 4 classes.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 4,\n          style = \"quantile\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\nsummary(mpsz_pop2020$DEPENDENCY)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n 0.0000  0.6519  0.7025  0.7742  0.7645 19.0000      92 \n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          breaks = c(0, 0.60, 0.70, 0.80, 0.90, 1.00)) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 6,\n          style = \"quantile\",\n          palette = \"Blues\") +\n  tm_borders(alpha = 0.5)\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          style = \"quantile\",\n          palette = \"-Greens\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"jenks\", \n          palette = \"Blues\", \n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(main.title = \"Distribution of Dependency Ratio by planning subzone \\n(Jenks classification)\",\n            main.title.position = \"center\",\n            main.title.size = 1,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            legend.outside = FALSE,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = FALSE) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"-Greens\") +\n  tm_borders(alpha = 0.5) +\n  tmap_style(\"classic\")\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"No. of persons\") +\n  tm_layout(main.title = \"Distribution of Dependency Ratio \\nby planning subzone\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar(width = 0.15) +\n  tm_grid(lwd = 0.1, alpha = 0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\\n and Population data from Department of Statistics DOS\", \n             position = c(\"left\", \"bottom\")) +\n  tmap_style(\"white\")\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(c(\"YOUNG\", \"AGED\"),\n          style = \"equal\", \n          palette = \"Blues\") +\n  tm_layout(legend.position = c(\"right\", \"bottom\")) +\n  tm_borders(alpha = 0.5) +\n  tmap_style(\"white\")\n\n\n\ntm_shape(mpsz_pop2020)+ \n  tm_polygons(c(\"DEPENDENCY\",\"AGED\"),\n          style = c(\"equal\", \"quantile\"), \n          palette = list(\"Blues\",\"Greens\")) +\n  tm_layout(legend.position = c(\"right\", \"bottom\"))\n\n\n\n\n\ntm_shape(mpsz_pop2020) +\n  tm_fill(\"DEPENDENCY\",\n          style = \"quantile\",\n          palette = \"Blues\",\n          thres.poly = 0) + \n  tm_facets(by=\"REGION_N\", \n            free.coords=TRUE, \n            drop.shapes=TRUE) +\n  tm_layout(legend.show = FALSE,\n            title.position = c(\"center\", \"center\"), \n            title.size = 20) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\nyoungmap <- tm_shape(mpsz_pop2020)+ \n  tm_polygons(\"YOUNG\", \n              style = \"quantile\", \n              palette = \"Blues\")\n\nagedmap <- tm_shape(mpsz_pop2020)+ \n  tm_polygons(\"AGED\", \n              style = \"quantile\", \n              palette = \"Blues\")\n\ntmap_arrange(youngmap, agedmap, asp=1, ncol=2)\n\n\n\n\n\ntm_shape(mpsz_pop2020[mpsz_pop2020$REGION_N==\"CENTRAL REGION\", ])+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\", \n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(legend.outside = TRUE,\n            legend.height = 0.45, \n            legend.width = 5.0,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = FALSE) +\n  tm_borders(alpha = 0.5)"
  },
  {
    "objectID": "posts/geo.html",
    "href": "posts/geo.html",
    "title": "Jordan OZR",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\n\nGeospatial analytics hold tremendous potential to address complex problems facing society. In this study, you are tasked to apply appropriate global and local measures of spatial Association techniques to reveals the spatial patterns of Not Functional water points. For the purpose of this study, Nigeria will be used as the study country.\n\n\n\n\ngeospatial\n\n\nsf\n\n\nspdep\n\n\ntmap\n\n\n \n\n\n\n\nNovember 25, 2022\n\n\nOng Zhi Rong Jordan\n\n\n\n\n\n\n  \n\n\n\n\n\nDescribing the presence of systematic spatial variation in a variable. “The first law of geography Everything is related to everything else, but near things are more related than distant things.” Waldo R. Tobler (Tobler, Waldo R. 1970)\n\n\n\n\ngeospatial\n\n\nsf\n\n\nspdep\n\n\ntmap\n\n\n \n\n\n\n\nNovember 24, 2022\n\n\nOng Zhi Rong Jordan\n\n\n\n\n\n\n  \n\n\n\n\n\nComputation of spatial weights using R. Understanding the spatial relationships that exist among the features in the dataset.\n\n\n\n\ngeospatial\n\n\nsf\n\n\nspdep\n\n\ntmap\n\n\n \n\n\n\n\nNovember 24, 2022\n\n\nOng Zhi Rong Jordan\n\n\n\n\n\n\n  \n\n\n\n\n\nUtilising the different libraries such as ggplot, tmap and leaflet to visualise geographical data.\n\n\n\n\ngeospatial\n\n\nsf\n\n\nggplot\n\n\ntmap\n\n\nleaflet\n\n\n \n\n\n\n\nNovember 23, 2022\n\n\nOng Zhi Rong Jordan\n\n\n\n\n\n\n  \n\n\n\n\n\nUtilising the sf and tidyverse packages to tidy geospatial data.\n\n\n\n\ngeospatial\n\n\nsf\n\n\n \n\n\n\n\nNovember 19, 2022\n\n\nOng Zhi Rong Jordan\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/index.html",
    "href": "posts/index.html",
    "title": "Jordan OZR",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n\nGeospatial analytics hold tremendous potential to address complex problems facing society. In this study, you are tasked to apply appropriate global and local measures of spatial Association techniques to reveals the spatial patterns of Not Functional water points. For the purpose of this study, Nigeria will be used as the study country.\n\n\n\ngeospatial\n\n\nsf\n\n\nspdep\n\n\ntmap\n\n\n\n\nOng Zhi Rong Jordan\n\n\nNovember 25, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDescribing the presence of systematic spatial variation in a variable. “The first law of geography Everything is related to everything else, but near things are more related than distant things.” Waldo R. Tobler (Tobler, Waldo R. 1970)\n\n\n\ngeospatial\n\n\nsf\n\n\nspdep\n\n\ntmap\n\n\n\n\nOng Zhi Rong Jordan\n\n\nNovember 24, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nComputation of spatial weights using R. Understanding the spatial relationships that exist among the features in the dataset.\n\n\n\ngeospatial\n\n\nsf\n\n\nspdep\n\n\ntmap\n\n\n\n\nOng Zhi Rong Jordan\n\n\nNovember 24, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUtilising the different libraries such as ggplot, tmap and leaflet to visualise geographical data.\n\n\n\ngeospatial\n\n\nsf\n\n\nggplot\n\n\ntmap\n\n\nleaflet\n\n\n\n\nOng Zhi Rong Jordan\n\n\nNovember 23, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUtilising the sf and tidyverse packages to tidy geospatial data.\n\n\n\ngeospatial\n\n\nsf\n\n\n\n\nOng Zhi Rong Jordan\n\n\nNovember 19, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLeveraging on simple data wrangling techniques to create a RFM model. Subsequently, leveraging on unsupervised classification to conduct customer segmentation for targeted marketing.\n\n\n\ntibble\n\n\nclustering\n\n\nunsupervised\n\n\n\n\nOng Zhi Rong Jordan\n\n\nJune 25, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUsing Static, Interactive and Statistical (SIS) Graphs to reveal the demographics and relationships of a city.\n\n\n\ntibble\n\n\nggstatsplot\n\n\n\n\nOng Zhi Rong Jordan\n\n\nJune 8, 2022\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Me/RFM/index.html",
    "href": "posts/Me/RFM/index.html",
    "title": "RFM Model",
    "section": "",
    "text": "The RFM model has become essential for businesses to identify high value customers and possible churn customers to conduct targeted marketing. Businesses have leverage RFM model to better understand customer behaviours and also calculate Customer Life Time Value (LTV). This could also translate to better budgeting for marketing cost using the (3:1) ratio of LTV:CAC. In this article, I will demonstrate how we can leverage on existing libraries to conduct unsupervised classification and lastly potential future works to enhance the model.\n\nknitr::include_graphics(\"RFM.png\")"
  },
  {
    "objectID": "posts/Me/RFM/index.html#libraries",
    "href": "posts/Me/RFM/index.html#libraries",
    "title": "RFM Model",
    "section": "Libraries",
    "text": "Libraries\nFor this analysis, we will use the following packages from CRAN.\ncluster - Methods for Cluster analysis. Much extended the original from Peter Rousseeuw, Anja Struyf and Mia Hubert, based on Kaufman and Rousseeuw (1990) “Finding Groups in Data”.tidyverse - Loading the core tidyverse packages which will be used for data wrangling and visualisation.factoextra - Extract and Visualize the Results of Multivariate Data Analyses. GGally Extension of ggplot2 by adding several functions to reduce the complexity of combining geometric objects with transformed data.\n\npacman::p_load(cluster, tidyverse, factoextra,lubridate,patchwork, GGally, moments,bestNormalize) #refer to 1st post to understand the usage of pacman\n\npackage 'estimability' successfully unpacked and MD5 sums checked\npackage 'ellipse' successfully unpacked and MD5 sums checked\npackage 'emmeans' successfully unpacked and MD5 sums checked\npackage 'flashClust' successfully unpacked and MD5 sums checked\npackage 'leaps' successfully unpacked and MD5 sums checked\npackage 'scatterplot3d' successfully unpacked and MD5 sums checked\npackage 'FactoMineR' successfully unpacked and MD5 sums checked\npackage 'factoextra' successfully unpacked and MD5 sums checked\n\nThe downloaded binary packages are in\n    C:\\Users\\jorda\\AppData\\Local\\Temp\\RtmpesZQVK\\downloaded_packages\npackage 'GGally' successfully unpacked and MD5 sums checked\n\nThe downloaded binary packages are in\n    C:\\Users\\jorda\\AppData\\Local\\Temp\\RtmpesZQVK\\downloaded_packages\npackage 'lamW' successfully unpacked and MD5 sums checked\npackage 'rngtools' successfully unpacked and MD5 sums checked\npackage 'lobstr' successfully unpacked and MD5 sums checked\npackage 'LambertW' successfully unpacked and MD5 sums checked\npackage 'nortest' successfully unpacked and MD5 sums checked\npackage 'doRNG' successfully unpacked and MD5 sums checked\npackage 'butcher' successfully unpacked and MD5 sums checked\npackage 'bestNormalize' successfully unpacked and MD5 sums checked\n\nThe downloaded binary packages are in\n    C:\\Users\\jorda\\AppData\\Local\\Temp\\RtmpesZQVK\\downloaded_packages"
  },
  {
    "objectID": "posts/Me/RFM/index.html#data-set",
    "href": "posts/Me/RFM/index.html#data-set",
    "title": "RFM Model",
    "section": "Data Set",
    "text": "Data Set\n\nWe will use a customer data set that consist of 6 columns.\n\nCustomer_ID: Identification of Customer\nCategoryGroup: Category group of the item purchased\nCategory: Category of the item purchased\nInvoiceDate: The date of purchased\nQuantity: The number of items purchased\nTotalPrice: The total amount spend on that item"
  },
  {
    "objectID": "posts/Me/RFM/index.html#data-wrangling",
    "href": "posts/Me/RFM/index.html#data-wrangling",
    "title": "RFM Model",
    "section": "Data Wrangling",
    "text": "Data Wrangling\n\ncustomer <- readRDS(\"data/customer.rds\")\n\nLet’s examine the data!\nFrom the summary, we can identify a few potential problems!\n\nCustomer_ID is in numeric not character. # I prefer IDs to be in character form since it is for representation of customer instead of number of customers.\n\nInvoiceDate is not in date time format!\n\nTotalPrice is not in numeric (the symbol was attached to the number)\n\n\nsummary (customer)\n\n  Customer_ID    CategoryGroup        Category         InvoiceDate       \n Min.   :12348   Length:395888      Length:395888      Length:395888     \n 1st Qu.:14132   Class :character   Class :character   Class :character  \n Median :15535   Mode  :character   Mode  :character   Mode  :character  \n Mean   :15462                                                           \n 3rd Qu.:16841                                                           \n Max.   :18287                                                           \n    Quantity        TotalPrice       \n Min.   :   1.00   Length:395888     \n 1st Qu.:   2.00   Class :character  \n Median :   4.00   Mode  :character  \n Mean   :   8.29                     \n 3rd Qu.:  12.00                     \n Max.   :1500.00                     \n\nhead(customer$TotalPrice)\n\n[1] \"£8\" \"£2\" \"£1\" \"£3\" \"£3\" \"£5\"\n\n\nChange of data class\nFor the date time format, we will leverage on lubridate functions to convert our exisiting date to date time format. Since the format is Month/Day/Year, we will use the function mdy. For TotalPrice, there are two symbols found, £ and ,. We will use the gsub function and replace all symbols to an empty space. Lastly, using as.numeric to convert it to a numeric class. For CustomerID, simply use as.character to convert it to character class.\n\ncustomer$InvoiceDate <- mdy(customer$InvoiceDate)\ncustomer$TotalPrice <- as.numeric(gsub(\"[£]|[,]\",\"\",customer$TotalPrice, perl=TRUE))\ncustomer$Customer_ID <- as.character(customer$Customer_ID)\n\nExtracting Recency, Frequency and Monetary\nRecency\nTo extract how recent the customer purchase an item from the store, we will use the last InvoiceDate to substract all the dates a customer purchase from the store and retrieve the minimum number. Since the format of Recency will be in datetime format, we will convert it using the as.numeric function.\n\ncustomer_recency <- customer %>%\n  mutate(recency = (max(InvoiceDate) + 1) - InvoiceDate) %>%\n  group_by(Customer_ID) %>%\n  summarise (Recency = as.numeric(min(recency)))\n\nFrequency\nTo extract how frequent the customer purchase an item from the store, we will use the n() function to find out how many different dates the customer visited the store.\n\ncustomer_frequency <- customer %>%\n  group_by(Customer_ID,InvoiceDate) %>%\n  summarise (count = n()) %>%\n  ungroup() %>%\n  group_by (Customer_ID) %>%\n  summarise (Frequency = n()) %>%\n  ungroup()\n\n\ncustomer_monetary <- customer %>%\n  group_by(Customer_ID) %>%\n  summarise (Monetary = sum(TotalPrice))\n\n\ncustomer_RFM <- customer_recency %>%\n  left_join (customer_frequency, by = \"Customer_ID\") %>%\n  left_join (customer_monetary, by = \"Customer_ID\")\n\nExamining the distribution of the RFM model\n\n# Histogram overlaid with kernel density curve\nrdplot <- ggplot(customer_RFM, aes(x=Recency)) + \n    geom_histogram(aes(y=..density..),      # Histogram with density instead of count on y-axis\n                   binwidth=10,\n                   colour=\"black\", fill=\"white\") +\n    geom_density(alpha=.2, fill=\"#ff9285\") + \n  ylab(\"Density\") +\n  theme_classic() +\n  theme(axis.title.y= element_text(angle=0), axis.ticks.x= element_blank(),\n        axis.line= element_line(color= 'grey'))\n\nfdplot <- ggplot(customer_RFM, aes(x=Frequency)) + \n    geom_histogram(aes(y=..density..),      # Histogram with density instead of count on y-axis\n                   binwidth=2,\n                   colour=\"black\", fill=\"white\") +\n    geom_density(alpha=.2, fill=\"#906efa\") + \n  ylab(\"Density\") +\n  theme_classic() +\n  theme(axis.title.y= element_text(angle=0), axis.ticks.x= element_blank(),\n        axis.line= element_line(color= 'grey'))\n\nmdplot <- ggplot(customer_RFM, aes(x=Monetary)) + \n    geom_histogram(aes(y=..density..),      # Histogram with density instead of count on y-axis\n                   binwidth=250,\n                   colour=\"black\", fill=\"white\") +\n    geom_density(alpha=.2, fill=\"#d18500\") + \n  ylab(\"Density\") +\n  theme_classic() +\n  theme(axis.title.y= element_text(angle=0), axis.ticks.x= element_blank(),\n        axis.line= element_line(color= 'grey'))\n\n\nrdplot + fdplot + mdplot\n\n\n\n\nThrough the skewness and the histogrm, we can conclude that the attributes does not conform to normal distribution. Since all three attributes does not conform to a normal distribution and K-means would perform better with a normal distributed data, we will conduct data transformation. Utilizing the bestNormalise library, we can identify which normalization techniques best suits each attributes based on their distribution.\n\nskewness(customer_RFM$Recency)\n\n[1] 0.7240294\n\nskewness(customer_RFM$Frequency)\n\n[1] 3.781094\n\nskewness(customer_RFM$Monetary)\n\n[1] 1.435198\n\n\nWe can\n\nbestNormalize(customer_RFM$Recency)\n\nBest Normalizing transformation with 5010 Observations\n Estimated Normality Statistics (Pearson P / df, lower => more normal):\n - arcsinh(x): 11.2249\n - Box-Cox: 7.8746\n - Center+scale: 27.8779\n - Exp(x): 22.7727\n - Log_b(x+a): 11.2725\n - orderNorm (ORQ): 1.1695\n - sqrt(x + a): 10.1741\n - Yeo-Johnson: 8.0219\nEstimation method: Out-of-sample via CV with 10 folds and 5 repeats\n \nBased off these, bestNormalize chose:\norderNorm Transformation with 5010 nonmissing obs and ties\n - 552 unique values \n - Original quantiles:\n  0%  25%  50%  75% 100% \n   1   32  121  382  676 \n\nbestNormalize(customer_RFM$Frequency)\n\nBest Normalizing transformation with 5010 Observations\n Estimated Normality Statistics (Pearson P / df, lower => more normal):\n - arcsinh(x): 82.5826\n - Box-Cox: 83.172\n - Center+scale: 82.3083\n - Exp(x): 74.932\n - Log_b(x+a): 82.5978\n - orderNorm (ORQ): 82.2105\n - sqrt(x + a): 82.6232\n - Yeo-Johnson: 83.3307\nEstimation method: Out-of-sample via CV with 10 folds and 5 repeats\n \nBased off these, bestNormalize chose:\nStandardized exp(x) Transformation with 5010 nonmissing obs.:\n Relevant statistics:\n - mean (before standardization) = 2.741283e+28 \n - sd (before standardization) = 1.940317e+30 \n\nbestNormalize(customer_RFM$Monetary)\n\nBest Normalizing transformation with 5010 Observations\n Estimated Normality Statistics (Pearson P / df, lower => more normal):\n - arcsinh(x): 2.3416\n - Box-Cox: 1.8263\n - Center+scale: 20.5024\n - Log_b(x+a): 2.3412\n - orderNorm (ORQ): 1.199\n - sqrt(x + a): 5.5771\n - Yeo-Johnson: 1.815\nEstimation method: Out-of-sample via CV with 10 folds and 5 repeats\n \nBased off these, bestNormalize chose:\norderNorm Transformation with 5010 nonmissing obs and ties\n - 2312 unique values \n - Original quantiles:\n     0%     25%     50%     75%    100% \n   4.00  313.25  713.50 1555.00 5004.00 \n\n\n\ncustomer_RFM_dt <- customer_RFM %>%\n  select(Recency, Frequency, Monetary)\n\nRecency <- orderNorm(customer_RFM_dt$Recency)\nFrequency <- boxcox (customer_RFM_dt$Frequency)\nMonetary <- orderNorm(customer_RFM_dt$Monetary)\n\ncustomer_RFM_dt$Recency <- Recency$x.t\ncustomer_RFM_dt$Frequency <- Frequency$x.t\ncustomer_RFM_dt$Monetary <- Monetary$x.t\n\nskewness(customer_RFM_dt$Recency)\n\n[1] 0.01507482\n\nskewness(customer_RFM_dt$Frequency)\n\n[1] 0.08897177\n\nskewness(customer_RFM_dt$Monetary)\n\n[1] 0.0006851529\n\n\n\n# Histogram overlaid with kernel density curve\nrdplot_dt <- ggplot(customer_RFM_dt, aes(x=Recency)) + \n    geom_histogram(aes(y=..density..),      # Histogram with density instead of count on y-axis\n                   colour=\"black\", fill=\"white\") +\n    geom_density(alpha=.2, fill=\"#ff9285\") + \n  ylab(\"Density\") +\n  theme_classic() +\n  theme(axis.title.y= element_text(angle=0), axis.ticks.x= element_blank(),\n        axis.line= element_line(color= 'grey'))\n\nfdplot_dt <- ggplot(customer_RFM_dt, aes(x=Frequency)) + \n    geom_histogram(aes(y=..density..),      # Histogram with density instead of count on y-axis\n                   colour=\"black\", fill=\"white\") +\n    geom_density(alpha=.2, fill=\"#906efa\") + \n  ylab(\"Density\") +\n  theme_classic() +\n  theme(axis.title.y= element_text(angle=0), axis.ticks.x= element_blank(),\n        axis.line= element_line(color= 'grey'))\n\nmdplot_dt <- ggplot(customer_RFM_dt, aes(x=Monetary)) + \n    geom_histogram(aes(y=..density..),      # Histogram with density instead of count on y-axis\n                   colour=\"black\", fill=\"white\") +\n    geom_density(alpha=.2, fill=\"#d18500\") + \n  ylab(\"Density\") +\n  theme_classic() +\n  theme(axis.title.y= element_text(angle=0), axis.ticks.x= element_blank(),\n        axis.line= element_line(color= 'grey'))\n\n\nrdplot_dt + fdplot_dt + mdplot_dt\n\n\n\n\n\ncustomer_RFM_cluster <- customer_RFM_dt %>%\n  select(Recency, Frequency, Monetary)\n\nK-means Clustering\nTo identify the optimal number of clusters using K means clustering, we will use the fviz_nbclust function and the silhouette and wss. Based on the silhouette score, the optimal cluster is 2 while the WSS score shows either 2 or 3. We will now explore both cluster size.\n\nset.seed(1234)\n\nfviz_nbclust(customer_RFM_cluster, kmeans, method = \"silhouette\")\n\n\n\nfviz_nbclust(customer_RFM_cluster, kmeans, method = \"wss\")\n\n\n\n\nInsights from Cluster\n\nkm_cluster2 <- kmeans(customer_RFM_cluster, \n                     2, \n                     nstart = 25)\n\n\n\nkm_cluster3 <- kmeans(customer_RFM_cluster, \n                     3, \n                     nstart = 25)\n\ncustomer_RFM$km_cluster2 <- as.character(km_cluster2$cluster)\n\ncustomer_RFM$km_cluster3 <- as.character(km_cluster3$cluster)\n\nFrom the table, we can identify that cluster 1 consist of customers on average made a purchase within 94 days, frequent the store 5 times and spend 1.7k. Whereas for cluster 2, the customers recency period on average is about 325 days, frequent on average 1 time and spend about $392. We can say that cluster 1 consist of our high value customers and cluster 2 consist of potential churn customers.\n\ncustomer_RFM %>%\n  group_by(km_cluster2) %>%\n  summarise(mean_recency = mean(Recency),\n            mean_frequency = mean(Frequency),\n            mean_monetary = mean(Monetary),\n            members = n()) \n\n# A tibble: 2 × 5\n  km_cluster2 mean_recency mean_frequency mean_monetary members\n  <chr>              <dbl>          <dbl>         <dbl>   <int>\n1 1                   94.1           5.45         1774.    2595\n2 2                  326.            1.39          393.    2415\n\ncustomer_RFM %>%\n  group_by(km_cluster3) %>%\n  summarise(mean_recency = mean(Recency),\n            mean_frequency = mean(Frequency),\n            mean_monetary = mean(Monetary),\n            members = n()) \n\n# A tibble: 3 × 5\n  km_cluster3 mean_recency mean_frequency mean_monetary members\n  <chr>              <dbl>          <dbl>         <dbl>   <int>\n1 1                  364.            1.11          295.    1681\n2 2                   54.6           7.32         2431.    1391\n3 3                  177.            2.80          865.    1938\n\n\nTo better visualise the distribution of our customers based on their cluster, we will leverage on the ggparcoord to visualise the distribution using a parallel coordinates plot.\n\n# Plot\nggparcoord(customer_RFM,\n    columns = 2:4, groupColumn = 5,\n    showPoints = TRUE,\n    scale=\"uniminmax\",\n    title = \"Parallel Coordinate Plot for the Customer Data\",\n    alphaLines = 0.3\n    ) + \n  theme_classic()+\n  theme(\n    plot.title = element_text(size=10)\n  )  + scale_color_brewer(palette = \"Set2\") + \n  guides(color=guide_legend(title=\"Cluster\"))"
  },
  {
    "objectID": "posts/Me/SIS Representation/index.html",
    "href": "posts/Me/SIS Representation/index.html",
    "title": "SIS Visual Representation",
    "section": "",
    "text": "In this article, I will share how we can leverage on Static, Interactive and Statistical (SIS) graphs to conduct appropriate data visualisation and draw statistical conclusion from the data set. In this article, we will explore varios libraries such as parsetR, ggstatsplot and ggplot."
  },
  {
    "objectID": "posts/Me/SIS Representation/index.html#libraries",
    "href": "posts/Me/SIS Representation/index.html#libraries",
    "title": "SIS Visual Representation",
    "section": "Libraries",
    "text": "Libraries\nInstead of using the base R function such as library() or install.packages(),we will use the p_load function from the pacman package that combine these functions together. Before using the package, you will need to install the package from CRAN.\n\ninstall.packages(\"pacman\")\n\nFor this analysis, we will use the following packages from CRAN.\nparsetR - Visualize your data with interactive d3.js parallel sets with the power and convenience of an htmlwidget.tidyverse - Loading the core tidyverse packages which will be used for data wrangling and visualisation.ggstatsplot - An extension of ggplot2 package for creating graphics with details from statistical tests included in the information-rich plots themselves.patchwork - Combine separate ggplots into the same graphic.\n\npacman::p_load(parsetR, tidyverse, ggstatsplot, patchwork, hrbrthemes)"
  },
  {
    "objectID": "posts/Me/SIS Representation/index.html#data-set",
    "href": "posts/Me/SIS Representation/index.html#data-set",
    "title": "SIS Visual Representation",
    "section": "Data Set",
    "text": "Data Set\n\nTwo different data set for this analysis:\n\n\nParticipants.csv - Information of all participants.\n\nFinancialJournal.csv- Input of the participant’s wages and expenses."
  },
  {
    "objectID": "posts/Me/SIS Representation/index.html#data-wrangling",
    "href": "posts/Me/SIS Representation/index.html#data-wrangling",
    "title": "SIS Visual Representation",
    "section": "Data Wrangling",
    "text": "Data Wrangling\n\nknitr::include_graphics(\"qn1_concept.png\")\n\n\n\n\n\n\n\n\nparticipants <- read_csv(\"rawdata/Participants.csv\")\nfinance <- read_csv(\"rawdata/FinancialJournal.csv\")\n\nReducing of File Size uploading to Git\nTo reduce the requirement to upload the original data set, I will use the saveRDS function to convert my working tibble dataframe to a R data format namely .rds. We will subsequently use the readRDS function to read the data files in R.\n\nsaveRDS(participants, \"participants.rds\")\nsaveRDS(finance, \"finance.rds\")\n\n\nparticipants <- readRDS(\"data/participants.rds\")\nfinance <- readRDS(\"data/finance.rds\")"
  },
  {
    "objectID": "posts/Me/SIS Representation/index.html#data-preparation",
    "href": "posts/Me/SIS Representation/index.html#data-preparation",
    "title": "SIS Visual Representation",
    "section": "Data Preparation",
    "text": "Data Preparation\nThrough the data from the participants, we can identify a total of 1011 participants ad 6 different attributes. The finance data shows the timestamp of the participants log and a category column. It seems like the data is in the long format and therefore we will subsequently pivot the data table to a wide format. We can also see that household size should be a categorical data rather than a numerical data. We address these issues using the dplyr package.\n\nsummary(participants)\n\n participantId    householdSize    haveKids            age       \n Min.   :   0.0   Min.   :1.000   Mode :logical   Min.   :18.00  \n 1st Qu.: 252.5   1st Qu.:1.000   FALSE:710       1st Qu.:29.00  \n Median : 505.0   Median :2.000   TRUE :301       Median :39.00  \n Mean   : 505.0   Mean   :1.964                   Mean   :39.07  \n 3rd Qu.: 757.5   3rd Qu.:3.000                   3rd Qu.:50.00  \n Max.   :1010.0   Max.   :3.000                   Max.   :60.00  \n educationLevel     interestGroup        joviality       \n Length:1011        Length:1011        Min.   :0.000204  \n Class :character   Class :character   1st Qu.:0.240074  \n Mode  :character   Mode  :character   Median :0.477539  \n                                       Mean   :0.493794  \n                                       3rd Qu.:0.746819  \n                                       Max.   :0.999234  \n\nsummary(finance)\n\n participantId      timestamp                        category        \n Min.   :   0.0   Min.   :2022-03-01 00:00:00.00   Length:1856330    \n 1st Qu.: 222.0   1st Qu.:2022-06-14 12:30:00.00   Class :character  \n Median : 464.0   Median :2022-10-06 16:20:00.00   Mode  :character  \n Mean   : 480.8   Mean   :2022-10-07 12:36:41.13                     \n 3rd Qu.: 726.0   3rd Qu.:2023-01-29 19:10:00.00                     \n Max.   :1010.0   Max.   :2023-05-25 00:05:00.00                     \n     amount         \n Min.   :-1562.726  \n 1st Qu.:   -5.594  \n Median :   -4.000  \n Mean   :   19.922  \n 3rd Qu.:   22.856  \n Max.   : 4096.526  \n\n\nAs part of Data Preparation, I prefer to ensure my columns are well worded. This would reduce the need to reword the X and Y axis subsequently for all the plots.\n\nparticipants <- participants %>%\n  rename('Participant Id' = 'participantId', \n         'Household Size' = 'householdSize', \n         'Have Kids' = 'haveKids', \n         'Age' = 'age', \n         'Education Level' = 'educationLevel', \n         'Interest Group' = 'interestGroup', \n         'Joviality' = 'joviality')\n\n\ncolnames(participants) # verify if the columns have been renamed correctly \n\n[1] \"Participant Id\"  \"Household Size\"  \"Have Kids\"       \"Age\"            \n[5] \"Education Level\" \"Interest Group\"  \"Joviality\"      \n\n#rename value \nparticipants$`Education Level` <- sub('HighSchoolOrCollege', \n                                    'High School or College',\n                                    participants$`Education Level`)\n\nparticipants$`Household Size` <- as.factor(participants$`Household Size`)\nparticipants$`Education Level` <- factor(participants$`Education Level`, levels = c(\n  \"Low\", \"High School or College\", \"Bachelors\", \"Graduate\"), ordered = TRUE) #create factor data object to categorise the Education Level by levels.\n\nWe will now examine how many different input categories are there. There are 6 different categories and 1011 participants throughout the period of 1 year and 2 months based on the timestamp. There should be a total of 2,547,720 financial records but the total recorded data was only 1,856,330. This shows some participants might not have recorded their finance throughout the period. We will now identify participants that are not consistent in their input.\n\nunique(finance$category)\n\n[1] \"Wage\"           \"Shelter\"        \"Education\"      \"RentAdjustment\"\n[5] \"Food\"           \"Recreation\"    \n\n\nBased on our analysis of the data, there are 131 participants who have only logged in less than 12 times throughout the period of analysis. We will identify these participants as inactive and will exclude them during our analysis of the the population demographics.\n\nincome <- finance %>% \n  filter(category == 'Wage') %>% # extract only wage data\n  select(participantId, amount) %>%\n  group_by(participantId) %>%\n  summarise(count = n()) %>%\n  ungroup()\n\ninactive <- finance %>% \n  filter(category == 'Wage') %>% # extract only wage data\n  select(participantId, amount) %>%\n  group_by(participantId) %>%\n  summarise(count = n()) %>%\n  filter (count < 13) %>%\n  ungroup()\n\n\ninactivepart <- inactive$participantId\n\nactive_participants <- subset(participants, !(`Participant Id` %in% inactivepart))\n\nSince the period of study is 15 months, we will extract the average monthly wage of each active participants using the summarise function and rounding the answer to 2 decimal place.\n\nactive_finance <- subset(finance, !(participantId %in% inactivepart))\n\nactive_finance <- active_finance %>% \n  filter(category == 'Wage') %>% # extract only wage data\n  select(participantId, amount) %>%\n  group_by(participantId) %>%\n  summarise (Income = round(sum(amount)/15,2)) %>%\n  ungroup()\n\n\nactive_participants <- active_participants %>%\n  left_join (active_finance, by = c(\"Participant Id\" = \"participantId\")) %>%\n  mutate(Joviality = Joviality * 100)"
  },
  {
    "objectID": "posts/Me/SIS Representation/index.html#visualisation-and-insights",
    "href": "posts/Me/SIS Representation/index.html#visualisation-and-insights",
    "title": "SIS Visual Representation",
    "section": "Visualisation and Insights",
    "text": "Visualisation and Insights\nVisualising using Static Graph\nWe will first visualise the distribution of the different attributes.\n\n\ngeom_text() is used to add annotations of the count and % values for geom_bar()\n\nGrids and background color are removed for a cleaner look as annotations are included.\nTo choose the different colours for the graph, I use medialab to decide on the Hue colors based on the number of graphs.\n\n\nage <- ggplot (active_participants, aes (x=Age)) +\n  geom_histogram(binwidth=5, fill=\"#c96d44\", color=\"#e9ecef\", alpha=0.9) +\n    labs(title = \"Age Distribution of Active Participants\", subtitle = \"Bin Size 5\") +\n    theme_ipsum() +\n    theme(\n      plot.title = element_text(size=15), axis.title.y= element_text(angle=0)\n    )\n\nhKids <- active_participants %>%\n  ggplot(aes(x = `Have Kids`)) +\n  geom_bar(fill= '#777acd') +\n  geom_text(stat = 'count',\n           aes(label= paste0(stat(count), ', ', \n                             round(stat(count)/sum(stat(count))*100, \n                             1), '%')), vjust= -0.5, size= 3) +\n  labs(y= 'No. of\\nParticipants', title = \"Distribution of Participants \\nwith/without Kids\") +\n  theme(axis.title.y= element_text(angle=0), axis.ticks.x= element_blank(),\n        panel.background= element_blank(), axis.line= element_line(color= 'grey'))\n\n\nhousehold <- active_participants %>%\n  ggplot(aes(x = `Household Size`)) +\n   geom_bar(fill= '#7aa456') +\n  geom_text(stat = 'count',\n           aes(label= paste0(stat(count), ', ', \n                             round(stat(count)/sum(stat(count))*100, \n                             1), '%')), vjust= -0.5, size= 3) +\n  labs(y= 'No. of\\nParticipants', title = \"Distribution of Participants \\nbased on Household Size\") +\n  theme(axis.title.y= element_text(angle=0), axis.ticks.x= element_blank(),\n        panel.background= element_blank(), axis.line= element_line(color= 'grey'))\n\neducation <- active_participants %>%\n  ggplot(aes(x = `Education Level`)) +\n   geom_bar(fill= '#c65999') +\n  geom_text(stat = 'count',\n           aes(label= paste0(stat(count), ', ', \n                             round(stat(count)/sum(stat(count))*100, \n                             1), '%')), vjust= -0.5, size= 3) +\n  labs(y= 'No. of\\nParticipants', title = \"Distribution of Participants \\nbased on Education Level\") +\n  theme(axis.title.y= element_text(angle=0), axis.ticks.x= element_blank(),\n        panel.background= element_blank(), axis.line= element_line(color= 'grey'), title = element_text(size = 10))\n\n\n(age + hKids)/(household + education) #using patchwork to stitch the different graphs together\n\n\n\n\n\njoy <- ggplot (active_participants, aes (x=Joviality)) +\n  geom_histogram(binwidth=5, fill=\"#9c954d\", color=\"#e9ecef\", alpha=0.9) +\n    labs(title = \"Joviality Distribution of Active Participants\", subtitle = \"Bin Size 5\") +\n    theme_ipsum() +\n    theme(\n      plot.title = element_text(size=15), axis.title.y= element_text(angle=0)\n    )\n\nincome <- ggplot (active_participants, aes (x=Income)) +\n  geom_histogram(binwidth=1000, fill=\"#b067a3\", color=\"#e9ecef\", alpha=0.9) +\n    labs(title = \"Income Distribution of Active Participants\", subtitle = \"Bin Size 1000\") +\n    theme_ipsum() +\n    theme(\n      plot.title = element_text(size=15), axis.title.y= element_text(angle=0)\n    )\n\nincome + joy\n\n\n\n\nWe will conduct binning on our numerical data such as Age, Income and Joviality. We use the ntile function to break the values and case_when() to change the group labels accordingly.\n\nactive_participants_grouped <- active_participants %>%\n  mutate (Income_group = ntile(Income, 4)) %>%\n  mutate (Joviality_group = ntile(Joviality, 4)) %>%\n  mutate (Income_group = case_when(\n    Income_group == 1 ~ \"Low Income\",\n    Income_group == 2 ~ \"Mid-Low Income\",\n    Income_group == 3 ~ \"Mid-High Income\",\n    Income_group == 4 ~ \"High Income\"\n  )) %>%\n  mutate (Joviality_group = case_when(\n    Joviality_group == 1 ~ \"Low Joy\",\n    Joviality_group == 2 ~ \"Mid-Low Joy\",\n    Joviality_group == 3 ~ \"Mid-High Joy\",\n    Joviality_group == 4 ~ \"High Joy\"\n  ))\n\nVisualising using Interactive Graph\nWe will now analyse the data using interactive graphs such as parallel set plot. We will leverage on the parset library to provide interactive function. The interesting feature about the parset function is that it allows the user to dynamically shift the levels of the attributes (top-bottom and left-right), providing the user a more interactive visualisation of the data set.\n\nactive_participants_parset <- active_participants_grouped %>%\n  select (`Household Size`, `Have Kids`, `Education Level`, `Interest Group`, Income_group, Joviality_group)\n\nparset(active_participants_parset)\n\n\n\n\n\nVisualising using Statistical Graph\nFrom the Parset plot, we identify a few probable relationship such as Education Level to Income Level etc. We will now use statistical plot to verify our claim. The ggstatsplot library provides a suite of statistical plot to allow user to choose the plot based on its data set. For this study, since our attributes are in categorical form, I will leverage on the ggbarstats.\nInsights\nPearson’s \\(x^2\\)-test of independence revealed that, across 880 participants,there was a significant association between Income Level, Education Level and Joviality Level. (p-value below alpha value of 0.05). The Bayes Factor for the left analysis revealed that the data were \\(8e^{66}\\) times more probable under the alternative hypothesis as compared to the null hypothesis. This can be considered extreme evidence (Sandra Andraszewicz, 2015) in favor of the alternative hypothesis. The Bayes Factor for the right analysis revealed that the data were 23968348874 times more probable under the alternative hypothesis as compared to the null hypothesis. This can also be considered extreme evidence in favor of the alternative hypothesis.\n\nactive_participants_parset$Income_group <- factor(active_participants_parset$Income_group, levels = c(\n  \"Low Income\", \"Mid-Low Income\", \"Mid-High Income\", \"High Income\"), ordered = TRUE) #create factor data object to segment the Education Level by levels.\n\nincome <- ggbarstats(\n  data = active_participants_parset,\n  x = `Education Level`,\n  y = Income_group,\n  type = \"np\",\n  xlab = \"Income Group\"\n)\n\njoy <- ggbarstats(\n  data = active_participants_parset,\n  x = Joviality_group,\n  y = Income_group,\n  type = \"np\",\n  xlab = \"Income Group\"\n)\n\n\nincome + joy"
  },
  {
    "objectID": "posts/Me/SIS Representation/index.html#conclusion",
    "href": "posts/Me/SIS Representation/index.html#conclusion",
    "title": "SIS Visual Representation",
    "section": "Conclusion",
    "text": "Conclusion\nIt is important for data analyst to understand the importance of static and interactive graphs, how we should leverage these tools to provide appropriate data visualisation and subsequently use statistical graphs to draw statistical conclusion to support the hypothesis."
  },
  {
    "objectID": "posts/me.html",
    "href": "posts/me.html",
    "title": "Jordan OZR",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\n\nLeveraging on simple data wrangling techniques to create a RFM model. Subsequently, leveraging on unsupervised classification to conduct customer segmentation for targeted marketing.\n\n\n\n\ntibble\n\n\nclustering\n\n\nunsupervised\n\n\n \n\n\n\n\nJune 25, 2022\n\n\nOng Zhi Rong Jordan\n\n\n\n\n\n\n  \n\n\n\n\n\nUsing Static, Interactive and Statistical (SIS) Graphs to reveal the demographics and relationships of a city.\n\n\n\n\ntibble\n\n\nggstatsplot\n\n\n \n\n\n\n\nJune 8, 2022\n\n\nOng Zhi Rong Jordan\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/visual.html",
    "href": "posts/visual.html",
    "title": "Jordan OZR",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Geo/Geospatial_Choropleth/index.html",
    "href": "posts/Geo/Geospatial_Choropleth/index.html",
    "title": "Introduction to Choropleth Mapping",
    "section": "",
    "text": "For this analysis, we will use the following packages from CRAN.\n\n\nsf - Support for simple features, a standardized way to encode spatial vector data. Binds to ‘GDAL’ for reading and writing data, to ‘GEOS’ for geometrical operations, and to ‘PROJ’ for projection conversions and datum transformations. Uses by default the ‘s2’ package for spherical geometry operations on ellipsoidal (long/lat) coordinates.\n\ntidyverse - Loading the core tidyverse packages which will be used for data wrangling and visualisation.\n\ntmap - Thematic maps are geographical maps in which spatial data distributions are visualized. This package offers a flexible, layer-based, and easy to use approach to create thematic maps, such as choropleths and bubble maps.\n\nleaflet - Create and customize interactive maps using the ‘Leaflet’ JavaScript library and the ‘htmlwidgets’ package.\n\nscales - Graphical scales map data to aesthetics, and provide methods for automatically determining breaks and labels for axes and legends.\n\nviridis - color scales in this package to make plots that are pretty, better represent your data, easier to read by those with colorblindness, and print well in gray scale\n\n\npacman::p_load(sf, tidyverse, tmap, leaflet, scales, viridis)"
  },
  {
    "objectID": "posts/Geo/Geospatial_Choropleth/index.html#libraries",
    "href": "posts/Geo/Geospatial_Choropleth/index.html#libraries",
    "title": "Introduction to Choropleth Mapping",
    "section": "Libraries",
    "text": "Libraries\nFor this analysis, we will use the following packages from CRAN.\n\n\nsf - Support for simple features, a standardized way to encode spatial vector data. Binds to ‘GDAL’ for reading and writing data, to ‘GEOS’ for geometrical operations, and to ‘PROJ’ for projection conversions and datum transformations. Uses by default the ‘s2’ package for spherical geometry operations on ellipsoidal (long/lat) coordinates.\n\ntidyverse - Loading the core tidyverse packages which will be used for data wrangling and visualisation.\n\ntmap - Thematic maps are geographical maps in which spatial data distributions are visualized. This package offers a flexible, layer-based, and easy to use approach to create thematic maps, such as choropleths and bubble maps.\n\nleaflet - Create and customize interactive maps using the ‘Leaflet’ JavaScript library and the ‘htmlwidgets’ package.\n\nscales - Graphical scales map data to aesthetics, and provide methods for automatically determining breaks and labels for axes and legends.\n\nviridis - Color scales in this package to make plots that are pretty, better represent your data, easier to read by those with colorblindness, and print well in gray scale.\n\n\npacman::p_load(sf, tidyverse, tmap, leaflet, scales, viridis)"
  },
  {
    "objectID": "posts/Geo/Geospatial_Choropleth/index.html#data-preparation",
    "href": "posts/Geo/Geospatial_Choropleth/index.html#data-preparation",
    "title": "Introduction to Choropleth Mapping",
    "section": "Data Preparation",
    "text": "Data Preparation\nTwo data set will be used to create the choropleth map. They are:\n\nMaster Plan 2014 Subzone Boundary (Web) (i.e. MP14_SUBZONE_WEB_PL) in ESRI shapefile format. It can be downloaded at data.gov.sg This is a geospatial data. It consists of the geographical boundary of Singapore at the planning subzone level. The data is based on URA Master Plan 2014.\nSingapore Residents by Planning Area / Subzone, Age Group, Sex and Type of Dwelling, June 2011-2020 in csv format (i.e. respopagesextod2011to2020.csv). This is an aspatial data fie. It can be downloaded at Department of Statistics, Singapore Although it does not contain any coordinates values, but it’s PA and SZ fields can be used as unique identifiers to geocode to MP14_SUBZONE_WEB_PL shapefile.\n\nImporting of Geospatial Data\nThe code chunk below uses the st_read() function of sf package to import MP14_SUBZONE_WEB_PL shapefile into R as a simple feature data frame called mpsz. To view the tibble data frame, we can simply call the tibble file name mpsz. When you print a tibble, it only shows the first ten rows and all the columns that fit on one screen. It also prints an abbreviated description of the column type.\n\nmpsz <- st_read(dsn = \"data/geospatial\", \n                layer = \"MP14_SUBZONE_WEB_PL\") %>%\n  st_transform(crs = 3414)\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `C:\\jordanong09\\ISSS624_Geospatial\\posts\\Geo\\Geospatial_Choropleth\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\nmpsz\n\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21 / Singapore TM\nFirst 10 features:\n   OBJECTID SUBZONE_NO       SUBZONE_N SUBZONE_C CA_IND      PLN_AREA_N\n1         1          1    MARINA SOUTH    MSSZ01      Y    MARINA SOUTH\n2         2          1    PEARL'S HILL    OTSZ01      Y          OUTRAM\n3         3          3       BOAT QUAY    SRSZ03      Y SINGAPORE RIVER\n4         4          8  HENDERSON HILL    BMSZ08      N     BUKIT MERAH\n5         5          3         REDHILL    BMSZ03      N     BUKIT MERAH\n6         6          7  ALEXANDRA HILL    BMSZ07      N     BUKIT MERAH\n7         7          9   BUKIT HO SWEE    BMSZ09      N     BUKIT MERAH\n8         8          2     CLARKE QUAY    SRSZ02      Y SINGAPORE RIVER\n9         9         13 PASIR PANJANG 1    QTSZ13      N      QUEENSTOWN\n10       10          7       QUEENSWAY    QTSZ07      N      QUEENSTOWN\n   PLN_AREA_C       REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR\n1          MS CENTRAL REGION       CR 5ED7EB253F99252E 2014-12-05 31595.84\n2          OT CENTRAL REGION       CR 8C7149B9EB32EEFC 2014-12-05 28679.06\n3          SR CENTRAL REGION       CR C35FEFF02B13E0E5 2014-12-05 29654.96\n4          BM CENTRAL REGION       CR 3775D82C5DDBEFBD 2014-12-05 26782.83\n5          BM CENTRAL REGION       CR 85D9ABEF0A40678F 2014-12-05 26201.96\n6          BM CENTRAL REGION       CR 9D286521EF5E3B59 2014-12-05 25358.82\n7          BM CENTRAL REGION       CR 7839A8577144EFE2 2014-12-05 27680.06\n8          SR CENTRAL REGION       CR 48661DC0FBA09F7A 2014-12-05 29253.21\n9          QT CENTRAL REGION       CR 1F721290C421BFAB 2014-12-05 22077.34\n10         QT CENTRAL REGION       CR 3580D2AFFBEE914C 2014-12-05 24168.31\n     Y_ADDR SHAPE_Leng SHAPE_Area                       geometry\n1  29220.19   5267.381  1630379.3 MULTIPOLYGON (((31495.56 30...\n2  29782.05   3506.107   559816.2 MULTIPOLYGON (((29092.28 30...\n3  29974.66   1740.926   160807.5 MULTIPOLYGON (((29932.33 29...\n4  29933.77   3313.625   595428.9 MULTIPOLYGON (((27131.28 30...\n5  30005.70   2825.594   387429.4 MULTIPOLYGON (((26451.03 30...\n6  29991.38   4428.913  1030378.8 MULTIPOLYGON (((25899.7 297...\n7  30230.86   3275.312   551732.0 MULTIPOLYGON (((27746.95 30...\n8  30222.86   2208.619   290184.7 MULTIPOLYGON (((29351.26 29...\n9  29893.78   6571.323  1084792.3 MULTIPOLYGON (((20996.49 30...\n10 30104.18   3454.239   631644.3 MULTIPOLYGON (((24472.11 29...\n\n\nImporting Attribute Data into R\nNext, we will import respopagsex2000to2018.csv file into RStudio and save the file into an R dataframe called popdata.\nThe task will be performed by using read_csv() function of readr package as shown in the code chunk below.\n\npopdata <- read_csv(\"data/respopagesextod2011to2020.csv\")\n\nData Wrangling\nThe following data wrangling and transformation functions will be used:\n\npivot_wider() of tidyr package, and\nmutate(), filter(), group_by() and select() of dplyr package\n\n\npopdata2020 <- popdata %>%\n  filter(Time == 2020) %>%\n  group_by(PA, SZ, AG) %>%\n  summarise(`POP` = sum(`Pop`)) %>%\n  ungroup()%>%\n  pivot_wider(names_from=AG, \n              values_from=POP) %>%\n  mutate(YOUNG = rowSums(.[3:6])\n         +rowSums(.[12])) %>%\nmutate(`ECONOMY ACTIVE` = rowSums(.[7:11])+\nrowSums(.[13:15]))%>%\nmutate(`AGED`=rowSums(.[16:21])) %>%\nmutate(`TOTAL`=rowSums(.[3:21])) %>%  \nmutate(`DEPENDENCY` = (`YOUNG` + `AGED`)\n/`ECONOMY ACTIVE`) %>%\n  select(`PA`, `SZ`, `YOUNG`, \n       `ECONOMY ACTIVE`, `AGED`, \n       `TOTAL`, `DEPENDENCY`)\n\nJoining of attribute and geospatial data frame\nBefore we can perform the georelational join, one extra step is required to convert the values in PA and SZ fields to uppercase. This is because the values of PA and SZ fields are made up of upper- and lowercase. On the other, hand the SUBZONE_N and PLN_AREA_N are in uppercase.\nNext, left_join() of dplyr is used to join the geographical data and attribute table using planning subzone name e.g. SUBZONE_N and SZ as the common identifier.\n\npopdata2020 <- popdata2020 %>%\n  mutate_at(.vars = vars(PA, SZ), \n          .funs = funs(toupper)) %>%\n  filter(`ECONOMY ACTIVE` > 0)\n\nmpsz_pop2020 <- left_join(mpsz, popdata2020,\n                          by = c(\"SUBZONE_N\" = \"SZ\"))"
  },
  {
    "objectID": "posts/Geo/Geospatial_Choropleth/index.html#choropleth-mapping-of-geospatial-data",
    "href": "posts/Geo/Geospatial_Choropleth/index.html#choropleth-mapping-of-geospatial-data",
    "title": "Introduction to Choropleth Mapping",
    "section": "Choropleth Mapping of Geospatial Data",
    "text": "Choropleth Mapping of Geospatial Data\nPlotting using TMap\ntmap has similar syntax to the popular ggplot2 but will also produce a reasonable map with only a couple of lines of code. A default colour scheme will be given where this is not specified and passed to tm_polygons and a legend will also be created by default.\ntmap also offer the user two views, static (plot) or interactive (view).\nTwo approaches can be used to prepare thematic map using tmap, they are:\n\nPlotting a thematic map quickly by using qtm().\nPlotting highly customisable thematic map by using tmap elements.\n\nPlotting a choropleth map quickly by using qtm()\nqtm is termed as quick thematic mode allow users to quickly draw a choropleth with a single line of code. It is concise and provides a good default visualisation in many cases. We will explore the different view that tmap provides.\nThe code chunk below will draw an interactive cartographic standard choropleth map as shown below. The fill argument is used to map the attribute. (i.e. DEPENDENCY)\nThe interactive mode uses the leaflet library. Since the leaflet library require the sf object to be in WGS84, we need to set the tmap_options to true to allow our data set which is SVY21 to be plotted on the leaflet map.\n\ntmap_mode(\"view\")\ntmap_options(check.and.fix = TRUE)\nqtm(mpsz_pop2020, \n    fill = \"DEPENDENCY\")\n\n\n\n\n\ntmap_mode(\"plot\")\n\nThe code chunk below will draw a static cartographic standard choropleth map as shown below.\n\nqtm(mpsz_pop2020, \n    fill = \"DEPENDENCY\")\n\n\n\n\n\nstatic map using qtm\n\n\n\n\nCreating a choropleth map by using tmap’s elements\nDespite its usefulness of drawing a choropleth map quickly and easily, the disadvantge of qtm() is that it makes aesthetics of individual layers harder to control. To draw a high quality cartographic choropleth map as shown in the figure below, tmap’s drawing elements should be used. In the following sub-section, we will share with you tmap functions that used to plot these elements.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"Dependency ratio\") +\n  tm_layout(main.title = \"Distribution of Dependency Ratio by planning subzone\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar() +\n  tm_grid(alpha =0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\\n and Population data from Department of Statistics DOS\", \n             position = c(\"left\", \"bottom\"))\n\n\n\n\n\nstatic tmap with tmap elements\n\n\n\n\nDrawing a base map\nThe basic building block of tmap is tm_shape() followed by one or more layer elemments such as tm_fill() and tm_polygons().\nIn the code chunk below, tm_shape() is used to define the input data (i.e mpsz_pop2020) and tm_polygons() is used to draw the planning subzone polygons\n\ntm_shape(mpsz_pop2020) +\n  tm_polygons()\n\n\n\n\n\nbase map without elements\n\n\n\n\nDrawing a choropleth map using tm_polygons()\nTo draw a choropleth map showing the geographical distribution of a selected variable by planning subzone, we just need to assign the target variable such as Dependency to tm_polygons(). This is similar to the qtm drawn earlier.\n\nThe default interval binning used to draw the choropleth map is called “pretty”.\nThe default colour scheme used is YlOrRd of ColorBrewer\nBy default, Missing value will be shaded in grey.\n\n\ntm_shape(mpsz_pop2020)+\n  tm_polygons(\"DEPENDENCY\")\n\n\n\n\n\nmap with polygons\n\n\n\n\nDrawing a choropleth map using tm_fill() and *tm_border()**\nActually, tm_polygons() is a wraper of tm_fill() and tm_border(). tm_fill() shades the polygons by using the default colour scheme and tm_borders() adds the borders of the shapefile onto the choropleth map.\nThe code chunk below draws a choropleth map by using tm_fill() alone.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\")\n\n\n\n\n\nmap with fill only\n\n\n\n\nTo add the boundary of the planning subzones, tm_borders will be used as shown in the code chunk below.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\") +\n  tm_borders(lwd = 0.1,  alpha = 1)\n\n\n\n\n\nmap with fill and borders\n\n\n\n\nData classification methods of tmap\nMost choropleth maps employ some methods of data classification. The point of classification is to take a large number of observations and group them into data ranges or classes.\ntmap provides a total ten data classification methods, namely: fixed, sd, equal, pretty (default), quantile, kmeans, hclust, bclust, fisher, and jenks.\nThe code chunk below shows multiple data classification methods and classes to illustrate the difference. tmap_arrange is used to display the consolidated maps in grid form.\n\ntmap1 <-  tm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 4,\n          style = \"quantile\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(title = \"Quantile - 4 Class\", title.size = 0.7, legend.text.size = 0.4)\n\ntmap2 <-  tm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 4,\n          style = \"jenks\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(title = \"Jenks - 4 Class\", title.size = 0.7, legend.text.size = 0.4)\n\ntmap3 <-  tm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 4,\n          style = \"equal\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(title = \"Equal - 5 Class\", title.size = 0.7, legend.text.size = 0.4)\n\ntmap4 <-  tm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"hclust\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(title = \"Hclust - 5 Class\", title.size = 0.7, legend.text.size = 0.4)\n\ntmap5 <-  tm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"sd\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(title = \"Sd - 5 Class\", title.size = 0.7, legend.text.size = 0.4)\n\n\ntmap6 <-  tm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"kmeans\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(title = \"Kmeans - 5 Class\", title.size = 0.7, legend.text.size = 0.4)\n\ntmap_arrange(tmap1, tmap2, tmap3, tmap4, tmap5, tmap6, ncol = 3)\n\n\n\n\n\ntmap with multiple data classification methods and classes\n\n\n\n\n\nsummary(mpsz_pop2020$DEPENDENCY)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n 0.0000  0.6519  0.7025  0.7742  0.7645 19.0000      92 \n\ntmap_mode(\"plot\")\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          breaks = c(0, 0.60, 0.70, 0.80, 0.90, 1.00)) +\n  tm_borders(alpha = 0.5) \n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 6,\n          style = \"quantile\",\n          palette = \"Blues\") +\n  tm_borders(alpha = 0.5)\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          style = \"quantile\",\n          palette = \"-Greens\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"jenks\", \n          palette = \"Blues\", \n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(main.title = \"Distribution of Dependency Ratio by planning subzone \\n(Jenks classification)\",\n            main.title.position = \"center\",\n            main.title.size = 1,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            legend.outside = FALSE,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = FALSE) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"-Greens\") +\n  tm_borders(alpha = 0.5) +\n  tmap_style(\"classic\")\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"No. of persons\") +\n  tm_layout(main.title = \"Distribution of Dependency Ratio \\nby planning subzone\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar(width = 0.15) +\n  tm_grid(lwd = 0.1, alpha = 0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\\n and Population data from Department of Statistics DOS\", \n             position = c(\"left\", \"bottom\")) +\n  tmap_style(\"white\")\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(c(\"YOUNG\", \"AGED\"),\n          style = \"equal\", \n          palette = \"Blues\") +\n  tm_layout(legend.position = c(\"right\", \"bottom\")) +\n  tm_borders(alpha = 0.5) +\n  tmap_style(\"white\")\n\n\n\ntm_shape(mpsz_pop2020)+ \n  tm_polygons(c(\"DEPENDENCY\",\"AGED\"),\n          style = c(\"equal\", \"quantile\"), \n          palette = list(\"Blues\",\"Greens\")) +\n  tm_layout(legend.position = c(\"right\", \"bottom\"))\n\n\n\n\n\ntm_shape(mpsz_pop2020) +\n  tm_fill(\"DEPENDENCY\",\n          style = \"quantile\",\n          palette = \"Blues\",\n          thres.poly = 0) + \n  tm_facets(by=\"REGION_N\", \n            free.coords=TRUE, \n            drop.shapes=TRUE) +\n  tm_layout(legend.show = FALSE,\n            title.position = c(\"center\", \"center\"), \n            title.size = 20) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\nyoungmap <- tm_shape(mpsz_pop2020)+ \n  tm_polygons(\"YOUNG\", \n              style = \"quantile\", \n              palette = \"Blues\")\n\nagedmap <- tm_shape(mpsz_pop2020)+ \n  tm_polygons(\"AGED\", \n              style = \"quantile\", \n              palette = \"Blues\")\n\ntmap_arrange(youngmap, agedmap, asp=1, ncol=2)\n\n\n\n\n\ntm_shape(mpsz_pop2020[mpsz_pop2020$REGION_N==\"CENTRAL REGION\", ])+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\", \n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(legend.outside = TRUE,\n            legend.height = 0.45, \n            legend.width = 5.0,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = FALSE) +\n  tm_borders(alpha = 0.5)\n\n\n\n\nPlotting using ggplot\nggplot provides the user much more flexibility in the layers required on the map. Since our object is an sf object, we will use geom_sf which will automatically detect a geometry column and map it. coord_sf is also used to govern the map projection.\n\nggmap <- ggplot(data = mpsz_pop2020) +\n  geom_sf(aes(fill = YOUNG)) +\n  geom_text(aes(x = X_ADDR, y = Y_ADDR, label = PLN_AREA_C), size = 1) + #input the planning area labels\n  xlab(\"Longitude\") + ylab(\"Latitude\") + #x and y axis name\n  ggtitle(\"Dependency level across Planning Area\") + #title\n  theme_bw() + #theme chosen\n  theme(panel.grid.major = element_line(color = gray(.5), linetype = \"dashed\", size = 0.5),\n        panel.background = element_rect(fill = \"aliceblue\")) + \n  coord_sf(crs = st_crs(3414)) \n\nggmap\n\n\n\n\nThe viridis package also allow the user to improve the colour scaling on the plot. Since we use fill to fill the map with the Young attribute, we will use scale_fill_viridis to scale the variable based on the viridis palette.\n\nggmap + scale_fill_viridis(option = \"magma\", direction = -1)\n\n\n\n\nPlotting using leaflet\nLeaflet is one of the most popular open-source JavaScript libraries for interactive maps. This package has grown significantly in popularity in recent years and has fast become common currency amongst companies wishing to dynamically visualize its data. It is an excellent option to consider where the patterns in your data are large and complex and where you have constituent polygons of varying sizes.\nFeatures\n\nInteractive panning/zooming\n\nCompose maps using arbitrary combinations of:\n\nMap tiles\nMarkers\nPolygons\nLines\nPopups\nGeoJSON\n\n\nCreate maps right from the R console or RStudio\nEmbed maps in knitr/R Markdown documents and Shiny apps\nEasily render spatial objects from the sp or sf packages, or data frames with latitude/longitude columns\nUse map bounds and mouse events to drive Shiny logic\nDisplay maps in non spherical mercator projections\nAugment map features using chosen plugins from leaflet plugins repository\n\nData Preparation for leaflet mapping\nFirstly, we create a new column and scale the Young attribute from 0 - 100. We use the colorBin function to maps numeric input data to a fixed number of output colors using the bin created. We then create the interactive labels using the sprintf function.\n\nmpsz_pop2020_leaflet <- mpsz_pop2020 %>%\n  mutate (youngpct = rescale(YOUNG, to = c(0,100)))\n\nmpsz_pop2020_leaflet$youngpct[is.nan(mpsz_pop2020_leaflet$youngpct)]<-0\n\nbins <- c(0, 20, 30, 40, 50, 60, 70, 80, 90, Inf)\npal <- colorBin(\"YlOrRd\", domain = mpsz_pop2020_leaflet$youngpct, bins = bins)\n\nlabels <- sprintf(\n  \"<strong>%s</strong><br/>%g Young Pct\",\n  mpsz_pop2020_leaflet$PLN_AREA_N, mpsz_pop2020_leaflet$youngpct\n) %>% lapply(htmltools::HTML)\n\nCRS projection for leaflet mapping\nThe Leaflet package expects all point, line, and shape data to be specified in latitude and longitude using WGS 84 (a.k.a. EPSG:4326). By default, when displaying this data it projects everything to EPSG:3857 and expects that any map tiles are also displayed in EPSG:3857.\nTherefore, we will need to transform our sf object to the correct crs using st_transform.\n\nmpsz_pop2020_leaflet <- mpsz_pop2020_leaflet %>%\n  st_transform(crs = 4326)\n\nPlotting of leaflet map\nThe easiest way to add tiles is by calling addTiles() with no arguments; by default, OpenStreetMap tiles are used. But many popular free third-party basemaps can be added using the addProviderTiles() function, which is implemented using the leaflet-providers plugin.\nAs a convenience, leaflet also provides a named list of all the third-party tile providers that are supported by the plugin. This enables you to use auto-completion feature of your favorite R IDE (like RStudio) and not have to remember or look up supported tile providers; just type providers$ and choose from one of the options. You can also use names(providers) to view all of the options. For this visualisation, I will use the CartoDB.Positron tiles.\n\nleaflet(mpsz_pop2020_leaflet) %>%\n  addProviderTiles(providers$CartoDB.Positron) %>%\n  addPolygons(\n  fillColor = ~pal(youngpct),\n  weight = 1,\n  opacity = 1,\n  color = \"white\",\n  dashArray = \"3\",\n  fillOpacity = 0.7,\n  highlight = highlightOptions(\n    weight = 5,\n    color = \"#666\",\n    dashArray = \"\",\n    fillOpacity = 0.7,\n    bringToFront = TRUE),\n  label = labels,\n  labelOptions = labelOptions(\n    style = list(\"font-weight\" = \"normal\", padding = \"3px 8px\"),\n    textsize = \"15px\",\n    direction = \"auto\")) %>%\naddLegend(pal = pal, values = ~youngpct, opacity = 0.7, title = NULL,\n                position = \"bottomright\")"
  },
  {
    "objectID": "posts/Geo/Geospatial_Datawrangling/index.html",
    "href": "posts/Geo/Geospatial_Datawrangling/index.html",
    "title": "Data Wrangling of Geospatial Data",
    "section": "",
    "text": "For this analysis, we will use the following packages from CRAN.\n\n\nsf - Support for simple features, a standardized way to encode spatial vector data. Binds to ‘GDAL’ for reading and writing data, to ‘GEOS’ for geometrical operations, and to ‘PROJ’ for projection conversions and datum transformations. Uses by default the ‘s2’ package for spherical geometry operations on ellipsoidal (long/lat) coordinates.\n\ntidyverse - Loading the core tidyverse packages which will be used for data wrangling and visualisation.\n\n\npacman::p_load(sf, tidyverse)"
  },
  {
    "objectID": "posts/Geo/Geospatial_Datawrangling/index.html#dataset",
    "href": "posts/Geo/Geospatial_Datawrangling/index.html#dataset",
    "title": "Data Wrangling of Geospatial Data",
    "section": "Dataset",
    "text": "Dataset\nFor this analysis, we will extract data that is available from the web.\n\nMaster Plan 2014 Subzone Boundary (Web) from data.gov.sg\nPre-Schools Location from data.gov.sg\nCycling Path from LTADataMall\nLatest version of Singapore Airbnb listing data from Inside Airbnb\n\nExtracting geographical information from Dataset\nWe will leverage on the st_read function to retrieve polygon, line and point feature in both ESRI shapefile and KML format.\nThe st_geometry function returns an object of class sfc whereas the glimpse function from dplyr act as a transposed version of the print function that shows the values of the different columns.\nTo check all the classes within the dataset, we use the sapply function to run the class function through all the columns within the data set and return their classes.\n\nmpsz <- st_read(dsn = \"data/geospatial\", \n                  layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `C:\\jordanong09\\ISSS624_Geospatial\\posts\\Geo\\Geospatial_Datawrangling\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\ncyclingpath <- st_read(dsn = \"data/geospatial\", \n                      layer = \"CyclingPath\")\n\nReading layer `CyclingPath' from data source \n  `C:\\jordanong09\\ISSS624_Geospatial\\posts\\Geo\\Geospatial_Datawrangling\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 1625 features and 2 fields\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: 12711.19 ymin: 28711.33 xmax: 42626.09 ymax: 48948.15\nProjected CRS: SVY21\n\npreschool <- st_read(\"data/geospatial/pre-schools-location-kml.kml\")\n\nReading layer `PRESCHOOLS_LOCATION' from data source \n  `C:\\jordanong09\\ISSS624_Geospatial\\posts\\Geo\\Geospatial_Datawrangling\\data\\geospatial\\pre-schools-location-kml.kml' \n  using driver `KML'\nSimple feature collection with 1359 features and 2 fields\nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 103.6824 ymin: 1.248403 xmax: 103.9897 ymax: 1.462134\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84\n\nst_geometry(mpsz)\n\nGeometry set for 323 features \nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\nFirst 5 geometries:\n\nglimpse(mpsz)\n\nRows: 323\nColumns: 16\n$ OBJECTID   <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, …\n$ SUBZONE_NO <int> 1, 1, 3, 8, 3, 7, 9, 2, 13, 7, 12, 6, 1, 5, 1, 1, 3, 2, 2, …\n$ SUBZONE_N  <chr> \"MARINA SOUTH\", \"PEARL'S HILL\", \"BOAT QUAY\", \"HENDERSON HIL…\n$ SUBZONE_C  <chr> \"MSSZ01\", \"OTSZ01\", \"SRSZ03\", \"BMSZ08\", \"BMSZ03\", \"BMSZ07\",…\n$ CA_IND     <chr> \"Y\", \"Y\", \"Y\", \"N\", \"N\", \"N\", \"N\", \"Y\", \"N\", \"N\", \"N\", \"N\",…\n$ PLN_AREA_N <chr> \"MARINA SOUTH\", \"OUTRAM\", \"SINGAPORE RIVER\", \"BUKIT MERAH\",…\n$ PLN_AREA_C <chr> \"MS\", \"OT\", \"SR\", \"BM\", \"BM\", \"BM\", \"BM\", \"SR\", \"QT\", \"QT\",…\n$ REGION_N   <chr> \"CENTRAL REGION\", \"CENTRAL REGION\", \"CENTRAL REGION\", \"CENT…\n$ REGION_C   <chr> \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\",…\n$ INC_CRC    <chr> \"5ED7EB253F99252E\", \"8C7149B9EB32EEFC\", \"C35FEFF02B13E0E5\",…\n$ FMEL_UPD_D <date> 2014-12-05, 2014-12-05, 2014-12-05, 2014-12-05, 2014-12-05…\n$ X_ADDR     <dbl> 31595.84, 28679.06, 29654.96, 26782.83, 26201.96, 25358.82,…\n$ Y_ADDR     <dbl> 29220.19, 29782.05, 29974.66, 29933.77, 30005.70, 29991.38,…\n$ SHAPE_Leng <dbl> 5267.381, 3506.107, 1740.926, 3313.625, 2825.594, 4428.913,…\n$ SHAPE_Area <dbl> 1630379.27, 559816.25, 160807.50, 595428.89, 387429.44, 103…\n$ geometry   <MULTIPOLYGON [m]> MULTIPOLYGON (((31495.56 30..., MULTIPOLYGON (…\n\nsapply(mpsz, class)\n\n$OBJECTID\n[1] \"integer\"\n\n$SUBZONE_NO\n[1] \"integer\"\n\n$SUBZONE_N\n[1] \"character\"\n\n$SUBZONE_C\n[1] \"character\"\n\n$CA_IND\n[1] \"character\"\n\n$PLN_AREA_N\n[1] \"character\"\n\n$PLN_AREA_C\n[1] \"character\"\n\n$REGION_N\n[1] \"character\"\n\n$REGION_C\n[1] \"character\"\n\n$INC_CRC\n[1] \"character\"\n\n$FMEL_UPD_D\n[1] \"Date\"\n\n$X_ADDR\n[1] \"numeric\"\n\n$Y_ADDR\n[1] \"numeric\"\n\n$SHAPE_Leng\n[1] \"numeric\"\n\n$SHAPE_Area\n[1] \"numeric\"\n\n$geometry\n[1] \"sfc_MULTIPOLYGON\" \"sfc\""
  },
  {
    "objectID": "posts/Geo/Geospatial_Datawrangling/index.html#plotting-of-geospatial-data",
    "href": "posts/Geo/Geospatial_Datawrangling/index.html#plotting-of-geospatial-data",
    "title": "Data Wrangling of Geospatial Data",
    "section": "Plotting of geospatial data",
    "text": "Plotting of geospatial data\nUnlike non-geospatial dataset where we plot the data using charts, we will leverage on map-based visualisation to draw insights from our geospatial data. The plot function uses the geometry data, contained primarily in the polygons slot. plot is one of the most useful functions in R, as it changes its behaviour depending on the input data. From the example below, we can see how we manipulate the plot based on how we subset the dataset.\n\nplot(mpsz) #plot based on the different column attributes\n\n\n\nplot(mpsz[\"PLN_AREA_N\"]) #colour plot based on column `PLN_AREA_N`\n\n\n\nplot(st_geometry(mpsz)) #only plot the basic geometry of the polygon data\ncondition <- mpsz$SUBZONE_NO > 5 #set a condition\nplot(mpsz[condition, ], col = \"turquoise\", add = TRUE) #layer the condition above the initial plot"
  },
  {
    "objectID": "posts/Geo/Geospatial_Datawrangling/index.html#changing-of-projection",
    "href": "posts/Geo/Geospatial_Datawrangling/index.html#changing-of-projection",
    "title": "Data Wrangling of Geospatial Data",
    "section": "Changing of Projection",
    "text": "Changing of Projection\nThe Coordinate Reference System (CRS) of spatial objects defines where they are placed on the Earth’s surface. We need to ensure the CRS of our sf objects are correct. Since Singapore uses EPSG:3414 - SVY21 / Singapore TM and from the above details, we understand that all the sf object does not conform to the correct CRS (WGS 84 or SVY21). We will utilise two different function, st_set_crs or st_transform to manually change the CRS of our sp object to the desired value.\n\nmpsz3414 <- st_set_crs(mpsz, 3414)\n\npreschool3414 <- st_transform(preschool, \n                              crs = 3414)\n\n\nst_geometry(mpsz3414)\n\nGeometry set for 323 features \nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21 / Singapore TM\nFirst 5 geometries:\n\nst_geometry(preschool3414)\n\nGeometry set for 1359 features \nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 11203.01 ymin: 25667.6 xmax: 45404.24 ymax: 49300.88\nz_range:       zmin: 0 zmax: 0\nProjected CRS: SVY21 / Singapore TM\nFirst 5 geometries:"
  },
  {
    "objectID": "posts/Geo/Geospatial_Datawrangling/index.html#importing-and-converting-an-aspatial-data",
    "href": "posts/Geo/Geospatial_Datawrangling/index.html#importing-and-converting-an-aspatial-data",
    "title": "Data Wrangling of Geospatial Data",
    "section": "Importing and Converting an Aspatial Data",
    "text": "Importing and Converting an Aspatial Data\nR provides the function to convert any foreign object to an sf object using the st_as_sf function. This will allow user to provide a data table that consist of the longitude and latitude and select the correct CRS to transform it to the approriate sf object.\nAfter importing the data, we will examine the dataframe using the list function.\n\nlistings <- read_csv(\"data/aspatial/listings.csv\")\nlist(listings) \n\n[[1]]\n# A tibble: 4,252 × 16\n       id name         host_id host_name neighbourhood_g… neighbourhood latitude\n    <dbl> <chr>          <dbl> <chr>     <chr>            <chr>            <dbl>\n 1  50646 Pleasant Ro…  227796 Sujatha   Central Region   Bukit Timah       1.33\n 2  71609 Ensuite Roo…  367042 Belinda   East Region      Tampines          1.35\n 3  71896 B&B  Room 1…  367042 Belinda   East Region      Tampines          1.35\n 4  71903 Room 2-near…  367042 Belinda   East Region      Tampines          1.35\n 5 275343 Convenientl… 1439258 Joyce     Central Region   Bukit Merah       1.29\n 6 275344 15 mins to … 1439258 Joyce     Central Region   Bukit Merah       1.29\n 7 294281 5 mins walk… 1521514 Elizabeth Central Region   Newton            1.31\n 8 301247 Nice room w… 1552002 Rahul     Central Region   Geylang           1.32\n 9 324945 20 Mins to … 1439258 Joyce     Central Region   Bukit Merah       1.29\n10 330089 Accomo@ RED… 1439258 Joyce     Central Region   Bukit Merah       1.29\n# … with 4,242 more rows, and 9 more variables: longitude <dbl>,\n#   room_type <chr>, price <dbl>, minimum_nights <dbl>,\n#   number_of_reviews <dbl>, last_review <date>, reviews_per_month <dbl>,\n#   calculated_host_listings_count <dbl>, availability_365 <dbl>\n\n\nThe output reveals that the data frame consists of 4252 rows and 16 columns. The column longtitude and latitude will be required for to transform this data frame to a sf object.\n\nlistings_sf <- st_as_sf(listings, \n                       coords = c(\"longitude\", \"latitude\"),\n                       crs=4326) %>%\n  st_transform(crs = 3414)\n\nst_crs(listings_sf)\n\nCoordinate Reference System:\n  User input: EPSG:3414 \n  wkt:\nPROJCRS[\"SVY21 / Singapore TM\",\n    BASEGEOGCRS[\"SVY21\",\n        DATUM[\"SVY21\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4757]],\n    CONVERSION[\"Singapore Transverse Mercator\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"northing (N)\",north,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"easting (E)\",east,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Cadastre, engineering survey, topographic mapping.\"],\n        AREA[\"Singapore - onshore and offshore.\"],\n        BBOX[1.13,103.59,1.47,104.07]],\n    ID[\"EPSG\",3414]]"
  },
  {
    "objectID": "posts/Geo/Geospatial_Datawrangling/index.html#joining-sf-and-tibble-dataframe",
    "href": "posts/Geo/Geospatial_Datawrangling/index.html#joining-sf-and-tibble-dataframe",
    "title": "Data Wrangling of Geospatial Data",
    "section": "Joining sf and tibble dataframe",
    "text": "Joining sf and tibble dataframe\nOne way to manipulate a dataframe is to combine two different sets of data frame together to combine the information retrieved. We will now aggregate the room price of the apartment based on the planning area.\n\n\nmutate: Adds new variables and preserves existing ones. If the new column is referencing an exisiting column, it will replace the variable. Since all the planning area are in uppercase in the mpsz data frame, we will use toupper to convert all the variables inside neighbourhood to uppercase.\n\nfilter: To remove irrelevant rows that are not required for the join.\n\nrename: Rename the column. I will be changing the neighbourhood to PLN_AREA_N to allow both data frame to identify the keys for the join.\n\nsummarise: After grouping the variables through the group_by function, we will summarise it to one row with the average price using the mean function.\n\n\nlistings_tidy <- listings %>%\n  mutate (neighbourhood = toupper(neighbourhood)) %>%\n  filter ((neighbourhood %in% unique(mpsz$PLN_AREA_N))) %>%\n  rename(\"PLN_AREA_N\" = \"neighbourhood\") %>%\n  group_by(PLN_AREA_N) %>%\n  summarise (avgprice = mean(price)) %>%\n  ungroup()\n\n\nmpsz3414 <- mpsz3414 %>%\n  left_join(listings_tidy)"
  },
  {
    "objectID": "posts/Geo/Geospatial_Datawrangling/index.html#geoprocessing-with-sf-package",
    "href": "posts/Geo/Geospatial_Datawrangling/index.html#geoprocessing-with-sf-package",
    "title": "Data Wrangling of Geospatial Data",
    "section": "Geoprocessing with sf package",
    "text": "Geoprocessing with sf package\nBuffering\nIn some cases, there is a need to create a buffering zone along the linestring object. An example would be to expand 5m along a road and understanding the total area increased through the expansion. One way we can do this is to use the st_buffer function that computes a buffer around this geometry/each geometry. To find out the overall area, st_area will be used. If the coordinates are in degrees longtitude/latitude, st_geod_area is used for area calculation.\n\nbuffer_cycling <- st_buffer(cyclingpath, \n                               dist=5, nQuadSegs = 30)\n\nbuffer_cycling$AREA <- st_area(buffer_cycling)\n\nsum(buffer_cycling$AREA)\n\n773143.9 [m^2]\n\n\nVisualising of buffering\nFrom the below visualisation, we are able to better understand how the buffer distance is being calculated and the different endCapStyle to be use for the buffer.\n\ncyclingpath_buffer <- cyclingpath[1,] %>%\n  select (-CYL_PATH_1)\n\nop = par(mfrow=c(2,3))\nplot(st_buffer(cyclingpath_buffer, dist = 1, endCapStyle=\"ROUND\"), reset = FALSE, main = \"endCapStyle: ROUND, distance 1\")\nplot(cyclingpath_buffer,col='blue',add=TRUE)\nplot(st_buffer(cyclingpath_buffer, dist = 2, endCapStyle=\"FLAT\"), reset = FALSE, main = \"endCapStyle: FLAT, distance 2\")\nplot(cyclingpath_buffer,col='blue',add=TRUE)\nplot(st_buffer(cyclingpath_buffer, dist = 3, endCapStyle=\"SQUARE\"), reset = FALSE, main = \"endCapStyle: SQUARE, distance 3\")\nplot(cyclingpath_buffer,col='blue',add=TRUE)\n\n\n\n\n\nmpsz3414$`PreSch Count`<- lengths(st_intersects(mpsz3414, preschool3414))\n\nsummary(mpsz3414$`PreSch Count`)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  0.000   0.000   2.000   4.207   6.000  37.000 \n\ntop_n(mpsz3414, 1, `PreSch Count`)\n\nSimple feature collection with 1 feature and 17 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 23449.05 ymin: 46001.23 xmax: 25594.22 ymax: 47996.47\nProjected CRS: SVY21 / Singapore TM\n  OBJECTID SUBZONE_NO      SUBZONE_N SUBZONE_C CA_IND PLN_AREA_N PLN_AREA_C\n1      290          3 WOODLANDS EAST    WDSZ03      N  WOODLANDS         WD\n      REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR   Y_ADDR\n1 NORTH REGION       NR C90769E43EE6B0F2 2014-12-05 24506.64 46991.63\n  SHAPE_Leng SHAPE_Area avgprice                       geometry PreSch Count\n1   6603.608    2553464 74.53191 MULTIPOLYGON (((24786.75 46...           37"
  },
  {
    "objectID": "posts/Geo/Geospatial_Datawrangling/index.html#plotting-of-geographical-data",
    "href": "posts/Geo/Geospatial_Datawrangling/index.html#plotting-of-geographical-data",
    "title": "Data Wrangling of Geospatial Data",
    "section": "Plotting of geographical data",
    "text": "Plotting of geographical data\n\nmpsz3414$Area <- mpsz3414 %>%\n  st_area()\n\nmpsz3414 <- mpsz3414 %>%\n  mutate(`PreSch Density` = `PreSch Count`/Area * 1000000)\n\nhist(mpsz3414$`PreSch Density`)"
  },
  {
    "objectID": "posts/Geo/Geospatial_Datawrangling/index.html#visualising-of-geographical-data-using-ggplot2",
    "href": "posts/Geo/Geospatial_Datawrangling/index.html#visualising-of-geographical-data-using-ggplot2",
    "title": "Data Wrangling of Geospatial Data",
    "section": "Visualising of geographical data using ggplot2",
    "text": "Visualising of geographical data using ggplot2\n\nggplot(data=mpsz3414, \n       aes(x= as.numeric(`PreSch Density`)))+\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"light blue\") +\n  labs(title = \"Are pre-school even distributed in Singapore?\",\n       subtitle= \"There are many planning sub-zones with a single pre-school, on the other hand, \\nthere are two planning sub-zones with at least 20 pre-schools\",\n      x = \"Pre-school density (per km sq)\",\n      y = \"Frequency\")\n\n\n\n\n\nggplot(data=mpsz3414, \n       aes(x= as.numeric(`PreSch Density`), y = as.numeric(`PreSch Count`)))+\n  geom_point() +\n  labs(title = \"Are pre-school even distributed in Singapore?\",\n      x = \"Pre-school density (per km sq)\",\n      y = \"Pre-school Count\")"
  },
  {
    "objectID": "posts/Geo/Geospatial_Choropleth/index.html#conclusion",
    "href": "posts/Geo/Geospatial_Choropleth/index.html#conclusion",
    "title": "Introduction to Choropleth Mapping",
    "section": "Conclusion",
    "text": "Conclusion"
  },
  {
    "objectID": "posts/Geo/Geospatial_SpatialWeight/index.html",
    "href": "posts/Geo/Geospatial_SpatialWeight/index.html",
    "title": "Introduction to Spatial Weights and Application",
    "section": "",
    "text": "For this analysis, we will use the following packages from CRAN.\n\n\nsf - Support for simple features, a standardized way to encode spatial vector data. Binds to ‘GDAL’ for reading and writing data, to ‘GEOS’ for geometrical operations, and to ‘PROJ’ for projection conversions and datum transformations. Uses by default the ‘s2’ package for spherical geometry operations on ellipsoidal (long/lat) coordinates.\n\ntidyverse - Loading the core tidyverse packages which will be used for data wrangling and visualisation.\n\ntmap - Thematic maps are geographical maps in which spatial data distributions are visualized. This package offers a flexible, layer-based, and easy to use approach to create thematic maps, such as choropleths and bubble maps.\n\nspdep - A collection of functions to create spatial weights matrix objects from polygon ‘contiguities’, from point patterns by distance and tessellations, for summarizing these objects, and for permitting their use in spatial data analysis, including regional aggregation by minimum spanning tree; a collection of tests for spatial ‘autocorrelation’\n\n\npacman::p_load(sf, spdep, tmap, tidyverse)"
  },
  {
    "objectID": "posts/Geo/Geospatial_SpatialWeight/index.html#data-preparation",
    "href": "posts/Geo/Geospatial_SpatialWeight/index.html#data-preparation",
    "title": "Introduction to Spatial Weights and Application",
    "section": "Data Preparation",
    "text": "Data Preparation\nTwo dataset will be used for this study:\n\nHunan.shp: A shapefile of the Hunan Province that consist of all the capital\nHunan.csv: A csv file containing multiple attributes of each capital within Hunan\n\nImporting of data\nWe will use the st_read to import the shape file and read_csv to import the aspatial data into the R environment. We will then use a relational join left_join to combine the spatial and aspatial data together.\n\nhunan <- st_read(dsn = \"data/geospatial\", \n                 layer = \"Hunan\")\n\nhunan2012 <- read_csv(\"data/aspatial/Hunan_2012.csv\")\n\n\nhunan <- left_join(hunan,hunan2012)"
  },
  {
    "objectID": "posts/Geo/Geospatial_SpatialWeight/index.html#visualisation-of-spatial-data",
    "href": "posts/Geo/Geospatial_SpatialWeight/index.html#visualisation-of-spatial-data",
    "title": "Introduction to Spatial Weights and Application",
    "section": "Visualisation of spatial data",
    "text": "Visualisation of spatial data\nFor the visualisation, we will only be using tmap. We will prepare a basemap anbd a choropleth map to visualise the distribution of GDP per capita among the capital.\n\nbasemap <- tm_shape(hunan) +\n  tm_polygons() +\n  tm_text(\"NAME_3\", size=0.4)\n\ngdppc <- qtm(hunan, \"GDPPC\")\n\ntmap_arrange(basemap, gdppc, asp=1, ncol=2)\n\n\n\n\n\nbase map and choropleth map"
  },
  {
    "objectID": "posts/Geo/Geospatial_SpatialWeight/index.html#computing-contiguity-spatial-weights",
    "href": "posts/Geo/Geospatial_SpatialWeight/index.html#computing-contiguity-spatial-weights",
    "title": "Introduction to Spatial Weights and Application",
    "section": "Computing Contiguity Spatial Weights",
    "text": "Computing Contiguity Spatial Weights\nContiguity means that two spatial units share a common border of non-zero length. There are multiple criterion of contiguity such as:\n\n\nRook: When only common sides of the polygons are considered to define the neighbor relation (common vertices are ignored).\n\nQueen: The difference between the rook and queen criterion to determine neighbors is that the latter also includes common vertices.\n\nBishop: Is based on the existence of common vertices between two spatial units.\n\n\n\n\n\nContiguity Weights\n\n\n\n\nExcept in the simplest of circumstances, visual examination or manual calculation cannot be used to create the spatial weights from the geometry of the data. It is necessary to utilize explicit spatial data structures to deal with the placement and layout of the polygons in order to determine whether two polygons are contiguous.\nWe will use the poly2nb function to construct neighbours list based on the regions with contiguous boundaries. Based on the documentation, user will be able to pass a queen argument that takes in True or False. The argument the default is set to TRUE, that is, if you don’t specify queen = FALSE this function will return a list of first order neighbours using the Queen criteria.\nComputing (QUEEN) contiguity based neighbour\nThe code chunk below is used to compute Queen contiguity weight matrix.\n\nwm_q <- poly2nb(hunan)\nsummary(wm_q)\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \nLink number distribution:\n\n 1  2  3  4  5  6  7  8  9 11 \n 2  2 12 16 24 14 11  4  2  1 \n2 least connected regions:\n30 65 with 1 link\n1 most connected region:\n85 with 11 links\n\n\nbased on the summary report above,the report shows that there are 88 area units in Hunan. The most connected area unit has 11 neighbours. There are two area units with only one neighbours.\nFor each polygon in our polygon object, wm_q lists all neighboring polygons. For example, to see the neighbors for the first polygon in the object, type:\n\nwm_q[[1]]\n\n[1]  2  3  4 57 85\n\n\nPolygon 1 has 5 neighbors. The numbers represent the polygon IDs as stored in hunan SpatialPolygonsDataFrame class.\nTo reveal the county names of the five neighboring polygons, the code chunk will be used:\n\nhunan$NAME_3[c(2,3,4,57,85)]\n\n[1] \"Hanshou\" \"Jinshi\"  \"Li\"      \"Nan\"     \"Taoyuan\"\n\n\nWe can retrieve the GDPPC of these five countries by using the code chunk below.\n\nhunan$GDPPC[wm_q[[1]]]\n\n[1] 20981 34592 24473 21311 22879\n\n\nThe printed output above shows that the GDPPC of the five nearest neighbours based on Queen’s method are 20981, 34592, 24473, 21311 and 22879 respectively.\nYou can display the complete weight matrix by using str().\n\nstr(wm_q)\n\nList of 88\n $ : int [1:5] 2 3 4 57 85\n $ : int [1:5] 1 57 58 78 85\n $ : int [1:4] 1 4 5 85\n $ : int [1:4] 1 3 5 6\n $ : int [1:4] 3 4 6 85\n $ : int [1:5] 4 5 69 75 85\n $ : int [1:4] 67 71 74 84\n $ : int [1:7] 9 46 47 56 78 80 86\n $ : int [1:6] 8 66 68 78 84 86\n $ : int [1:8] 16 17 19 20 22 70 72 73\n $ : int [1:3] 14 17 72\n $ : int [1:5] 13 60 61 63 83\n $ : int [1:4] 12 15 60 83\n $ : int [1:3] 11 15 17\n $ : int [1:4] 13 14 17 83\n $ : int [1:5] 10 17 22 72 83\n $ : int [1:7] 10 11 14 15 16 72 83\n $ : int [1:5] 20 22 23 77 83\n $ : int [1:6] 10 20 21 73 74 86\n $ : int [1:7] 10 18 19 21 22 23 82\n $ : int [1:5] 19 20 35 82 86\n $ : int [1:5] 10 16 18 20 83\n $ : int [1:7] 18 20 38 41 77 79 82\n $ : int [1:5] 25 28 31 32 54\n $ : int [1:5] 24 28 31 33 81\n $ : int [1:4] 27 33 42 81\n $ : int [1:3] 26 29 42\n $ : int [1:5] 24 25 33 49 54\n $ : int [1:3] 27 37 42\n $ : int 33\n $ : int [1:8] 24 25 32 36 39 40 56 81\n $ : int [1:8] 24 31 50 54 55 56 75 85\n $ : int [1:5] 25 26 28 30 81\n $ : int [1:3] 36 45 80\n $ : int [1:6] 21 41 47 80 82 86\n $ : int [1:6] 31 34 40 45 56 80\n $ : int [1:4] 29 42 43 44\n $ : int [1:4] 23 44 77 79\n $ : int [1:5] 31 40 42 43 81\n $ : int [1:6] 31 36 39 43 45 79\n $ : int [1:6] 23 35 45 79 80 82\n $ : int [1:7] 26 27 29 37 39 43 81\n $ : int [1:6] 37 39 40 42 44 79\n $ : int [1:4] 37 38 43 79\n $ : int [1:6] 34 36 40 41 79 80\n $ : int [1:3] 8 47 86\n $ : int [1:5] 8 35 46 80 86\n $ : int [1:5] 50 51 52 53 55\n $ : int [1:4] 28 51 52 54\n $ : int [1:5] 32 48 52 54 55\n $ : int [1:3] 48 49 52\n $ : int [1:5] 48 49 50 51 54\n $ : int [1:3] 48 55 75\n $ : int [1:6] 24 28 32 49 50 52\n $ : int [1:5] 32 48 50 53 75\n $ : int [1:7] 8 31 32 36 78 80 85\n $ : int [1:6] 1 2 58 64 76 85\n $ : int [1:5] 2 57 68 76 78\n $ : int [1:4] 60 61 87 88\n $ : int [1:4] 12 13 59 61\n $ : int [1:7] 12 59 60 62 63 77 87\n $ : int [1:3] 61 77 87\n $ : int [1:4] 12 61 77 83\n $ : int [1:2] 57 76\n $ : int 76\n $ : int [1:5] 9 67 68 76 84\n $ : int [1:4] 7 66 76 84\n $ : int [1:5] 9 58 66 76 78\n $ : int [1:3] 6 75 85\n $ : int [1:3] 10 72 73\n $ : int [1:3] 7 73 74\n $ : int [1:5] 10 11 16 17 70\n $ : int [1:5] 10 19 70 71 74\n $ : int [1:6] 7 19 71 73 84 86\n $ : int [1:6] 6 32 53 55 69 85\n $ : int [1:7] 57 58 64 65 66 67 68\n $ : int [1:7] 18 23 38 61 62 63 83\n $ : int [1:7] 2 8 9 56 58 68 85\n $ : int [1:7] 23 38 40 41 43 44 45\n $ : int [1:8] 8 34 35 36 41 45 47 56\n $ : int [1:6] 25 26 31 33 39 42\n $ : int [1:5] 20 21 23 35 41\n $ : int [1:9] 12 13 15 16 17 18 22 63 77\n $ : int [1:6] 7 9 66 67 74 86\n $ : int [1:11] 1 2 3 5 6 32 56 57 69 75 ...\n $ : int [1:9] 8 9 19 21 35 46 47 74 84\n $ : int [1:4] 59 61 62 88\n $ : int [1:2] 59 87\n - attr(*, \"class\")= chr \"nb\"\n - attr(*, \"region.id\")= chr [1:88] \"1\" \"2\" \"3\" \"4\" ...\n - attr(*, \"call\")= language poly2nb(pl = hunan)\n - attr(*, \"type\")= chr \"queen\"\n - attr(*, \"sym\")= logi TRUE\n\n\nComputing (ROOK) contiguity based neighbour\nThe code chunk below is used to compute Rook contiguity weight matrix.\n\nwm_r <- poly2nb(hunan, queen=FALSE)\nsummary(wm_r)\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 440 \nPercentage nonzero weights: 5.681818 \nAverage number of links: 5 \nLink number distribution:\n\n 1  2  3  4  5  6  7  8  9 10 \n 2  2 12 20 21 14 11  3  2  1 \n2 least connected regions:\n30 65 with 1 link\n1 most connected region:\n85 with 10 links\n\n\nThe summary report above shows that there are 88 area units in Hunan. The most connect area unit has 10 neighbours. There are two area units with only one heighbours.\nVisualising the weights matrix\nA connectivity graph takes a point and displays a line to each neighboring point. We are working with polygons at the moment, so we will need to get points in order to make our connectivity graphs. The most typically method for this will be polygon centroids. To retrieve the centroid of each area, we will use the st_centroid function.\n\ncoords <- st_centroid(st_geometry(hunan))\n\nPlotting Queen and Rook contiguity based neighbours map\n\npar(mfrow=c(1,2))\n\nplot(st_geometry(hunan), border=\"grey\", main = \"Queen Contiguity\")\nplot(wm_q, coords,pch = 19, cex = 0.6, add = TRUE, col= \"red\")\n\nplot(st_geometry(hunan), border=\"grey\", main = \"Rook Contiguity\")\nplot(wm_r, coords,pch = 19, cex = 0.6, add = TRUE, col= \"blue\")"
  },
  {
    "objectID": "posts/Geo/Geospatial_SpatialWeight/index.html#computing-distance-based-neighbours",
    "href": "posts/Geo/Geospatial_SpatialWeight/index.html#computing-distance-based-neighbours",
    "title": "Introduction to Spatial Weights and Application",
    "section": "Computing distance based neighbours",
    "text": "Computing distance based neighbours\nIn this section, you will learn how to derive distance-based weight matrices by using dnearneigh() of spdep package. The function identifies neighbours of region points by Euclidean distance in the metric of the points between lower (greater than or equal to and upper (less than or equal to) bounds.\nDetermine the cut-off distance\nFirstly, we need to determine the upper limit for distance band by using the steps below:\n\nReturn a matrix with the indices of points belonging to the set of the k nearest neighbours of each other by using knearneigh() of spdep.\nConvert the knn object returned by knearneigh() into a neighbours list of class nb with a list of integer vectors containing neighbour region number ids by using knn2nb().\nReturn the length of neighbour relationship edges by using nbdists() of spdep. The function returns in the units of the coordinates if the coordinates are projected, in km otherwise.\nRemove the list structure of the returned object by using unlist().\n\n\nk1 <- knn2nb(knearneigh(coords))\nk1dists <- unlist(nbdists(k1, coords))\nsummary(k1dists)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  24.80   32.53   38.06   38.91   44.66   58.25 \n\n\nThe summary report shows that the largest first nearest neighbour distance is 58.25 km, so using this as the upper threshold gives certainty that all units will have at least one neighbour.\nComputing fixed distance weight matrix\nNow, we will compute the distance weight matrix by using dnearneigh() as shown in the code chunk below.\n\nwm_d59 <- dnearneigh(coords, 0, 59)\nwm_d59\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 298 \nPercentage nonzero weights: 3.84814 \nAverage number of links: 3.386364 \n\n\nThe report shows that on average, every area should have at least 3 neighbours (links).\nTo display the structure of the weight matrix is to combine table() and card() of spdep.\n\nhead(table(hunan$County, card(wm_d59)),10)\n\n           \n            1 2 3 4 5 6\n  Anhua     1 0 0 0 0 0\n  Anren     0 0 0 1 0 0\n  Anxiang   0 0 0 0 1 0\n  Baojing   0 0 0 1 0 0\n  Chaling   0 0 1 0 0 0\n  Changning 0 0 1 0 0 0\n  Changsha  0 0 0 1 0 0\n  Chengbu   1 0 0 0 0 0\n  Chenxi    0 0 0 1 0 0\n  Cili      1 0 0 0 0 0\n\n\nVisualising distance weight matrix\nThe left graph with the red lines show the links of 1st nearest neighbours and the right graph with the black lines show the links of neighbours within the cut-off distance of 59km.\n\npar(mfrow=c(1,2))\nplot(hunan$geometry, border=\"lightgrey\", main=\"1st nearest neighbours\")\nplot(k1, coords, add=TRUE, col=\"red\", length=0.08)\nplot(hunan$geometry, border=\"lightgrey\", main=\"Distance link\")\nplot(wm_d59, coords, add=TRUE, pch = 19, cex = 0.6)\n\n\n\n\nAdaptive distance weight matrix\nOther than using distance as a criteria to decide the neighbours, it is possible to control the numbers of neighbours directly using k-nearest neighbours, either accepting asymmetric neighbours or imposing symmetry as shown in the code chunk below:\n\nknn6 <- knn2nb(knearneigh(coords, k=6)) #k refers to the number of neighbours per area\nknn6\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 528 \nPercentage nonzero weights: 6.818182 \nAverage number of links: 6 \nNon-symmetric neighbours list\n\n\nPlotting adaptive distance weight\nWe can plot the adaptive distance weight matrix using the code chunk below:\n\nplot(hunan$geometry, border=\"lightgrey\", main = \"Adaptive Distance Weight\")\nplot(knn6, coords, pch = 19, cex = 0.6, add = TRUE, col = \"red\")\n\n\n\n\nWeights based on Inversed Distance Weighting (IDW)\nIn this section, you will learn how to derive a spatial weight matrix based on Inversed Distance method.\nFirst, we will compute the distances between areas by using nbdists() of spdep.\nWe will use the [lapply()] (https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/lapply) to apply the inverse function through the list.\n\ndist <- nbdists(wm_q, coords, longlat = TRUE)\nids <- lapply(dist, function(x) 1/(x))\nhead(ids,5)\n\n[[1]]\n[1] 1.694446 3.805533 1.847048 2.867007 1.166097\n\n[[2]]\n[1] 1.694446 1.832614 1.889267 2.537233 1.681209\n\n[[3]]\n[1] 3.805533 3.007305 3.588446 1.468997\n\n[[4]]\n[1] 1.847048 3.007305 3.731278 1.490622\n\n[[5]]\n[1] 3.588446 3.731278 1.526472 1.775459\n\n\nNext, we will use the nb2listw to apply the weights list with values given by the coding scheme style chosen. There are multiple style to choose from:\n\nB (Basic Binary Coding)\nW (Row Standardised) - sums over all links to n\nC (Globally Standardised) - sums over all links to n\nU (Globally Standardised / No of neighbours) - sums over all links to unity\nS (Variance-Stabilizing Coding Scheme) - sums over all links to n\nminmax - divides the weights by the minimum of the maximum row sums and maximum column sums of the input weights\n\nFor the simplifed analysis, we will use the W (Row Standardised).\n\nrswm_q <- nb2listw(wm_q, style=\"W\", zero.policy = TRUE)\nrswm_q\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \n\nWeights style: W \nWeights constants summary:\n   n   nn S0       S1       S2\nW 88 7744 88 37.86334 365.9147\n\n\nFrom the earlier example, we know that the first Id has 5 neighbours. We take a look at the weight distribution of these 5 neighours. Since we are using Row Standardised, they should be equal.\n\nrswm_q$weights[1]\n\n[[1]]\n[1] 0.2 0.2 0.2 0.2 0.2\n\n\nEach neighbor is assigned a 0.2 of the total weight. This means that when R computes the average neighboring income values, each neighbor’s income will be multiplied by 0.2 before being tallied."
  },
  {
    "objectID": "posts/Geo/Geospatial_SpatialWeight/index.html#application-of-spatial-weight-matrix",
    "href": "posts/Geo/Geospatial_SpatialWeight/index.html#application-of-spatial-weight-matrix",
    "title": "Introduction to Spatial Weights and Application",
    "section": "Application of Spatial Weight Matrix",
    "text": "Application of Spatial Weight Matrix\nIn this section, you will learn how to create four different spatial lagged variables, they are:\n\nspatial lag with row-standardized weights\nspatial lag as a sum of neighbouring values\nspatial window average\nspatial window sum\n\nSpatial lag with row-standardized weights\nFirstly, we’ll compute the average neighbor GDPPC value for each polygon using the lag.listw() that can compute the lag of a vector. These values are often referred to as spatially lagged values.\n\nGDPPC.lag <- lag.listw(rswm_q, hunan$GDPPC)\nhead(GDPPC.lag)\n\n[1] 24847.20 22724.80 24143.25 27737.50 27270.25 21248.80\n\n\nIn the previous section, we retrieved the GDPPC of the neighbours of the first area by using the following code chunk:\n\nhunan$GDPPC[wm_q[[1]]]\n\n[1] 20981 34592 24473 21311 22879\n\n\nFrom this, we can understand that the spatial lag with row-standardized weights is actually the average GDPPC of its neighbours.\n\\[(20981+34592+24473+21311+22879/5 = 24847.20)\\]\nWe will now append these lagged values to our Hunan data frame.\n\nlag.df <- as.data.frame(list(hunan$NAME_3,GDPPC.lag))\ncolnames(lag.df) <- c(\"NAME_3\", \"lag GDPPC\")\nhunan <- left_join(hunan,lag.df)\n\nNext, we will plot both the GDPPC and spatial lag GDPPC for comparison using the code chunk below.\n\ngdppc <- tm_shape(hunan) +\n  tm_fill(\"GDPPC\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"GDPPC without lagged values\", main.title.size = 0.7, legend.text.size = 0.4,\n            main.title.fontface = \"bold\",main.title.position = \"center\")\n  \nlag_gdppc <- tm_shape(hunan) +\n  tm_fill(\"lag GDPPC\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"GDPPC with lagged values\", main.title.size = 0.7, legend.text.size = 0.4,\n            main.title.fontface = \"bold\",main.title.position = \"center\")\ntmap_arrange(gdppc, lag_gdppc, asp=1, ncol=2)\n\n\n\n\n\nGDPPC vs lag GDPPC\n\n\n\n\nSpatial lag as a sum of neighboring values\nWe can calculate spatial lag as a sum of neighboring values by assigning binary weights. This requires us to go back to our neighbors list, then apply a function that will assign binary weights, then we use glist = in the nb2listw function to explicitly assign these weights.\nWe start by applying a function that will assign a value of 1 per each neighbor. This is done with lapply, which we have been using to manipulate the neighbors structure throughout the past notebooks. Basically it applies a function across each value in the neighbors structure.\n\nb_weights <- lapply(wm_q, function(x) 0*x + 1)\n\nb_weights2 <- nb2listw(wm_q, \n                       glist = b_weights, \n                       style = \"B\")\nb_weights2\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \n\nWeights style: B \nWeights constants summary:\n   n   nn  S0  S1    S2\nB 88 7744 448 896 10224\n\n\nWith the proper weights assigned, we can use lag.listw to compute a lag variable from our weight and GDPPC.\n\nlag_df <- as.data.frame (list(hunan$NAME_3, lag.listw(b_weights2, hunan$GDPPC)))\ncolnames(lag_df) <- c(\"NAME_3\", \"lag_sum GDPPC\")\nhunan <- left_join(hunan, lag_df)\n\nlag_df\n\n          NAME_3 lag_sum GDPPC\n1        Anxiang        124236\n2        Hanshou        113624\n3         Jinshi         96573\n4             Li        110950\n5          Linli        109081\n6         Shimen        106244\n7        Liuyang        174988\n8      Ningxiang        235079\n9      Wangcheng        273907\n10         Anren        256221\n11       Guidong         98013\n12         Jiahe        104050\n13         Linwu        102846\n14       Rucheng         92017\n15       Yizhang        133831\n16      Yongxing        158446\n17        Zixing        141883\n18     Changning        119508\n19      Hengdong        150757\n20       Hengnan        153324\n21      Hengshan        113593\n22       Leiyang        129594\n23        Qidong        142149\n24        Chenxi        100119\n25     Zhongfang         82884\n26       Huitong         74668\n27      Jingzhou         43184\n28        Mayang         99244\n29       Tongdao         46549\n30      Xinhuang         20518\n31          Xupu        140576\n32      Yuanling        121601\n33      Zhijiang         92069\n34 Lengshuijiang         43258\n35    Shuangfeng        144567\n36        Xinhua        132119\n37       Chengbu         51694\n38        Dongan         59024\n39       Dongkou         69349\n40       Longhui         73780\n41      Shaodong         94651\n42       Suining        100680\n43        Wugang         69398\n44       Xinning         52798\n45       Xinshao        140472\n46      Shaoshan        118623\n47    Xiangxiang        180933\n48       Baojing         82798\n49     Fenghuang         83090\n50       Guzhang         97356\n51       Huayuan         59482\n52        Jishou         77334\n53      Longshan         38777\n54          Luxi        111463\n55      Yongshun         74715\n56         Anhua        174391\n57           Nan        150558\n58     Yuanjiang        122144\n59      Jianghua         68012\n60       Lanshan         84575\n61      Ningyuan        143045\n62     Shuangpai         51394\n63       Xintian         98279\n64       Huarong         47671\n65      Linxiang         26360\n66         Miluo        236917\n67     Pingjiang        220631\n68      Xiangyin        185290\n69          Cili         64640\n70       Chaling         70046\n71        Liling        126971\n72       Yanling        144693\n73           You        129404\n74       Zhuzhou        284074\n75       Sangzhi        112268\n76       Yueyang        203611\n77        Qiyang        145238\n78      Taojiang        251536\n79      Shaoyang        108078\n80      Lianyuan        238300\n81     Hongjiang        108870\n82      Hengyang        108085\n83       Guiyang        262835\n84      Changsha        248182\n85       Taoyuan        244850\n86      Xiangtan        404456\n87           Dao         67608\n88     Jiangyong         33860\n\n\nFrom the above data table and the GDPPC from the previous section, we know that the lagged sum is the addition of all the GDPPC of its neighbours.\n\\[(20981+34592+24473+21311+22879 = 124236)\\]\n\ngdppc <- tm_shape(hunan) +\n  tm_fill(\"GDPPC\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"GDPPC without lagged values\", main.title.size = 0.7, legend.text.size = 0.4,\n            main.title.fontface = \"bold\",main.title.position = \"center\")\n  \nlag_gdppc <- tm_shape(hunan) +\n  tm_fill(\"lag GDPPC\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"GDPPC with lagged values\", main.title.size = 0.7, legend.text.size = 0.4,\n            main.title.fontface = \"bold\",main.title.position = \"center\")\n\nlag_sum_gdppc <- tm_shape(hunan) +\n  tm_fill(\"lag_sum GDPPC\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"GDPPC with lagged sum values\", main.title.size = 0.7, legend.text.size = 0.4,\n            main.title.fontface = \"bold\",main.title.position = \"center\")\n\ntmap_arrange(gdppc, lag_gdppc, lag_sum_gdppc, asp=1, ncol=3)\n\n\n\n\n\nGDPPC vs lag GDPPC vs lag sum GDPPC\n\n\n\n\nSpatial Window Average\nThe spatial window average uses row-standardized weights and includes the diagonal element. (region itself) We will use the include.self().\n\nwm_q_self <- include.self(wm_q)\n\nWe will now obtain the weight and retrieve the new spatial window average and combine it with our exisiting Hunan dataframe.\n\nwm_q_self_list <- nb2listw(wm_q_self)\nlag_w_avg_gpdpc <- lag.listw(wm_q_self_list, \n                             hunan$GDPPC)\n\nlag_w_avg_df <- as.data.frame(list(hunan$NAME_3, lag_w_avg_gpdpc))\n\ncolnames(lag_w_avg_df) <- c(\"NAME_3\", \"lag_window_avg GDPPC\")\n\nhunan <- left_join(hunan, lag_w_avg_df)\n\n\ngdppc <- tm_shape(hunan) +\n  tm_fill(\"GDPPC\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"GDPPC without lagged values\", main.title.size = 0.7, legend.text.size = 0.4,\n            main.title.fontface = \"bold\",main.title.position = \"center\")\n  \nlag_gdppc <- tm_shape(hunan) +\n  tm_fill(\"lag GDPPC\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"GDPPC with lagged values\", main.title.size = 0.7, legend.text.size = 0.4,\n            main.title.fontface = \"bold\",main.title.position = \"center\")\n\nlag_sum_gdppc <- tm_shape(hunan) +\n  tm_fill(\"lag_sum GDPPC\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"GDPPC with lagged sum values\", main.title.size = 0.7, legend.text.size = 0.4,\n            main.title.fontface = \"bold\",main.title.position = \"center\")\n\nlag_sum_avg_gdppc <- tm_shape(hunan) +\n  tm_fill(\"lag_window_avg GDPPC\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"GDPPC with lagged average values\", main.title.size = 0.7, legend.text.size = 0.4,\n            main.title.fontface = \"bold\",main.title.position = \"center\")\n\ntmap_arrange(gdppc, lag_gdppc, lag_sum_gdppc, lag_sum_avg_gdppc, asp=1, ncol=2)\n\n\n\n\n\nGDPPC vs lag GDPPC vs lag sum GDPPC vs lag avg GDPPC\n\n\n\n\nSpatial Window Sum\nThe spatial Window sum is similar to the window average but using the binary weights. Therefore we will repeat the following steps of the Spatial lag as a sum of neighboring values and to include its own region.\n\nb_weights <- lapply(wm_q_self, function(x) 0*x + 1)\nb_weights[1]\n\n[[1]]\n[1] 1 1 1 1 1 1\n\n\nFrom the result, we can see now the first area instead of 5 neighbours, it has 6 neighbours which include itself. We will now retrieve the spatial window sum and combine it with our exisiting Hunan dataframe.\n\nb_weights2 <- nb2listw(wm_q_self, \n                       glist = b_weights, \n                       style = \"B\")\nw_sum_gdppc_df <- as.data.frame(list(hunan$NAME_3, lag.listw(b_weights2, hunan$GDPPC)))\ncolnames(w_sum_gdppc_df) <- c(\"NAME_3\", \"w_sum GDPPC\")\n\nhunan <- left_join(hunan, w_sum_gdppc_df)\n\nWe will now visualise all the plots we created and visualise the difference in each method (excluding the original GDPPC).\n\nlag_gdppc <- tm_shape(hunan) +\n  tm_fill(\"lag GDPPC\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"GDPPC with lagged values\", main.title.size = 0.7, legend.text.size = 0.4,\n            main.title.fontface = \"bold\",main.title.position = \"center\")\n\nlag_sum_gdppc <- tm_shape(hunan) +\n  tm_fill(\"lag_sum GDPPC\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"GDPPC with lagged sum values\", main.title.size = 0.7, legend.text.size = 0.4,\n            main.title.fontface = \"bold\",main.title.position = \"center\")\n\nlag_sum_avg_gdppc <- tm_shape(hunan) +\n  tm_fill(\"lag_window_avg GDPPC\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"GDPPC with lagged average values\", main.title.size = 0.7, legend.text.size = 0.4,\n            main.title.fontface = \"bold\",main.title.position = \"center\")\n\nlag_sum_window_gdppc <- tm_shape(hunan) +\n  tm_fill(\"w_sum GDPPC\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"GDPPC with lagged sum average values\", main.title.size = 0.7, legend.text.size = 0.4,\n            main.title.fontface = \"bold\",main.title.position = \"center\")\n\ntmap_arrange(lag_gdppc, lag_sum_gdppc, lag_sum_avg_gdppc, lag_sum_window_gdppc, asp=1, ncol=2)\n\n\n\n\n\nlag GDPPC vs lag sum GDPPC vs lag avg GDPPC vs lag sum avg GDPPC"
  },
  {
    "objectID": "posts/Geo/Geospatial_SpatialWeight/index.html#conclusion",
    "href": "posts/Geo/Geospatial_SpatialWeight/index.html#conclusion",
    "title": "Introduction to Spatial Weights and Application",
    "section": "Conclusion",
    "text": "Conclusion\nThis study allow us to understand the different contiguity spatial weights and different methods to utilise the neighbours information. There is no best method but which method suits your analysis of your work."
  },
  {
    "objectID": "posts/Geo/Geospatial_Autocorrelation/index.html",
    "href": "posts/Geo/Geospatial_Autocorrelation/index.html",
    "title": "Introduction to Global and Local Measures of Spatial Autocorrelation",
    "section": "",
    "text": "The Spatial Autocorrelation measures spatial autocorrelation based on feature locations and feature values simultaneously. Given a set of features and an associated attribute, it evaluates whether the expressed pattern is clustered, scattered, or random. The tool calculates the Moran’s I index value as well as a z-score and p-value to assess the significance of this index. P-values are numerical approximations of the area under the curve for a known distribution, bounded by the test statistic.\nIn this study we will explore the computation of Global and Local Measure of Spatial Autocorrelation (GLSA) by using spdep package."
  },
  {
    "objectID": "posts/Geo/Geospatial_Autocorrelation/index.html#libraries",
    "href": "posts/Geo/Geospatial_Autocorrelation/index.html#libraries",
    "title": "Introduction to Global and Local Measures of Spatial Autocorrelation",
    "section": "Libraries",
    "text": "Libraries\nFor this study, we will use the following packages from CRAN.\n\n\nsf - Support for simple features, a standardized way to encode spatial vector data. Binds to ‘GDAL’ for reading and writing data, to ‘GEOS’ for geometrical operations, and to ‘PROJ’ for projection conversions and datum transformations. Uses by default the ‘s2’ package for spherical geometry operations on ellipsoidal (long/lat) coordinates.\n\ntidyverse - Loading the core tidyverse packages which will be used for data wrangling and visualisation.\n\ntmap - Thematic maps are geographical maps in which spatial data distributions are visualized. This package offers a flexible, layer-based, and easy to use approach to create thematic maps, such as choropleths and bubble maps.\n\nspdep - A collection of functions to create spatial weights matrix objects from polygon ‘contiguities’, from point patterns by distance and tessellations, for summarizing these objects, and for permitting their use in spatial data analysis, including regional aggregation by minimum spanning tree; a collection of tests for spatial ‘autocorrelation’\n\n\npacman::p_load(sf, spdep, tmap, tidyverse)"
  },
  {
    "objectID": "posts/Geo/Geospatial_Autocorrelation/index.html#data-preparation",
    "href": "posts/Geo/Geospatial_Autocorrelation/index.html#data-preparation",
    "title": "Introduction to Global and Local Measures of Spatial Autocorrelation",
    "section": "Data Preparation",
    "text": "Data Preparation\nTwo dataset will be used for this study:\n\nHunan.shp: A shapefile of the Hunan Province that consist of all the capital\nHunan.csv: A csv file containing multiple attributes of each capital within Hunan\n\nImporting of data\nWe will use the st_read to import the shape file and read_csv to import the aspatial data into the R environment. We will then use a relational join left_join to combine the spatial and aspatial data together.\n\nhunan <- st_read(dsn = \"data/geospatial\", \n                 layer = \"Hunan\")\n\nhunan2012 <- read_csv(\"data/aspatial/Hunan_2012.csv\")\n\n\nhunan <- left_join(hunan,hunan2012)"
  },
  {
    "objectID": "posts/Geo/Geospatial_Autocorrelation/index.html#visualisation-of-spatial-data",
    "href": "posts/Geo/Geospatial_Autocorrelation/index.html#visualisation-of-spatial-data",
    "title": "Introduction to Global and Local Measures of Spatial Autocorrelation",
    "section": "Visualisation of spatial data",
    "text": "Visualisation of spatial data\nFor the visualisation, we will only be using tmap to show the distribution of GDPPC 2021.\n\ntm_shape(hunan) +\n  tm_fill(\"GDPPC\",\n          n = 5,\n          style = \"quantile\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"GDDPC using Quantile classification\", main.title.size = 0.7,\n            main.title.fontface = \"bold\", main.title.position = \"center\")"
  },
  {
    "objectID": "posts/Geo/Geospatial_Autocorrelation/index.html#global-spatial-autocorrelation",
    "href": "posts/Geo/Geospatial_Autocorrelation/index.html#global-spatial-autocorrelation",
    "title": "Introduction to Global and Local Measures of Spatial Autocorrelation",
    "section": "Global Spatial Autocorrelation",
    "text": "Global Spatial Autocorrelation\nBefore we commence with spatial autocorrelation, we need to construct the spatial weights of the study region. The spatial weights is used to define the neighbourhood relationships between the geographical units (i.e. county) in the study area. Refer to my previous post to understand the flow of constructing the spatial weight.\nComputing Contiguity Spatial Weights\nFor this study, we will be using the Queen contiguity weight matrix. The code chunk below will construct the weight matrix and subsequently implement the row-standardised weight matrix using the nb2listw() function.\n\nwm_q <- poly2nb(hunan, \n                queen=TRUE)\n\nrswm_q <- nb2listw(wm_q, \n                   style=\"W\", \n                   zero.policy = TRUE)\nrswm_q\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \n\nWeights style: W \nWeights constants summary:\n   n   nn S0       S1       S2\nW 88 7744 88 37.86334 365.9147\n\n\nGlobal Spatial Autocorrelation: Moran’s I\nWe will now perform Moran’s I statistics testing by using the moran.test() from spdep.\n\nmoran.test(hunan$GDPPC, \n           listw=rswm_q, \n           zero.policy = TRUE, \n           na.action=na.omit)\n\n\n    Moran I test under randomisation\n\ndata:  hunan$GDPPC  \nweights: rswm_q    \n\nMoran I statistic standard deviate = 4.7351, p-value = 1.095e-06\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n      0.300749970      -0.011494253       0.004348351 \n\n\nTo better understand the result, we will reference the following table.\n\n\n\n\n\n\n\nThe p-value is not statistically significant.\nYou cannot reject the null hypothesis. It is quite possible that the spatial distribution of feature values is the result of random spatial processes. The observed spatial pattern of feature values could very well be one of many, many possible versions of complete spatial randomness (CSR).\n\n\nThe p-value is statistically significant, and the z-score is positive.\nYou may reject the null hypothesis. The spatial distribution of high values and/or low values in the dataset is more spatially clustered than would be expected if underlying spatial processes were random.\n\n\nThe p-value is statistically significant, and the z-score is not positive.\nYou may reject the null hypothesis. The spatial distribution of high values and low values in the dataset is more spatially dispersed than would be expected if underlying spatial processes were random. A dispersed spatial pattern often reflects some type of competitive process—a feature with a high value repels other features with high values; similarly, a feature with a low value repels other features with low values.\n\n\n\nThe hypothesis:\nH0 : The attribute being analyzed is randomly distributed among the features in your study area.\nH1: The attribute being analyzed is not randomly distributed among the features in your study area.\nSince the above result has a p-value below 0.05 and a positive z-score, we can conclude with statistical evidence that the attribute a not randomly distributed and the spatial distribution of high values and/or low values in the dataset is more spatially clustered\nComputing Monte Carlo Moran’s I\nThe code chunk below performs permutation test for Moran’s I statistic by using moran.mc() of spdep. A total of 1000 simulation will be performed.\n\nset.seed(1234)\nbperm= moran.mc(hunan$GDPPC, \n                listw=rswm_q, \n                nsim=999, \n                zero.policy = TRUE, \n                na.action=na.omit)\nbperm\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  hunan$GDPPC \nweights: rswm_q  \nnumber of simulations + 1: 1000 \n\nstatistic = 0.30075, observed rank = 1000, p-value = 0.001\nalternative hypothesis: greater\n\n\nVisualising Monte Carlo Moran’s I\nWe will use a density plot to visualise the output of the Monte Carlo Moran’s I. First, we need to extract the res value and convert it into a dataframe. We then visualise the test statistic result using geom_density from the ggplot package.\n\nmonte_carlo <- as.data.frame(bperm[7])\n\nggplot(monte_carlo, aes(x=res)) + \n  geom_density(fill=\"lightblue\") +\n  geom_vline(aes(xintercept=0.30075),\n            color=\"blue\", linetype=\"dashed\", size=1) +\n  labs(title = \"Density plot of Monte Carlo Simulation of Moran's I\", x = \"Test Statistic\", y = \"Density\") +\n  theme_minimal() \n\n\n\n\nFrom the plot, we can see the actual Moran’s I statistic (blue line) is far outside the simulated data (shaded in blue), indicating a significant evidence of positive autocorrelation.\nGlobal Spatial Autocorrelation: Geary’s\nWe will now perform the Geary’c statistic testing by using the geary.test() function. We will also compute the Monte Carlo Geary’sc using the geary.mc.\n\ngeary.test(hunan$GDPPC, listw=rswm_q)\n\n\n    Geary C test under randomisation\n\ndata:  hunan$GDPPC \nweights: rswm_q \n\nGeary C statistic standard deviate = 3.6108, p-value = 0.0001526\nalternative hypothesis: Expectation greater than statistic\nsample estimates:\nGeary C statistic       Expectation          Variance \n        0.6907223         1.0000000         0.0073364 \n\nset.seed(1234)\ngperm=geary.mc(hunan$GDPPC, \n               listw=rswm_q, \n               nsim=999)\ngperm\n\n\n    Monte-Carlo simulation of Geary C\n\ndata:  hunan$GDPPC \nweights: rswm_q \nnumber of simulations + 1: 1000 \n\nstatistic = 0.69072, observed rank = 1, p-value = 0.001\nalternative hypothesis: greater\n\n\nBased on the result, the p-value is below the alpha value of 0.05 and therefore we can statistical evidence to reject the null hypothesis.\nVisualising the Monte Carlo Geary’s C\n\nmonte_carlo_geary <- as.data.frame(gperm[7])\n\nggplot(monte_carlo_geary, aes(x=res)) + \n  geom_density(fill=\"lightblue\") +\n  geom_vline(aes(xintercept=0.69072),\n            color=\"blue\", linetype=\"dashed\", size=1) +\n  labs(title = \"Density plot of Monte Carlo Simulation of Geary’s C\", x = \"Test Statistic\", y = \"Density\") +\n  theme_minimal() \n\n\n\n\nUnlike the Moran’s I where the statistical value is located on the right side of the density graph, the test statistic for Geary’s C is inversely related to Moran’s I where the value less than 1 indicates positive spatial autocorrelation, while a value larger than 1 points to negative spatial autocorrelation. Therefore, based on the plot, we can conclude there is significant evidence of positive autocorrelation."
  },
  {
    "objectID": "posts/Geo/Geospatial_Autocorrelation/index.html#spatial-correlogram",
    "href": "posts/Geo/Geospatial_Autocorrelation/index.html#spatial-correlogram",
    "title": "Introduction to Global and Local Measures of Spatial Autocorrelation",
    "section": "Spatial Correlogram",
    "text": "Spatial Correlogram\nA nonparametric spatial correlogram is another measure of overall spatial autocorrelation that does not rely on specifying a matrix of spatial weights. Instead, a local regression is fitted to the calculated covariances or correlations for all pairs of observations based on the distance between them. They show how correlated are pairs of spatial observations when you increase the distance (lag) between them - they are plots of some index of autocorrelation (Moran’s I or Geary’s c) against distance.Although correlograms are not as fundamental as variograms (a keystone concept of geostatistics), they are very useful as an exploratory and descriptive tool. For this purpose they actually provide richer information than variograms.\nCompute Moran’s I and Geary’C correlogram\nWe will utilise the sp.correlogram() function from the spdep package and compute a 6-lag spatial correlogram of GDPPC. We then use the plot() and print() function to visualise the output. The method function can take in three different inputs:\n\ncorr - correlation\nI - Moran’s I\nC - Geary’s C\n\nWe will illustrate the Moran’s I correlogram.\n\nMI_corr <- sp.correlogram(wm_q, \n                          hunan$GDPPC, \n                          order=6, \n                          method=\"I\", \n                          style=\"W\")\nplot(MI_corr)\n\n\n\nprint(MI_corr)\n\nSpatial correlogram for hunan$GDPPC \nmethod: Moran's I\n         estimate expectation   variance standard deviate Pr(I) two sided    \n1 (88)  0.3007500  -0.0114943  0.0043484           4.7351       2.189e-06 ***\n2 (88)  0.2060084  -0.0114943  0.0020962           4.7505       2.029e-06 ***\n3 (88)  0.0668273  -0.0114943  0.0014602           2.0496        0.040400 *  \n4 (88)  0.0299470  -0.0114943  0.0011717           1.2107        0.226015    \n5 (88) -0.1530471  -0.0114943  0.0012440          -4.0134       5.984e-05 ***\n6 (88) -0.1187070  -0.0114943  0.0016791          -2.6164        0.008886 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nNow, we will illustrate the Geary’C correlogram.\n\nGC_corr <- sp.correlogram(wm_q, \n                          hunan$GDPPC, \n                          order=6, \n                          method=\"C\", \n                          style=\"W\")\nplot(GC_corr)\n\n\n\nprint(GC_corr)\n\nSpatial correlogram for hunan$GDPPC \nmethod: Geary's C\n        estimate expectation  variance standard deviate Pr(I) two sided    \n1 (88) 0.6907223   1.0000000 0.0073364          -3.6108       0.0003052 ***\n2 (88) 0.7630197   1.0000000 0.0049126          -3.3811       0.0007220 ***\n3 (88) 0.9397299   1.0000000 0.0049005          -0.8610       0.3892612    \n4 (88) 1.0098462   1.0000000 0.0039631           0.1564       0.8757128    \n5 (88) 1.2008204   1.0000000 0.0035568           3.3673       0.0007592 ***\n6 (88) 1.0773386   1.0000000 0.0058042           1.0151       0.3100407    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nInterpretation of results\nFrom the results and plot shown above, we understand the relationship between Moran’s I and Geary’C (inverse). We also can identify lag 1 and 2 with statistical evidence to have a positive autocorrelation and lag 5 and 6 to have a negative correlation with p-values below 0.05."
  },
  {
    "objectID": "posts/Geo/Geospatial_Autocorrelation/index.html#cluster-and-outlier-analysis",
    "href": "posts/Geo/Geospatial_Autocorrelation/index.html#cluster-and-outlier-analysis",
    "title": "Introduction to Global and Local Measures of Spatial Autocorrelation",
    "section": "Cluster and Outlier Analysis",
    "text": "Cluster and Outlier Analysis\nLocal Indicators of Spatial Association (LISA) is a technique that allows analysts to identify areas on the map where data values are strongly positively or negatively correlated. We will now use techniques to detect clusters and/or outliers.\nComputing local Moran’s I\nTo compute local Moran’s I, the localmoran() function of spdep will be used. It computes Ii values, given a set of zi values and a listw object providing neighbour weighting information for the polygon associated with the zi values.\n\nfips <- order(hunan$County)\nlocalMI <- localmoran(hunan$GDPPC, rswm_q)\nhead(localMI)\n\n            Ii          E.Ii       Var.Ii        Z.Ii Pr(z != E(Ii))\n1 -0.001468468 -2.815006e-05 4.723841e-04 -0.06626904      0.9471636\n2  0.025878173 -6.061953e-04 1.016664e-02  0.26266425      0.7928094\n3 -0.011987646 -5.366648e-03 1.133362e-01 -0.01966705      0.9843090\n4  0.001022468 -2.404783e-07 5.105969e-06  0.45259801      0.6508382\n5  0.014814881 -6.829362e-05 1.449949e-03  0.39085814      0.6959021\n6 -0.038793829 -3.860263e-04 6.475559e-03 -0.47728835      0.6331568\n\n\nlocalmoran() function returns a matrix of values whose columns are:\n\nIi: the local Moran’s I statistics\nE.Ii: the expectation of local moran statistic under the randomisation hypothesis\nVar.Ii: the variance of local moran statistic under the randomisation hypothesis\nZ.Ii:the standard deviate of local moran statistic\nPr(): the p-value of local moran statistic\n\nThe z-scores and pseudo p-values represent the statistical significance of the computed index values.\nThe code chunk below list the content of the local Moran matrix derived by using printCoefmat().\n\nprintCoefmat(data.frame(localMI[fips,], row.names=hunan$County[fips]), check.names=FALSE)\n\n                       Ii        E.Ii      Var.Ii        Z.Ii Pr.z....E.Ii..\nAnhua         -2.2493e-02 -5.0048e-03  5.8235e-02 -7.2467e-02         0.9422\nAnren         -3.9932e-01 -7.0111e-03  7.0348e-02 -1.4791e+00         0.1391\nAnxiang       -1.4685e-03 -2.8150e-05  4.7238e-04 -6.6269e-02         0.9472\nBaojing        3.4737e-01 -5.0089e-03  8.3636e-02  1.2185e+00         0.2230\nChaling        2.0559e-02 -9.6812e-04  2.7711e-02  1.2932e-01         0.8971\nChangning     -2.9868e-05 -9.0010e-09  1.5105e-07 -7.6828e-02         0.9388\nChangsha       4.9022e+00 -2.1348e-01  2.3194e+00  3.3590e+00         0.0008\nChengbu        7.3725e-01 -1.0534e-02  2.2132e-01  1.5895e+00         0.1119\nChenxi         1.4544e-01 -2.8156e-03  4.7116e-02  6.8299e-01         0.4946\nCili           7.3176e-02 -1.6747e-03  4.7902e-02  3.4200e-01         0.7324\nDao            2.1420e-01 -2.0824e-03  4.4123e-02  1.0297e+00         0.3032\nDongan         1.5210e-01 -6.3485e-04  1.3471e-02  1.3159e+00         0.1882\nDongkou        5.2918e-01 -6.4461e-03  1.0748e-01  1.6338e+00         0.1023\nFenghuang      1.8013e-01 -6.2832e-03  1.3257e-01  5.1198e-01         0.6087\nGuidong       -5.9160e-01 -1.3086e-02  3.7003e-01 -9.5104e-01         0.3416\nGuiyang        1.8240e-01 -3.6908e-03  3.2610e-02  1.0305e+00         0.3028\nGuzhang        2.8466e-01 -8.5054e-03  1.4152e-01  7.7931e-01         0.4358\nHanshou        2.5878e-02 -6.0620e-04  1.0167e-02  2.6266e-01         0.7928\nHengdong       9.9964e-03 -4.9063e-04  6.7742e-03  1.2742e-01         0.8986\nHengnan        2.8064e-02 -3.2160e-04  3.7597e-03  4.6294e-01         0.6434\nHengshan      -5.8201e-03 -3.0437e-05  5.1076e-04 -2.5618e-01         0.7978\nHengyang       6.2997e-02 -1.3046e-03  2.1865e-02  4.3486e-01         0.6637\nHongjiang      1.8790e-01 -2.3019e-03  3.1725e-02  1.0678e+00         0.2856\nHuarong       -1.5389e-02 -1.8667e-03  8.1030e-02 -4.7503e-02         0.9621\nHuayuan        8.3772e-02 -8.5569e-04  2.4495e-02  5.4072e-01         0.5887\nHuitong        2.5997e-01 -5.2447e-03  1.1077e-01  7.9685e-01         0.4255\nJiahe         -1.2431e-01 -3.0550e-03  5.1111e-02 -5.3633e-01         0.5917\nJianghua       2.8651e-01 -3.8280e-03  8.0968e-02  1.0204e+00         0.3076\nJiangyong      2.4337e-01 -2.7082e-03  1.1746e-01  7.1800e-01         0.4728\nJingzhou       1.8270e-01 -8.5106e-04  2.4363e-02  1.1759e+00         0.2396\nJinshi        -1.1988e-02 -5.3666e-03  1.1334e-01 -1.9667e-02         0.9843\nJishou        -2.8680e-01 -2.6305e-03  4.4028e-02 -1.3543e+00         0.1756\nLanshan        6.3334e-02 -9.6365e-04  2.0441e-02  4.4972e-01         0.6529\nLeiyang        1.1581e-02 -1.4948e-04  2.5082e-03  2.3422e-01         0.8148\nLengshuijiang -1.7903e+00 -8.2129e-02  2.1598e+00 -1.1623e+00         0.2451\nLi             1.0225e-03 -2.4048e-07  5.1060e-06  4.5260e-01         0.6508\nLianyuan      -1.4672e-01 -1.8983e-03  1.9145e-02 -1.0467e+00         0.2952\nLiling         1.3774e+00 -1.5097e-02  4.2601e-01  2.1335e+00         0.0329\nLinli          1.4815e-02 -6.8294e-05  1.4499e-03  3.9086e-01         0.6959\nLinwu         -2.4621e-03 -9.0703e-06  1.9258e-04 -1.7676e-01         0.8597\nLinxiang       6.5904e-02 -2.9028e-03  2.5470e-01  1.3634e-01         0.8916\nLiuyang        3.3688e+00 -7.7502e-02  1.5180e+00  2.7972e+00         0.0052\nLonghui        8.0801e-01 -1.1377e-02  1.5538e-01  2.0787e+00         0.0376\nLongshan       7.5663e-01 -1.1100e-02  3.1449e-01  1.3690e+00         0.1710\nLuxi           1.8177e-01 -2.4855e-03  3.4249e-02  9.9561e-01         0.3194\nMayang         2.1852e-01 -5.8773e-03  9.8049e-02  7.1663e-01         0.4736\nMiluo          1.8704e+00 -1.6927e-02  2.7925e-01  3.5715e+00         0.0004\nNan           -9.5789e-03 -4.9497e-04  6.8341e-03 -1.0988e-01         0.9125\nNingxiang      1.5607e+00 -7.3878e-02  8.0012e-01  1.8274e+00         0.0676\nNingyuan       2.0910e-01 -7.0884e-03  8.2306e-02  7.5356e-01         0.4511\nPingjiang     -9.8964e-01 -2.6457e-03  5.6027e-02 -4.1698e+00         0.0000\nQidong         1.1806e-01 -2.1207e-03  2.4747e-02  7.6396e-01         0.4449\nQiyang         6.1966e-02 -7.3374e-04  8.5743e-03  6.7712e-01         0.4983\nRucheng       -3.6992e-01 -8.8999e-03  2.5272e-01 -7.1814e-01         0.4727\nSangzhi        2.5053e-01 -4.9470e-03  6.8000e-02  9.7972e-01         0.3272\nShaodong      -3.2659e-02 -3.6592e-05  5.0546e-04 -1.4510e+00         0.1468\nShaoshan       2.1223e+00 -5.0227e-02  1.3668e+00  1.8583e+00         0.0631\nShaoyang       5.9499e-01 -1.1253e-02  1.3012e-01  1.6807e+00         0.0928\nShimen        -3.8794e-02 -3.8603e-04  6.4756e-03 -4.7729e-01         0.6332\nShuangfeng     9.2835e-03 -2.2867e-03  3.1516e-02  6.5174e-02         0.9480\nShuangpai      8.0591e-02 -3.1366e-04  8.9838e-03  8.5358e-01         0.3933\nSuining        3.7585e-01 -3.5933e-03  4.1870e-02  1.8544e+00         0.0637\nTaojiang      -2.5394e-01 -1.2395e-03  1.4477e-02 -2.1002e+00         0.0357\nTaoyuan        1.4729e-02 -1.2039e-04  8.5103e-04  5.0903e-01         0.6107\nTongdao        4.6482e-01 -6.9870e-03  1.9879e-01  1.0582e+00         0.2900\nWangcheng      4.4220e+00 -1.1067e-01  1.3596e+00  3.8873e+00         0.0001\nWugang         7.1003e-01 -7.8144e-03  1.0710e-01  2.1935e+00         0.0283\nXiangtan       2.4530e-01 -3.6457e-04  3.2319e-03  4.3213e+00         0.0000\nXiangxiang     2.6271e-01 -1.2703e-03  2.1290e-02  1.8092e+00         0.0704\nXiangyin       5.4525e-01 -4.7442e-03  7.9236e-02  1.9539e+00         0.0507\nXinhua         1.1810e-01 -6.2649e-03  8.6001e-02  4.2409e-01         0.6715\nXinhuang       1.5725e-01 -4.1820e-03  3.6648e-01  2.6667e-01         0.7897\nXinning        6.8928e-01 -9.6674e-03  2.0328e-01  1.5502e+00         0.1211\nXinshao        5.7578e-02 -8.5932e-03  1.1769e-01  1.9289e-01         0.8470\nXintian       -7.4050e-03 -5.1493e-03  1.0877e-01 -6.8395e-03         0.9945\nXupu           3.2406e-01 -5.7468e-03  5.7735e-02  1.3726e+00         0.1699\nYanling       -6.9021e-02 -5.9211e-04  9.9306e-03 -6.8667e-01         0.4923\nYizhang       -2.6844e-01 -2.2463e-03  4.7588e-02 -1.2202e+00         0.2224\nYongshun       6.3064e-01 -1.1350e-02  1.8830e-01  1.4795e+00         0.1390\nYongxing       4.3411e-01 -9.0735e-03  1.5088e-01  1.1409e+00         0.2539\nYou            7.8750e-02 -7.2728e-03  1.2116e-01  2.4714e-01         0.8048\nYuanjiang      2.0004e-04 -1.7760e-04  2.9798e-03  6.9181e-03         0.9945\nYuanling       8.7298e-03 -2.2981e-06  2.3221e-05  1.8121e+00         0.0700\nYueyang        4.1189e-02 -1.9768e-04  2.3113e-03  8.6085e-01         0.3893\nZhijiang       1.0476e-01 -7.8123e-04  1.3100e-02  9.2214e-01         0.3565\nZhongfang     -2.2685e-01 -2.1455e-03  3.5927e-02 -1.1855e+00         0.2358\nZhuzhou        3.2864e-01 -5.2432e-04  7.2391e-03  3.8688e+00         0.0001\nZixing        -7.6849e-01 -8.8210e-02  9.4057e-01 -7.0144e-01         0.4830\n\n\nMapping the local Moran’s I\nWe have to combine the local Moran’s dataframe with the our exisiting Hunan spatialdataframe before plotting. We will use the cbind() function.\n\nhunan.localMI <- cbind(hunan,localMI) %>%\n  rename(Pr.Ii = Pr.z....E.Ii..)\n\nMapping local Moran’s I values\nUsing choropleth mapping functions of tmap package, we can plot the local Moran’s I values by using the code chinks below.\n\ntm_shape(hunan.localMI) +\n  tm_fill(col = \"Ii\", \n          style = \"pretty\",\n          palette = \"RdBu\",\n          title = \"Local Moran I value\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\nA positive value for I indicates that a feature has neighboring features with similarly high or low attribute values; this feature is part of a cluster. A negative value for I indicates that a feature has neighboring features with dissimilar values; this feature is an outlier. The choropleth shows there is evidence for both positive and negative Ii values. However, it is useful to consider the p-values for each of these values, as consider above.\nMapping local Moran’s I p-values\nThe code chunks below produce a choropleth map of Moran’s I p-values by using functions of tmap package.\n\ntm_shape(hunan.localMI) +\n  tm_fill(col = \"Pr.Ii\", \n          breaks=c(-Inf, 0.001, 0.01, 0.05, 0.1, Inf),\n          palette=\"-Blues\", \n          title = \"Local Moran's I p-values\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\nMapping Moran’s I values and p-values\nIt will be more useful to understand which area Moran’s I value is statistically significant. We will create another dataframe with only areas that are statistically significant with a p value < 0.05. We will then plot the overlay above the base map and identify the area with positive and negative I value.\n\nhunan.localMI.sub <- hunan.localMI %>%\n  filter (Pr.Ii <= 0.05)\n\nimap <- tm_shape(hunan.localMI) +\n  tm_fill(\"white\") +\n  tm_borders(\"grey\", lwd = 0.5, alpha = 0.5) +\n  tm_shape(hunan.localMI.sub) +\n  tm_fill (col = \"Ii\",\n           style = \"pretty\",\n           palette = \"RdBu\",\n           title = \"Local Moran I value\") +\n  tm_borders(\"grey\", lwd = 0.5, alpha = 0.5) +\n  tm_layout(main.title = \"Local Moran I value for p < 0.05\",\n            main.title.size = 0.7,\n            main.title.fontface = \"bold\")\n\nimap \n\n\n\n\nFrom our plot, we are able to derive that 11 areas have I values that are statically significant, and 2 areas have negative I value which means a dissimilar features."
  },
  {
    "objectID": "posts/Geo/Geospatial_Autocorrelation/index.html#creating-a-lisa-cluster-map",
    "href": "posts/Geo/Geospatial_Autocorrelation/index.html#creating-a-lisa-cluster-map",
    "title": "Introduction to Global and Local Measures of Spatial Autocorrelation",
    "section": "Creating a LISA Cluster Map",
    "text": "Creating a LISA Cluster Map\nThe LISA Cluster Map shows the significant locations color coded by type of spatial autocorrelation. The first step before we can generate the LISA cluster map is to plot the Moran scatterplot.\nPlotting Moran scatterplot\nThe Moran scatterplot is an illustration of the relationship between the values of the chosen attribute at each location and the average value of the same attribute at neighboring locations.\nThe code chunk below plots the Moran scatterplot of GDPPC 2012 by using moran.plot() of spdep.\n\nnci <- moran.plot(hunan$GDPPC, rswm_q,\n                  labels=as.character(hunan$County), \n                  xlab=\"GDPPC 2012\", \n                  ylab=\"Spatially Lag GDPPC 2012\")\n\n\n\n\nIn the upper right quadrant, there are cases where both the attribute value and the local average are greater than the global average. Similarly, in the lower left quadrant, there are cases where both the attribute value and the local mean are below the global mean. These conditions confirm positive spatial autocorrelation. Cases in the other two quadrants show negative spatial autocorrelation.\nPreparing LISA map classes\nWe will now prepare the LISA map classes. We first need to retrieve the quadrant for each area.\n\nquadrant <- vector(mode=\"numeric\",length=nrow(localMI))\n\nNext, we scale the GDPPC.\n\nDV <- scale(hunan.localMI$GDPPC)   \n\nThis is follow by finding the lag of the scaled GDPPC.\n\nC_mI <- lag.listw(rswm_q, DV)   \n\nUsing the Moran Scatterplot below, we filter all the area with p value < 0.05 and identify significant areas. We can see that the plot below is align with our Moran I plot where there are a total of 11 significant areas, 2 areas that are outliers (LH), and 9 areas that are clusters (7 HH and 2 LL).\n\nMIplot <- data.frame(cbind(DV,C_mI,localMI[,5]))\nMIplot <- MIplot %>%\n  filter (X3 < 0.05)\nplot(x = MIplot$X1, y = MIplot$X2, main = \"Moran Scatterplot PPOV\", xlab = \"scaled GDDPC\", ylab = \"Lag scaled GDPPC\")\nabline(h = 0, v = 0)\n\n\n\n\nWe will now then define the quadrant based on the following criteria and place non-significant Moran (p value < 0.05) in the category 0.:\n\nsignif <- 0.05 \nquadrant[DV >0 & C_mI>0] <- 4      \nquadrant[DV <0 & C_mI<0] <- 1      \nquadrant[DV <0 & C_mI>0] <- 2\nquadrant[DV >0 & C_mI<0] <- 3\nquadrant[localMI[,5]>signif] <- 0\n\nPlotting LISA map\nOnce the quadrant of each area has been decided, we will now plot the LISA map using tmap. We will plot both the base map with the GDDPC distribution and the LISA map to better understand the relationship.\n\ntmap_mode(\"plot\")\nhunan.localMI$quadrant <- quadrant\ncolors <- c(\"#ffffff\", \"#2c7bb6\", \"#abd9e9\", \"#fdae61\", \"#d7191c\")\nclusters <- c(\"insignificant\", \"low-low\", \"low-high\", \"high-low\", \"high-high\")\n\nlisamap <- tm_shape(hunan.localMI) +\n  tm_fill(col = \"quadrant\", \n          style = \"cat\", \n          palette = colors[c(sort(unique(quadrant)))+1], \n          labels = clusters[c(sort(unique(quadrant)))+1]) +\n  tm_view(set.zoom.limits = c(11,17)) +\n  tm_borders(alpha=0.5) +\n  tm_layout(main.title = \"LISA Map with Quadrant\", main.title.size = 0.7,\n            main.title.fontface = \"bold\", main.title.position = \"center\")\n\nbasemap <- tm_shape(hunan) +\n  tm_fill(\"GDPPC\",\n          n = 5,\n          style = \"quantile\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"GDDPC using Quantile classification\", main.title.size = 0.7,\n            main.title.fontface = \"bold\", main.title.position = \"center\")\n\ntmap_arrange (imap,lisamap,basemap)\n\n\n\n\nBased on the map plot above, we can see that the Moran I value map provide us with insights on which areas are consider outliers or clusters. The LISA map provide us with more in-depth information on whether the outliers are HL or LH and whether the clusters are HH or LL. These attributes can further be confirmed by referencing the base map on the right."
  },
  {
    "objectID": "posts/Geo/Geospatial_Autocorrelation/index.html#hot-spot-and-cold-spot-area-analysis",
    "href": "posts/Geo/Geospatial_Autocorrelation/index.html#hot-spot-and-cold-spot-area-analysis",
    "title": "Introduction to Global and Local Measures of Spatial Autocorrelation",
    "section": "Hot Spot and Cold Spot Area Analysis",
    "text": "Hot Spot and Cold Spot Area Analysis\nBy grouping points of occurrence into polygons or converging points that are close to one another based on a calculated distance, Hotspot Analysis uses vectors to locate statistically significant hot spots and cold spots in your data.\nGetis and Ord’s G-Statistics\nThe Getis and Ord’s G-statistics is used to measure the degree of clustering for either high or low values. The High/Low Clustering (Getis-Ord General G) statistic is an inferential statistic, which means that the null hypothesis is used to interpret the analysis’s findings. It is assumed that there is no spatial clustering of feature values when using the High/Low Clustering (General G) statistic.\nThe analysis consists of three steps:\n\nDeriving spatial weight matrix\nComputing Gi statistics\nMapping Gi statistics\nDeriving distance-based weight matrix\nFirst, we need to define a new set of neighbours. Whist the spatial autocorrelation considered units which shared borders, for Getis-Ord we are defining neighbours based on distance.\nThere are two type of , they are:\n\nfixed distance weight matrix\nadaptive distance weight matrix\n\nThese methods were explained on the previous post and therefore will not be elaborated here.\nThe code chunk below is to generate the fixed distance weight matrix:\n\ncoords <- st_centroid(st_geometry(hunan))\nk1 <- knn2nb(knearneigh(coords))\nk1dists <- unlist(nbdists(k1, coords))\nwm_d59 <- dnearneigh(coords, 0, 59)\nwm59_lw <- nb2listw(wm_d59, style = 'W')\n\nThe output spatial weights object is called wm59_lw.\nThe code chunk below is to generate the adaptive distance weight matrix:\n\nknn <- knn2nb(knearneigh(coords, k=8))\nknn_lw <- nb2listw(knn, style = 'B')"
  },
  {
    "objectID": "posts/Geo/Geospatial_Autocorrelation/index.html#computing-gi-statistics",
    "href": "posts/Geo/Geospatial_Autocorrelation/index.html#computing-gi-statistics",
    "title": "Introduction to Global and Local Measures of Spatial Autocorrelation",
    "section": "Computing Gi statistics",
    "text": "Computing Gi statistics\nGi statistics using fixed distance\n\nfips <- order(hunan$County)\ngi.fixed <- localG(hunan$GDPPC, wm59_lw)\ngi.fixed\n\n [1]  0.43607584 -0.26550565 -0.07303367  0.41301703  0.37236574 -0.37751078\n [7]  2.86389882  2.79435042  5.21612540  0.22823660  0.95103535 -0.53633423\n[13]  0.17676156  1.19556402 -0.03302061  1.24516289 -0.58575676 -0.41968056\n[19]  0.39900582  0.01205611 -0.14571653 -0.02715869 -0.31861529 -0.74894605\n[25] -0.96170058 -0.79685134 -1.03394977 -0.46097916 -0.28044932 -0.26667151\n[31] -0.88616861 -0.85547697 -0.92214318 -1.16232860 -0.14981495  0.20687930\n[37] -0.56988403 -1.25929908 -1.45225651 -1.54067112 -1.39501141 -1.63849079\n[43] -1.31411071 -0.76794446 -0.19288934  3.24270315  1.80919136 -0.85166126\n[49] -0.51198447 -0.83454636 -0.90817907 -1.54108152 -1.00485524 -1.07508016\n[55] -1.63107596 -0.74347225  0.41884239  0.83294375 -0.55849880 -0.44971882\n[61] -0.49323874 -1.08338678  0.04297905 -0.06907285  0.13633747  2.20341174\n[67]  2.69032995  4.45370322  0.17800261 -0.12931859  0.73780663 -1.24691266\n[73]  0.63536412  0.80351189 -0.99800120  1.21584987 -0.48719642  1.62617404\n[79] -1.06041680  0.84902447 -0.64525926 -0.43486100 -0.09446279  4.42439262\n[85] -0.22952462  1.36459800 -0.84344663 -0.71800062\nattr(,\"cluster\")\n [1] Low  Low  High High High High High High High Low  Low  High Low  Low  Low \n[16] High High High High Low  High High Low  Low  High Low  Low  Low  Low  Low \n[31] Low  Low  Low  High Low  Low  Low  Low  Low  Low  High Low  Low  Low  Low \n[46] High High Low  Low  Low  Low  High Low  Low  Low  Low  Low  High Low  Low \n[61] Low  Low  Low  High High High Low  High Low  Low  High Low  High High Low \n[76] High Low  Low  Low  Low  Low  Low  High High Low  High Low  Low \nLevels: Low High\nattr(,\"gstari\")\n[1] FALSE\nattr(,\"call\")\nlocalG(x = hunan$GDPPC, listw = wm59_lw)\nattr(,\"class\")\n[1] \"localG\"\n\n\nThe output of localG() is a vector of G or Gstar values, with attributes “gstari” set to TRUE or FALSE, “call” set to the function call, and class “localG”.\nThe Gi statistics is represented as a Z-score. Greater values represent a greater intensity of clustering and the direction (positive or negative) indicates high or low clusters.\nNext, we will join the Gi values to their corresponding hunan sf data frame by using the code chunk below.\n\nhunan.gi <- cbind(hunan, as.matrix(gi.fixed)) %>%\n  rename(gstat_fixed = as.matrix.gi.fixed.)\n\nIn fact, the code chunk above performs three tasks. First, it convert the output vector (i.e. gi.fixed) into r matrix object by using as.matrix(). Next, cbind() is used to join hunan@data and gi.fixed matrix to produce a new SpatialPolygonDataFrame called hunan.gi. Lastly, the field name of the gi values is renamed to gstat_fixed by using rename().\nMapping Gi values with fixed distance weights\nThe code chunk below shows the functions used to map the Gi values derived using fixed distance weight matrix.\n\ngdppc <- qtm(hunan, \"GDPPC\")\n\nGimap <-tm_shape(hunan.gi) +\n  tm_fill(col = \"gstat_fixed\", \n          style = \"pretty\",\n          palette=\"-RdBu\",\n          title = \"local Gi\") +\n  tm_borders(alpha = 0.5)\n\ntmap_arrange(gdppc, Gimap, asp=1, ncol=2)\n\n\n\n\nBased on the map plot, we can observe that the hotspots (higher GDDPC) are located within the northern eastern region and the coldspot (lower GDDPC) is located at the southern western and northern western region. The area consist of mostly cold spots than hot spots which might signify a generally uneven distribution of wealth within the Hunan province.\nGi statistics using adaptive distance\nThe code chunk below are used to compute the Gi values for GDPPC2012 by using an adaptive distance weight matrix (i.e knb_lw).\n\nfips <- order(hunan$County)\ngi.adaptive <- localG(hunan$GDPPC, knn_lw)\nhunan.gi <- cbind(hunan, as.matrix(gi.adaptive)) %>%\n  rename(gstat_adaptive = as.matrix.gi.adaptive.)\n\nMapping Gi values with adaptive distance weights\nIt is time for us to visualise the locations of hot spot and cold spot areas. The choropleth mapping functions of tmap package will be used to map the Gi values.\nThe code chunk below shows the functions used to map the Gi values derived using fixed distance weight matrix.\n\nGimap <- tm_shape(hunan.gi) + \n  tm_fill(col = \"gstat_adaptive\", \n          style = \"pretty\", \n          palette=\"-RdBu\", \n          title = \"local Gi\") + \n  tm_borders(alpha = 0.5)\n\ntmap_arrange(gdppc, \n             Gimap, \n             asp=1, \n             ncol=2)\n\n\n\n\nThe map plot differ slightly from the fixed weight GI statistic where we see a more concentrated hotspot at the north east region and only one concentrated cold spot at the south west region. The eastern region are mostly region with higher GDDPC compared to the western region (more hotspot in the East and more cold spot in the West.)"
  },
  {
    "objectID": "posts/Geo/Geospatial_Exercise/index.html",
    "href": "posts/Geo/Geospatial_Exercise/index.html",
    "title": "Geospatial Analytics for Social Good - Understanding Nigeria Water functional and non-functional water point rate",
    "section": "",
    "text": "The government of Nigeria deemed the Water, Sanitation, and Hygiene (WASH) sector to be in a state of emergency in 2018. In 2019, 60 million Nigerians were without access to basic drinking water due to a combination of bad infrastructure, a lack of necessary human resources, low investment, and a weak enabling regulatory environment, among other issues. 167 million people lacked access to even the most basic handwashing facilities, and 80 million lacked access to better sanitation facilities.\nOnly half of rural families have access to improved sanitation, and 39% of households in these areas conduct open defecation, a percentage that has barely changed since 1990.\n\nGeospatial analytics hold tremendous potential to address complex problems facing society. In this study, I will be applying appropriate global and local measures of spatial Association techniques to reveals the spatial patterns of Not Functional water points."
  },
  {
    "objectID": "posts/Geo/Geospatial_Exercise/index.html#libraries",
    "href": "posts/Geo/Geospatial_Exercise/index.html#libraries",
    "title": "Geospatial Analytics for Social Good - Understanding Nigeria Water functional and non-functional water point rate",
    "section": "Libraries",
    "text": "Libraries\nFor this study, we will use the following packages from CRAN.\n\n\nsf - Support for simple features, a standardized way to encode spatial vector data. Binds to ‘GDAL’ for reading and writing data, to ‘GEOS’ for geometrical operations, and to ‘PROJ’ for projection conversions and datum transformations. Uses by default the ‘s2’ package for spherical geometry operations on ellipsoidal (long/lat) coordinates.\n\ntidyverse - Loading the core tidyverse packages which will be used for data wrangling and visualisation.\n\ntmap - Thematic maps are geographical maps in which spatial data distributions are visualized. This package offers a flexible, layer-based, and easy to use approach to create thematic maps, such as choropleths and bubble maps.\n\nspdep - A collection of functions to create spatial weights matrix objects from polygon ‘contiguities’, from point patterns by distance and tessellations, for summarizing these objects, and for permitting their use in spatial data analysis, including regional aggregation by minimum spanning tree; a collection of tests for spatial ‘autocorrelation’.\n\npatchwork - Combine separate ggplots into the same graphic.\n\n\nShow the codepacman::p_load(sf, spdep, tmap, tidyverse,patchwork)"
  },
  {
    "objectID": "posts/Geo/Geospatial_Exercise/index.html#data-preparation",
    "href": "posts/Geo/Geospatial_Exercise/index.html#data-preparation",
    "title": "Geospatial Analytics for Social Good - Understanding Nigeria Water functional and non-functional water point rate",
    "section": "Data Preparation",
    "text": "Data Preparation\nTwo dataset will be used for this study:\n\nNigeria.shp: A shapefile of Nigeria from Humanitarian Data Exchange Portal that consist of all the Level-2 Administrative Boundary (also known as Local Government Area)\nNigeriaAttribute.csv: A csv file containing multiple water point attributes of each Level-2 Administrative Nigeria Boundary from the WPdx Global Data Repositories\n\n\nImporting of data\nWe will use the st_read to import the shape file and read_csv to import the aspatial data into the R environment.\n\n\nReading layer `geoBoundaries-NGA-ADM2' from data source \n  `C:\\jordanong09\\ISSS624_Geospatial\\posts\\Geo\\Geospatial_Exercise\\data' \n  using driver `ESRI Shapefile'\nSimple feature collection with 774 features and 5 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2.668534 ymin: 4.273007 xmax: 14.67882 ymax: 13.89442\nGeodetic CRS:  WGS 84\n\n\n\nShow the codenigeria <- st_read(dsn = \"data\", \n                 layer = \"geoBoundaries-NGA-ADM2\")\n\nnigeria_attribute <- read_csv(\"data/nigeriaattribute.csv\")\nnigeria <- nigeria %>%\n  st_transform(crs = 26391)\n\n\nData Wrangling\nThe practice of correcting or deleting inaccurate, damaged, improperly formatted, duplicate, or incomplete data from a dataset is known as data wrangling. There are numerous ways for data to be duplicated or incorrectly categorized when merging multiple data sources. We willl now proceed to ensure our data is cleaned before conducting our analysis.\nChecking of duplicated area name\nFirstly, we will order our dataframe by alphabetical order based on the shapeName. We will then use the duplicated function to retrieve all the shapeName that has duplicates and store it in a list. From the result below, we identified 12 shapeNames that are duplicates.\n\nShow the codenigeria <- (nigeria[order(nigeria$shapeName), ])\n\nduplicate_area <- nigeria$shapeName[ nigeria$shapeName %in% nigeria$shapeName[duplicated(nigeria$shapeName)] ]\n\nduplicate_area\n\n [1] \"Bassa\"    \"Bassa\"    \"Ifelodun\" \"Ifelodun\" \"Irepodun\" \"Irepodun\"\n [7] \"Nasarawa\" \"Nasarawa\" \"Obi\"      \"Obi\"      \"Surulere\" \"Surulere\"\n\n\nNext, we will leverage on the interactive viewer of tmap to check the location of each area. Through the use of Google, we are able to retrieve the actual name and state of the areas. The table below shows the index and the actual name of the area.\n\n\nIndex\nActual Area Name\n\n\n\n94\nBassa (Kogi)\n\n\n95\nBassa (Plateau)\n\n\n304\nIfelodun (Kwara)\n\n\n305\nIfelodun (Osun)\n\n\n355\nIrepodun (Kwara)\n\n\n356\nIrepodun (Osun)\n\n\n518\nNassarawa\n\n\n546\nObi (Benue)\n\n\n547\nObi(Nasarawa)\n\n\n693\nSurulere (lagos)\n\n\n694\nSurulere (Oyo)\n\n\n\n\nShow the codetmap_mode(\"view\")\n\ntm_shape(nigeria[nigeria$shapeName %in% duplicate_area,]) +\n  tm_polygons()\n\n\n\n\n\nShow the codetmap_mode(\"plot\")\n\n\nWe will now access the individual index of the nigeria data frame and change the value. Lastly, we use the length() function to ensure there is no more duplicated shapeName.\n\nShow the codenigeria$shapeName[c(94,95,304,305,355,356,519,546,547,693,694)] <- c(\"Bassa (Kogi)\",\"Bassa (Plateau)\",\n                                                                               \"Ifelodun (Kwara)\",\"Ifelodun (Osun)\",\n                                                                               \"Irepodun (Kwara)\",\"Irepodun (Osun)\",\n                                                                               \"Nassarawa\",\"Obi (Benue)\",\"Obi(Nasarawa)\",\n                                                                               \"Surulere (Lagos)\",\"Surulere (Oyo)\")\n\nlength((nigeria$shapeName[ nigeria$shapeName %in% nigeria$shapeName[duplicated(nigeria$shapeName)] ]))\n\n[1] 0\n\n\nProjection of sf dataframe\nSince our aspatial data was imported to a tibble dataframe, we will need to convert it to an sf object. First, we rename the columns for ease of representation using the rename() function from dyplr. We then only retain the columns required for analysis such as the name of the area name, latitude, longitude and status. We realised there were NA values within the status column, we will replace the NA values with Unknown using the mutate() function.\nWe will then use the st_as_sf() function to convert the dataframe to an sf object. We will have to input the column that specify the longitude and latitude, and lastly, the CRS projection of the coordinates.\n\nShow the codenigeriaT <- nigeria_attribute  %>%\n  rename (\"Country\" = \"#clean_country_name\",\n          \"clean_adm2\" = \"#clean_adm2\",\n          \"status\" = \"#status_clean\",\n          \"lat\" = \"#lat_deg\",\n          \"long\" = \"#lon_deg\") %>%\n  select (clean_adm2,status,lat,long) %>%\n  mutate(status = replace_na(status, \"Unknown\"))\n\nnigeriaT_sf <- st_as_sf(nigeriaT, coords = c(\"long\", \"lat\"),  crs = 4326)\n\n\nWe will now transform the coordinates from 4326 to 26391 projection using the st_transform() function.\n\nShow the codenigeriaT_sf <- st_transform(nigeriaT_sf, crs = 26391)\n\nst_crs (nigeria)\nst_crs (nigeriaT_sf)\n\n\nVisualising of distribution using ggplot\nWe will use the ggplot function to visualise the distribution of the different status. To sort the distribution by descending order fct_infreq will be use.\n\nShow the codeggplot(data= nigeriaT_sf, \n       aes(x= fct_infreq(status))) +\n  geom_bar(aes(fill = status), show.legend = FALSE) +\n  geom_text(stat = 'count',\n           aes(label= paste0(stat(count), ', ', \n                             round(stat(count)/sum(stat(count))*100, \n                             2), '%')), vjust= -0.5, size= 2.5) +\n  labs(y= 'No. of\\nOccurence', x= 'Status',\n       title = \"Distribution of Water Tap Status\") +\n  theme(axis.title.y= element_text(angle=0), axis.ticks.x= element_blank(),\n        panel.background= element_blank(), axis.line= element_line(color= 'grey'),\n        axis.text.x = element_text(angle = 90, vjust = 0.5))\n\n\n\n\n\n\nggplot of status frequency count\n\n\n\n\n\n\n\n\n\n\nExtracting Status of Water Point\nSince this analysis is on the functionality of the water taps, we have to extract the number of functional and non-functional water taps from the nigeriaT_sf dataframe. The status column reveal the status of the water tap. We will now see what values are recorded by using the unique() function. From the result below, we can identify mainly four categories of statuses; Functional, Non-Functional, Abandoned, Unknown.\nNote: For this analysis, abandoned water taps will be analysed under non-functional.\nFrom the result below, we can identify a pattern to classify the status based on our criteria. By extracting the first word of the sentence before the punctuation, we will be able to extract the word; Unknown, Abandoned, Functional and Non. This will assist us in grouping these status.\n\nShow the codeunique(nigeriaT_sf$status)\n\n[1] \"Unknown\"                          \"Abandoned/Decommissioned\"        \n[3] \"Non-Functional\"                   \"Functional\"                      \n[5] \"Functional but needs repair\"      \"Functional but not in use\"       \n[7] \"Abandoned\"                        \"Non functional due to dry season\"\n[9] \"Non-Functional due to dry season\"\n\n\nTo replace the original values, we will use the gsub() function. A regular expression “([A-Za-z]+).*” is used to extract all letters and \\\\1 is used to back reference the first capturing group. The result below shows the unique values left within the column.\n\nShow the codenigeriaT_sf$status <- gsub(\"([A-Za-z]+).*\", \"\\\\1\", nigeriaT_sf$status)\nunique(nigeriaT_sf$status)\n\n[1] \"Unknown\"    \"Abandoned\"  \"Non\"        \"Functional\"\n\n\nComputing Ratio of Functional and Non Functional Water Point\nInstead of creating another data frame to store the new values, we will leverage on the filter function by using the single square bracket “[]” operator. The coordinates beings with a row position and that will be used for our filtering condition. R Dataframe. Since our water taps are point data, we will use st_intersects() to retrieve every geometry point that intersect with the polygon of the Nigeria ADM area, and subsequently use the function lengths() to retrieve the number of points that intersects with the polygon.\n\nShow the codenigeria$functional <- lengths(st_intersects(nigeria, nigeriaT_sf[nigeriaT_sf$status == \"Functional\",]))\nnigeria$nonfunctional <- lengths(st_intersects(nigeria, nigeriaT_sf[nigeriaT_sf$status == \"Non\",])) + lengths(st_intersects(nigeria, nigeriaT_sf[nigeriaT_sf$status == \"Abandoned\",]))\nnigeria$unknown <- lengths(st_intersects(nigeria, nigeriaT_sf[nigeriaT_sf$status == \"Unknown\",]))\nnigeria$total <- lengths(st_intersects(nigeria, nigeriaT_sf))\n\n\nNext, for areas without any water taps, my assumption is that these areas do not need water taps for many possible reasons (lack of habitat, urbanised areas, etc), and therefore will be excluded from the analysis. I use the filter() function to remove areas without any water taps and mutate() function to create two new columns that shows the percentage of functional and non-functional water points over the total water point in the area. (Including unknown water point status)\n\nShow the codenigeria <- nigeria %>%\n  filter (total != 0) %>%\n  mutate (pct_functional = case_when(\n    functional == 0 ~ 0,\n    TRUE ~ (functional/total) * 100\n  )) %>%\n  mutate (pct_nonfunctional = case_when(\n    nonfunctional == 0 ~ 0,\n    TRUE ~ (nonfunctional/total) * 100\n  ))"
  },
  {
    "objectID": "posts/Geo/Geospatial_Exercise/index.html#spatial-correlogram",
    "href": "posts/Geo/Geospatial_Exercise/index.html#spatial-correlogram",
    "title": "Geospatial Analytics for Social Good - Understanding Nigeria Water functional and non-functional water point rate",
    "section": "Spatial Correlogram",
    "text": "Spatial Correlogram\nA nonparametric spatial correlogram is another measure of overall spatial autocorrelation that does not rely on specifying a matrix of spatial weights. Instead, a local regression is fitted to the calculated covariances or correlations for all pairs of observations based on the distance between them. They show how correlated are pairs of spatial observations when you increase the distance (lag) between them - they are plots of some index of autocorrelation (Moran’s I or Geary’s c) against distance.Although correlograms are not as fundamental as variograms (a keystone concept of geostatistics), they are very useful as an exploratory and descriptive tool. For this purpose they actually provide richer information than variograms.\nComputing local Moran’s I\nTo compute local Moran’s I, the localmoran() function of spdep will be used. It computes Ii values, given a set of zi values and a listw object providing neighbour weighting information for the polygon associated with the zi values.\n\nfips <- order(nigeria$shapeName)\nlocalMI_dw <- localmoran(nigeria$nonfunctional, rswm_dw)\nlocalMI_adp <- localmoran(nigeria$nonfunctional, bwm_apd)\n\nhead(localMI_dw)\n\n            Ii          E.Ii       Var.Ii       Z.Ii Pr(z != E(Ii))\n1  0.361394136 -9.995243e-04 1.128237e-02  3.4117747   0.0006454144\n2  0.074414950 -4.092463e-05 4.705097e-04  3.4325327   0.0005979717\n3  1.258199847 -1.627684e-03 6.280738e-01  1.5896655   0.1119102304\n4 -0.006652507 -5.427505e-05 4.151689e-03 -0.1024036   0.9184363392\n5  0.082615173 -2.590965e-04 3.325093e-03  1.4372021   0.1506605779\n6  0.006672593 -1.538445e-07 5.523369e-06  2.8392431   0.0045220690\n\nhead(localMI_adp)\n\n             Ii          E.Ii       Var.Ii       Z.Ii Pr(z != E(Ii))\n1 -0.0039915738 -7.478709e-05 7.815010e-04 -0.1401087    0.888574092\n2  0.0017815926 -3.170287e-06 3.404249e-05  0.3058932    0.759685936\n3  0.0109484618 -1.416360e-05 1.297391e-05  3.0435412    0.002338114\n4 -0.0010787454 -1.239643e-06 3.442702e-06 -0.5807242    0.561426373\n5  0.0034790130 -1.055977e-05 4.200930e-05  0.5383932    0.590305635\n6  0.0001767241 -4.123706e-09 1.126535e-08  1.6650737    0.095898060\n\n\nMapping the local Moran’s I\nWe have to combine the local Moran’s dataframe with the our exisiting Nigeria spatialdataframe before plotting. We will use the cbind() function.\n\nnigeria_localMI_dw <- cbind(nigeria,localMI_dw) %>%\n  rename(Pr.Ii = Pr.z....E.Ii..)\n\nnigeria_localMI_adp <- cbind(nigeria,localMI_adp) %>%\n  rename(Pr.Ii = Pr.z....E.Ii..)\n\nMapping local Moran’s I values\nUsing choropleth mapping functions of tmap package, we can plot the local Moran’s I values by using the code chinks below.\n\nnigeria_localMI_dw_sub <- nigeria_localMI_dw %>%\n  filter (Pr.Ii <= 0.05)\n\ntm_shape(nigeria_localMI_dw) +\n  tm_fill(\"white\") +\n  tm_borders(\"grey\", lwd = 0.5, alpha = 0.5) +\n  tm_shape(nigeria_localMI_dw_sub) +\n  tm_fill (col = \"Ii\",\n           style = \"pretty\",\n           palette = \"RdBu\",\n           title = \"Local Moran I value\") +\n  tm_borders(\"grey\", lwd = 0.5, alpha = 0.5) +\n  tm_layout(main.title = \"Local Moran I value for p < 0.05\",\n            main.title.size = 0.7,\n            main.title.fontface = \"bold\")\n\n\n\n\n\nnigeria_localMI_adp_sub <- nigeria_localMI_adp %>%\n  filter (Pr.Ii <= 0.05)\n\ntm_shape(nigeria_localMI_adp) +\n  tm_fill(\"white\") +\n  tm_borders(\"grey\", lwd = 0.5, alpha = 0.5) +\n  tm_shape(nigeria_localMI_adp_sub) +\n  tm_fill (col = \"Ii\",\n           style = \"pretty\",\n           palette = \"RdBu\",\n           title = \"Local Moran I value\") +\n  tm_borders(\"grey\", lwd = 0.5, alpha = 0.5) +\n  tm_layout(main.title = \"Local Moran I value for p < 0.05\",\n            main.title.size = 0.7,\n            main.title.fontface = \"bold\")"
  },
  {
    "objectID": "posts/Geo/Geospatial_Exercise/index.html#creating-a-lisa-cluster-map",
    "href": "posts/Geo/Geospatial_Exercise/index.html#creating-a-lisa-cluster-map",
    "title": "Geospatial Analytics for Social Good - Understanding Nigeria Water functional and non-functional water point rate",
    "section": "Creating a LISA Cluster Map",
    "text": "Creating a LISA Cluster Map\nThe LISA Cluster Map shows the significant locations color coded by type of spatial autocorrelation. The first step before we can generate the LISA cluster map is to plot the Moran scatterplot.\nPreparing LISA map classes\nWe will now prepare the LISA map classes. We first need to retrieve the quadrant for each area.\n\nShow the codequadrant <- vector(mode=\"numeric\",length=nrow(localMI_dw))\n\n\nWe will now then define the quadrant based on the following criteria and place non-significant Moran (p value < 0.05) in the category 0:\nNote: We will use the scaled variable and the lag scaled variable used earlier on for the Moran Scatterplot.\n\nShow the codesignif <- 0.05 \nquadrant[DV >0 & C_mI>0] <- 4      \nquadrant[DV <0 & C_mI<0] <- 1      \nquadrant[DV <0 & C_mI>0] <- 2\nquadrant[DV >0 & C_mI<0] <- 3\nquadrant[localMI_dw[,5]>signif] <- 0\nnigeria_localMI_dw$quadrant <- quadrant\n\nquadrant[DV_Fun >0 & C_mI>0] <- 4      \nquadrant[DV_Fun <0 & C_mI<0] <- 1      \nquadrant[DV_Fun <0 & C_mI>0] <- 2\nquadrant[DV_Fun >0 & C_mI<0] <- 3\nquadrant[localMI_dw[,5]>signif] <- 0\nnigeria_localMI_dw$quadrantfun <- quadrant\n\nquadrant[DV >0 & C_mI_adp>0] <- 4      \nquadrant[DV <0 & C_mI_adp<0] <- 1      \nquadrant[DV <0 & C_mI_adp>0] <- 2\nquadrant[DV >0 & C_mI_adp<0] <- 3\nquadrant[localMI_adp[,5]>signif] <- 0\nnigeria_localMI_adp$quadrant <- quadrant\n\nquadrant[DV_Fun >0 & C_mI_adp_Fun>0] <- 4      \nquadrant[DV_Fun <0 & C_mI_adp_Fun<0] <- 1      \nquadrant[DV_Fun <0 & C_mI_adp_Fun>0] <- 2\nquadrant[DV_Fun >0 & C_mI_adp_Fun<0] <- 3\nquadrant[localMI_adp[,5]>signif] <- 0\nnigeria_localMI_adp$quadrantfun <- quadrant\n\n\nPlotting LISA map\nOnce the quadrant of each area has been decided, we will now plot the LISA map using tmap.\n\nShow the codetmap_mode(\"plot\")\ncolors <- c(\"#ffffff\", \"#2c7bb6\", \"#abd9e9\", \"#fdae61\", \"#d7191c\")\nclusters <- c(\"insignificant\", \"low-low\", \"low-high\", \"high-low\", \"high-high\")\n\nlisamap_fd <- tm_shape(nigeria_localMI_dw) +\n  tm_fill(col = \"quadrant\", \n          style = \"cat\", \n          palette = colors[c(sort(unique(quadrant)))+1], \n          labels = clusters[c(sort(unique(quadrant)))+1]) +\n  tm_view(set.zoom.limits = c(11,17)) +\n  tm_borders(alpha=0.5) +\n  tm_layout(main.title = \"LISA Map (Non-Functional) with Quadrant\",\n            main.title.size = 0.6,\n            main.title.fontface = \"bold\",\n            main.title.position = \"center\",\n            title = \"Fixed Distance Weight\",\n            title.size = 0.6)\n\nlisamap_fd_fun <- tm_shape(nigeria_localMI_dw) +\n  tm_fill(col = \"quadrantfun\", \n          style = \"cat\", \n          palette = colors[c(sort(unique(quadrant)))+1], \n          labels = clusters[c(sort(unique(quadrant)))+1],\n          title = \"quadrant\") +\n  tm_view(set.zoom.limits = c(11,17)) +\n  tm_borders(alpha=0.5) +\n  tm_layout(main.title = \"LISA Map (Functional) with Quadrant\",\n            main.title.size = 0.6,\n            main.title.fontface = \"bold\",\n            main.title.position = \"center\",\n            title = \"Fixed Distance Weight\",\n            title.size = 0.6)\n\n\n\n\n\n\nLISA map of Fixed Distance (Non-Functional & Functional)\n\n\n\n\n\nShow the codetmap_mode(\"plot\")\ncolors <- c(\"#ffffff\", \"#2c7bb6\", \"#abd9e9\", \"#fdae61\", \"#d7191c\")\nclusters <- c(\"insignificant\", \"low-low\", \"low-high\", \"high-low\", \"high-high\")\n\nlisamap_ad <- tm_shape(nigeria_localMI_adp) +\n  tm_fill(col = \"quadrant\", \n          style = \"cat\", \n          palette = colors[c(sort(unique(quadrant)))+1], \n          labels = clusters[c(sort(unique(quadrant)))+1]) +\n  tm_view(set.zoom.limits = c(11,17)) +\n  tm_borders(alpha=0.5) +\n  tm_layout(main.title = \"LISA Map (Non Functional) with Quadrant\",\n            main.title.size = 0.6,\n            main.title.fontface = \"bold\",\n            main.title.position = \"center\",\n            title = \"Adaptive Distance Weight\",\n            title.size = 0.6)\n\nlisamap_ad_fun <- tm_shape(nigeria_localMI_adp) +\n  tm_fill(col = \"quadrantfun\", \n          style = \"cat\", \n          palette = colors[c(sort(unique(quadrant)))+1], \n          labels = clusters[c(sort(unique(quadrant)))+1],\n          title = \"quadrant\") +\n  tm_view(set.zoom.limits = c(11,17)) +\n  tm_borders(alpha=0.5) +\n  tm_layout(main.title = \"LISA Map (Functional) with Quadrant\",\n            main.title.size = 0.6,\n            main.title.fontface = \"bold\",\n            main.title.position = \"center\",\n            title = \"Adaptive Distance Weight\",\n            title.size = 0.6)\n\nfixedD_Lisa <- tmap_arrange (lisamap_fd,lisamap_fd_fun, ncol = 2, widths = 10)\nadaptiveD_Lisa <- tmap_arrange (lisamap_ad,lisamap_ad_fun, ncol = 2, widths = 10)\n\n\n\n\n\n\nLISA map of Adaptive Distance (Non-Functional & Functional)\n\n\n\n\nAnalysis of LISA map result\nFor both distance weight methods, we can see that the South West region shows a H-H for non-functional water point percentage and a few regions shows a L-L and L-H for functional water point percentage. These regions should be prioritize for repair since they have their regions and neighbouring regions most likely have a lot of non-functional water point and also very little functional water point.\nWhat we can see from the map is also the North East region seems to have good access to water point with regions showing H-H for functional water point percentage and a L-L or L-H for non-functional water point percentage."
  },
  {
    "objectID": "posts/Geo/Geospatial_Autocorrelation/index.html#conclusion",
    "href": "posts/Geo/Geospatial_Autocorrelation/index.html#conclusion",
    "title": "Introduction to Global and Local Measures of Spatial Autocorrelation",
    "section": "Conclusion",
    "text": "Conclusion\nGeospatial autocorrelation is important for us to draw statistical conclusion on whether areas are correlated with one another based on various attributes and of course, the connectivity of the land with others."
  },
  {
    "objectID": "posts/Geo/Geospatial_Exercise/index.html#lisa-cluster-map",
    "href": "posts/Geo/Geospatial_Exercise/index.html#lisa-cluster-map",
    "title": "Geospatial Analytics for Social Good - Understanding Nigeria Water functional and non-functional water point rate",
    "section": "LISA Cluster Map",
    "text": "LISA Cluster Map\nIntroduction\nThe Spatial Autocorrelation measures spatial autocorrelation based on feature locations and feature values simultaneously. Given a set of features and an associated attribute, it evaluates whether the expressed pattern is clustered, scattered, or random. The tool calculates the Moran’s I index value as well as a z-score and p-value to assess the significance of this index. P-values are numerical approximations of the area under the curve for a known distribution, bounded by the test statistic.\nIn this study we will explore the computation of Global and Local Measure of Spatial Autocorrelation (GLSA) by using spdep package.\nFor this study, we will be using the distance based weight matrix. There are two type of distance-based proximity matrix, they are:\n\nFixed Distance Weight Matrix\nAdaptive Distance Weight Matrix\n\nSince the study is regarding the prioritisation of water tap repair and to identify areas which have restriction in water supply access, neighbouring regions that are nearer to the selected region should have greater weights compared to neighbouring regions that are further away. Therefore, with this concept in mind, we will employ the Inverse distance weighting to take distance decay into consideration.\nDeriving distance-based and adaptive weight matrix\nThe first step is to retrieve the centroid for each area. To retrieve the centroid of each area, we will use the st_centroid() function. The st_centroid() function will calculates the geometric center of a spatial object.\n\nShow the codecoords <- st_centroid(st_geometry(nigeria))\ncoords[1]\n\nGeometry set for 1 feature \nGeometry type: POINT\nDimension:     XY\nBounding box:  xmin: 549364 ymin: 123694.9 xmax: 549364 ymax: 123694.9\nProjected CRS: Minna / Nigeria West Belt\n\n\nDetermine the cut-off distance for fixed distance weight matrix\nSecondly, we need to determine the upper limit for distance band by using the steps below:\n\nReturn a matrix with the indices of points belonging to the set of the k nearest neighbours of each other by using knearneigh() of spdep.\nConvert the knn object returned by knearneigh() into a neighbours list of class nb with a list of integer vectors containing neighbour region number ids by using knn2nb().\nReturn the length of neighbour relationship edges by using nbdists() of spdep. The function returns in the units of the coordinates if the coordinates are projected, in km otherwise.\nRemove the list structure of the returned object by using unlist().\n\n\nShow the codek1 <- knn2nb(knearneigh(coords))\nk1dists <- unlist(nbdists(k1, coords))\nsummary(k1dists)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   2669   12808   20008   21804   27013   72139 \n\nShow the codethreshold <- max(unlist(nbdists(k1, coords)))\n\n\nThe summary report shows that the largest first nearest neighbour distance is 72.139 km, so using this as the upper threshold gives certainty that all units will have at least one neighbour. We then save the max value as the threshold for the subsequent function.\nComputing Fixed distance weight matrix\nNow, we will compute the distance weight matrix by using dnearneigh() as shown in the code chunk below. The function identifies neighbours of region points by Euclidean distance in the metric of the points between lower (greater than or equal to and upper (less than or equal to) bounds.\n\nShow the codewm_d73 <- dnearneigh(coords, 0, threshold)\nwm_d73\n\nNeighbour list object:\nNumber of regions: 761 \nNumber of nonzero links: 18022 \nPercentage nonzero weights: 3.111958 \nAverage number of links: 23.682 \n\n\nFrom the result above, we can identify an average of 23 neighbours per region using the distance based weight matrix.\nComputing Inverse Adaptive distance weight matrix\nNext, we will compute the inverse adaptive distance weight matrix. The knearneigh() function uses spartial indexing to identify the nearest neighbour. For this analysis, we will set to number of neighbours to 8.\n\nShow the codek8 <- knn2nb(knearneigh(coords, k = 8))\nk8\n\nNeighbour list object:\nNumber of regions: 761 \nNumber of nonzero links: 6088 \nPercentage nonzero weights: 1.051248 \nAverage number of links: 8 \nNon-symmetric neighbours list\n\n\nComputing Inverse Distance Weights\nTo compute the inverse distance, we need a function that applies \\(\\frac{1}{x}\\) to the entire distance data structure. We use lapply to achieve this. The required parameters are the distance and the function specified by lapply. Use the functional operator and \\(\\frac{1}{x}\\) to get the corresponding function. The important thing to note is that the distance units in the dataset are meters. This means that the distance values between points can be very large, resulting in small reciprocals. To fix this scale dependency, rescale the distances by doing a \\(\\frac{x}{1000}\\)in the function before computing the reciprocal.\nComputing Inverse Distance Weights for Fixed Distance\n\nShow the codedistances <- nbdists(wm_d73,coords)\ndistances[1]\n\n[[1]]\n [1]  4004.008 45439.251 37710.356 51041.840 67445.464 65694.575 70278.881\n [8] 66402.395 70754.787 63293.565 44070.130 67132.783 30584.808 31191.772\n[15] 31060.775 61914.751 40430.849 50524.637 68364.757 57815.248 58326.473\n[22] 21477.914 55573.697 46904.765 62426.753 37887.549 52063.146 40086.745\n[29] 52077.350 19783.847 31237.020 65772.348 68438.043 48339.227 66025.875\n[36] 63788.895 32358.664 62335.064 71500.748 11065.063 48564.675 29667.531\n[43] 49875.572 55850.546 70099.782 57661.903 16123.664 69453.919 46496.314\n[50]  9313.577 56305.742 48780.437 56595.108 31208.261 53734.648 40998.087\n[57] 10676.236 35065.479 20859.370 23059.238 53898.687 43026.271 61625.223\n\nShow the codedistances <- lapply(distances, function(x) (1/(x/1000)))\ndistances[1]\n\n[[1]]\n [1] 0.24974975 0.02200741 0.02651791 0.01959177 0.01482679 0.01522196\n [7] 0.01422903 0.01505970 0.01413332 0.01579939 0.02269111 0.01489585\n[13] 0.03269597 0.03205974 0.03219495 0.01615124 0.02473359 0.01979232\n[19] 0.01462742 0.01729648 0.01714487 0.04655946 0.01799412 0.02131980\n[25] 0.01601877 0.02639390 0.01920744 0.02494590 0.01920221 0.05054629\n[31] 0.03201330 0.01520396 0.01461176 0.02068713 0.01514558 0.01567671\n[37] 0.03090362 0.01604234 0.01398587 0.09037454 0.02059110 0.03370688\n[43] 0.02004990 0.01790493 0.01426538 0.01734247 0.06202064 0.01439804\n[49] 0.02150708 0.10737014 0.01776018 0.02050002 0.01766937 0.03204280\n[55] 0.01860997 0.02439138 0.09366597 0.02851808 0.04794009 0.04336657\n[61] 0.01855333 0.02324161 0.01622712\n\n\nComputing Inverse Distance Weights for Adaptive Distance\n\nShow the codek.distances <- nbdists(k8, coords)\nk.distances[1]\n\n[[1]]\n[1]  4004.008 21477.914 19783.847 11065.063 16123.664  9313.577 10676.236\n[8] 20859.370\n\nShow the codeinvdistance <- lapply(k.distances, function(x) (1/(x/1000)))\ninvdistance[1]\n\n[[1]]\n[1] 0.24974975 0.04655946 0.05054629 0.09037454 0.06202064 0.10737014 0.09366597\n[8] 0.04794009\n\n\nVisualising Weight Matrices\nUsing the base R plot function, we will now plot to visualise the areas with their respective neighbours after assignment based on the various methods. The left graph with the red lines show the adaptive distance with 8 neighbours and the right graph with the black lines show the links of neighbours within the cut-off distance of the above threshold.\n\nShow the codepar(mfrow=c(1,2))\nplot(nigeria$geometry, border=\"lightgrey\", main=\"Adaptive Distance (8)\")\nplot(k8, coords, add=TRUE, col=\"red\", length=0.08)\nplot(nigeria$geometry, border=\"lightgrey\", main=\"Fixed Distance\")\nplot(wm_d73, coords, add=TRUE, pch = 19, cex = 0.6)\n\n\n\n\nConstructing Distance Binary Weight Matrix\nNow we will have to construct the distance weight matrix using the neighbours and inverse distance that were computed above. Since we have the inverse distance as a variable, we will use the Binary Weight Matrix and input the inverse distance as the weight. The nb2listw() function takes in a list input glist to identify the weight for each neighbour.\nThe code chunk below will construct the fixed distance weight matrix and subsequently implement the binary weight matrix using the nb2listw() function.\n\nShow the codebwm_fd <- nb2listw(wm_d73,\n                    glist = distances,\n                   style=\"B\", \n                   zero.policy = TRUE)\nsummary (bwm_fd)\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 761 \nNumber of nonzero links: 18022 \nPercentage nonzero weights: 3.111958 \nAverage number of links: 23.682 \nLink number distribution:\n\n 1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 \n 6 11 10 17 30 31 31 33 29 36 26 18 21 23 16 12 11 11 17 16 16 10 14 10  9  9 \n27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 \n12  6 15 13 11  9  8  8 12  6 12 17 14  9  9  4  5  3  8  5 11  8  5  5  4  2 \n53 54 55 56 57 58 59 60 61 62 63 64 65 67 68 70 \n 3  3  6  3  5  6  2  5  5  8  6  6  4  3  1  1 \n6 least connected regions:\n88 110 121 235 657 753 with 1 link\n1 most connected region:\n572 with 70 links\n\nWeights style: B \nWeights constants summary:\n    n     nn       S0       S1       S2\nB 761 579121 486.0122 43.00868 2155.714\n\n\nThe code chunk below will construct the adaptive distance weight matrix and subsequently implement the Binary weight matrix using the nb2listw() function.\n\nShow the codebwm_apd <- nb2listw(k8,\n                    glist = invdistance,\n                    style = \"B\",\n                    zero.policy = TRUE)\nsummary (bwm_apd)\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 761 \nNumber of nonzero links: 6088 \nPercentage nonzero weights: 1.051248 \nAverage number of links: 8 \nNon-symmetric neighbours list\nLink number distribution:\n\n  8 \n761 \n761 least connected regions:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 with 8 links\n761 most connected regions:\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755 756 757 758 759 760 761 with 8 links\n\nWeights style: B \nWeights constants summary:\n    n     nn       S0       S1       S2\nB 761 579121 237.6033 30.43191 453.5401\n\n\nGlobal Spatial Autocorrelation: Moran’s I\nTo identify whether the area analysed is clustered, dispersed or random, we will have to perform the spatial autocorrelation Moran’s I test.\nComputing Global Moran’s I\nWe will now perform Moran’s I statistics testing by using the moran.test() from spdep on both functional and non functional water point percentage. The global Moran’s I test will also be performed on both the fixed distance and adaptive distance weight matrices.\n\nShow the codemoran.test(nigeria$pct_nonfunctional, \n           listw=bwm_fd, \n           zero.policy = TRUE, \n           na.action=na.omit)\n\n\n    Moran I test under randomisation\n\ndata:  nigeria$pct_nonfunctional  \nweights: bwm_fd    \n\nMoran I statistic standard deviate = 32.052, p-value < 2.2e-16\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n     0.4210179196     -0.0013157895      0.0001736248 \n\n\n\nShow the codemoran.test(nigeria$pct_functional, \n           listw=bwm_fd, \n           zero.policy = TRUE, \n           na.action=na.omit)\n\n\n    Moran I test under randomisation\n\ndata:  nigeria$pct_functional  \nweights: bwm_fd    \n\nMoran I statistic standard deviate = 42.929, p-value < 2.2e-16\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n     0.5644376118     -0.0013157895      0.0001736823 \n\n\n\nShow the codemoran.test(nigeria$pct_nonfunctional, \n           listw=bwm_apd, \n           zero.policy = TRUE, \n           na.action=na.omit)\n\n\n    Moran I test under randomisation\n\ndata:  nigeria$pct_nonfunctional  \nweights: bwm_apd    \n\nMoran I statistic standard deviate = 22.233, p-value < 2.2e-16\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n     0.5115959890     -0.0013157895      0.0005322145 \n\n\n\nShow the codemoran.test(nigeria$pct_functional, \n           listw=bwm_apd, \n           zero.policy = TRUE, \n           na.action=na.omit)\n\n\n    Moran I test under randomisation\n\ndata:  nigeria$pct_functional  \nweights: bwm_apd    \n\nMoran I statistic standard deviate = 24.492, p-value < 2.2e-16\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n     0.5638015977     -0.0013157895      0.0005323946 \n\n\nInterpretation of results\nBased on all the results above, the p value is below the alpha value of 0.05, therefore we have enough statistical evidence to reject the null hypothesis that the attribute is randomly distributed and a positive Moran I value indicate more spatially clustered than would be expected if underlying spatial processes were random.\nComputing Monte Carlo Moran’s I\nThe code chunk below performs permutation test for Moran’s I statistic by using moran.mc() of spdep. A total of 1000 simulation will be performed.\n\nShow the codeset.seed(1234)\nfd_MC = moran.mc(nigeria$pct_nonfunctional, \n                listw=bwm_fd, \n                nsim=999, \n                zero.policy = TRUE,\n                na.action=na.omit)\nfd_MC\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  nigeria$pct_nonfunctional \nweights: bwm_fd  \nnumber of simulations + 1: 1000 \n\nstatistic = 0.42102, observed rank = 1000, p-value = 0.001\nalternative hypothesis: greater\n\n\n\nShow the codeset.seed(1234)\nfd_MC_fun = moran.mc(nigeria$pct_functional, \n                listw=bwm_fd, \n                nsim=999, \n                zero.policy = TRUE,\n                na.action=na.omit)\nfd_MC_fun\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  nigeria$pct_functional \nweights: bwm_fd  \nnumber of simulations + 1: 1000 \n\nstatistic = 0.56444, observed rank = 1000, p-value = 0.001\nalternative hypothesis: greater\n\n\n\nShow the codeset.seed(1234)\nadp_MC= moran.mc(nigeria$pct_nonfunctional, \n                listw=bwm_apd, \n                nsim=999, \n                zero.policy = TRUE, \n                na.action=na.omit)\nadp_MC\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  nigeria$pct_nonfunctional \nweights: bwm_apd  \nnumber of simulations + 1: 1000 \n\nstatistic = 0.5116, observed rank = 1000, p-value = 0.001\nalternative hypothesis: greater\n\n\n\nShow the codeset.seed(1234)\nadp_MC_fun = moran.mc(nigeria$pct_functional, \n                listw=bwm_apd, \n                nsim=999, \n                zero.policy = TRUE, \n                na.action=na.omit)\nadp_MC_fun\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  nigeria$pct_functional \nweights: bwm_apd  \nnumber of simulations + 1: 1000 \n\nstatistic = 0.5638, observed rank = 1000, p-value = 0.001\nalternative hypothesis: greater\n\n\nVisualising Monte Carlo Moran’s I\nWe will use a density plot to visualise the output of the Monte Carlo Moran’s I. First, we need to extract the res value and convert it into a dataframe. We then visualise the test statistic result using geom_density() from the ggplot package. geom_vline is use to represent the actual Moran I value.\nThe code chunk below is to extract the res value and convert it to a dataframe format using as.data.frame() function. The code chunk will generate the density plot using ggplot.\n\nShow the code# Fixed Distance Monte Carlo\n\nfd_monte_carlo <- as.data.frame(fd_MC[7])\n\nfd_mc <- ggplot(fd_monte_carlo, aes(x=res)) + \n  geom_density(fill=\"lightblue\") +\n  geom_vline(aes(xintercept=0.42102),\n            color=\"blue\", linetype=\"dashed\", size=1) +\n  labs(title = \"Density plot of Monte Carlo Simulation of Moran's I\", x = \"Test Statistic\", y = \"Density\") +\n  theme_minimal() +\n  theme(plot.title = element_text(size=8))\n\nfd_monte_carlo_fun <- as.data.frame(fd_MC_fun[7])\n\nfd_mc_fun <- ggplot(fd_monte_carlo_fun, aes(x=res)) + \n  geom_density(fill=\"lightblue\") +\n  geom_vline(aes(xintercept=0.56444),\n            color=\"blue\", linetype=\"dashed\", size=1) +\n  labs(title = \"Density plot of Monte Carlo Simulation of Moran's I\", x = \"Test Statistic\", y = \"Density\") +\n  theme_minimal() +\n  theme(plot.title = element_text(size=8)) \n\n# Adaptive Distance Monte Carlo\n\nadp_monte_carlo <- as.data.frame(adp_MC[7])\n\nad_mc <- ggplot(adp_monte_carlo, aes(x=res)) + \n  geom_density(fill=\"lightblue\") +\n  geom_vline(aes(xintercept=0.5116),\n            color=\"blue\", linetype=\"dashed\", size=1) +\n  labs(title = \"Density plot of Monte Carlo Simulation of Moran's I\", x = \"Test Statistic\", y = \"Density\") +\n  theme_minimal() +\n  theme(plot.title = element_text(size=8)) \n\nadp_monte_carlo_fun <- as.data.frame(adp_MC_fun[7])\n\nad_mc_fun <- ggplot(adp_monte_carlo_fun, aes(x=res)) + \n  geom_density(fill=\"lightblue\") +\n  geom_vline(aes(xintercept=0.5638),\n            color=\"blue\", linetype=\"dashed\", size=1) +\n  labs(title = \"Density plot of Monte Carlo Simulation of Moran's I\", x = \"Test Statistic\", y = \"Density\") +\n  theme_minimal() +\n  theme(plot.title = element_text(size=8))\n\n\n\nShow the code(fd_mc + fd_mc_fun) / (ad_mc + ad_mc_fun) \n\n\n\n\nInterpretation of Monte Carlo Results\nBased on the above plots and results of the Monte Carlos Moran I test, we can conclude that all the results are statistically significant with a p value < 0.05 and all the Moran I values falls way to the right of the distribution suggesting that the functional and non functional water points are clustered (a positive Moran’s I value suggests clustering)."
  },
  {
    "objectID": "posts/Geo/Geospatial_Exercise/index.html#introduction-1",
    "href": "posts/Geo/Geospatial_Exercise/index.html#introduction-1",
    "title": "Geospatial Analytics for Social Good - Understanding Nigeria Water functional and non-functional water point rate",
    "section": "Introduction",
    "text": "Introduction"
  },
  {
    "objectID": "posts/Geo/Geospatial_Exercise/index.html#mapping-of-functional-and-non-functional-water-point",
    "href": "posts/Geo/Geospatial_Exercise/index.html#mapping-of-functional-and-non-functional-water-point",
    "title": "Geospatial Analytics for Social Good - Understanding Nigeria Water functional and non-functional water point rate",
    "section": "Mapping of Functional and Non Functional Water Point",
    "text": "Mapping of Functional and Non Functional Water Point\nJenks Choropleth Map\nWe will now plot the choropleth map using tmap and jenks classification.\n\nShow the codetmap_mode (\"plot\")\nfun <- tm_shape (nigeria) +\n  tm_fill(\"pct_functional\",\n          style = \"jenks\",\n          n=6,\n          title = \"Functional (%)\") +\n  tm_layout(main.title = \"Distribution of Functional Water Tap (%) by ADM2\",\n            main.title.position = \"center\",\n            main.title.size = 0.7,\n            main.title.fontface = \"bold\",\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5)\n\nnfun <- tm_shape (nigeria) +\n  tm_fill(\"pct_nonfunctional\",\n          style = \"jenks\",\n          n=6,\n          title = \"Non-Functional (%)\") +\n  tm_layout(main.title = \"Distribution of Non Functional Water Tap (%) by ADM2\",\n            main.title.position = \"center\",\n            main.title.size = 0.7,\n            main.title.fontface = \"bold\",\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5)\n\ntmap_arrange (fun, nfun, ncol = 2, asp = 1)\n\n\n\n\n\n\ntmap of water tap % distribution (Functional & Non-Functional\n\n\n\n\nObservations from Jenks Choropleth Map\nBy looking at the two chloropleth map, we can make 2 inference: - The northen region have relatively higher percentage of functional water point compared to the southern region. - The southern region also have relatively higher percentage of non-functional water point.\nNow, answering the business question, should we focus our resources in repairing the water tap located at the southern region? Are there any more regions that require our attention? We will now proceed to conduct geospatial analysis to justify or rebut the claims made based on the choropleth map."
  },
  {
    "objectID": "posts/Geo/Geospatial_Exercise/index.html#cluster-and-outlier-analysis",
    "href": "posts/Geo/Geospatial_Exercise/index.html#cluster-and-outlier-analysis",
    "title": "Geospatial Analytics for Social Good - Understanding Nigeria Water functional and non-functional water point rate",
    "section": "Cluster and Outlier Analysis",
    "text": "Cluster and Outlier Analysis\nComputing local Moran’s I\nTo compute local Moran’s I, the localmoran() function of spdep will be used. It computes Ii values, given a set of zi values and a listw object providing neighbour weighting information for the polygon associated with the zi values.\n\nShow the codefips <- order(nigeria$shapeName)\nlocalMI_dw <- localmoran(nigeria$nonfunctional, bwm_fd)\nlocalMI_dw_fun <- localmoran(nigeria$functional, bwm_fd)\nlocalMI_adp <- localmoran(nigeria$nonfunctional, bwm_apd)\nlocalMI_adp_fun <- localmoran(nigeria$functional, bwm_apd)\n\n\nhead(localMI_dw,3)\n\n           Ii          E.Ii       Var.Ii       Z.Ii Pr(z != E(Ii))\n1  0.45028841 -2.015620e-03 0.0998886136  1.4311082     0.15239922\n2  0.13674255 -9.714502e-05 0.0050358216  1.9283104     0.05381653\n3 -0.00773083 -1.704297e-05 0.0004332892 -0.3705772     0.71095246\n\nShow the codehead(localMI_dw_fun,3)\n\n            Ii          E.Ii      Var.Ii        Z.Ii Pr(z != E(Ii))\n1  0.848290175 -0.0014284243 0.070810822  3.19319400   0.0014070838\n2  0.577851238 -0.0005902825 0.030591200  3.30720567   0.0009423168\n3 -0.002253816 -0.0001063072 0.002701745 -0.04131549   0.9670443851\n\nShow the codehead(localMI_adp,3)\n\n           Ii          E.Ii       Var.Ii        Z.Ii Pr(z != E(Ii))\n1 -0.02804274 -7.980020e-04 0.0819743028 -0.09515767      0.9241896\n2  0.02276868 -3.975715e-05 0.0041969420  0.35206997      0.7247858\n3 -0.01086825 -1.515025e-05 0.0004136275 -0.53364083      0.5935901\n\nShow the codehead(localMI_adp_fun,3)\n\n            Ii          E.Ii      Var.Ii        Z.Ii Pr(z != E(Ii))\n1  0.333118954 -5.655260e-04 0.058111406  1.38422062      0.1662909\n2  0.252550827 -2.415764e-04 0.025495242  1.58319514      0.1133770\n3 -0.005107121 -9.450119e-05 0.002579146 -0.09870221      0.9213747\n\n\nMapping the local Moran’s I\nWe have to combine the local Moran’s dataframe with the our exisiting Nigeria spatialdataframe before plotting. We will use the cbind() function.\n\nShow the codenigeria_localMI_dw <- cbind(nigeria,localMI_dw) %>%\n  rename(Pr.Ii = Pr.z....E.Ii..)\n\nnigeria_localMI_dw <- cbind(nigeria_localMI_dw,localMI_dw_fun) %>%\n  rename(Pr.Ii.fun = Pr.z....E.Ii..)\n\nnigeria_localMI_adp <- cbind(nigeria,localMI_adp) %>%\n  rename(Pr.Ii = Pr.z....E.Ii..)\n\nnigeria_localMI_adp <- cbind(nigeria_localMI_adp,localMI_adp_fun) %>%\n  rename(Pr.Ii.fun = Pr.z....E.Ii..)\n\n\nVisualisation of Moran Scatterplot\nWe will now visualise the moran values using ggplot.\nUsing the Moran Scatterplot below, we filter all the area with p value < 0.05 and identify significant areas.\nWe will first have to calculate the scaled attribute and the lagged scaled attribute using the scale() function and lag.listw() function.\nWe scale the the percentage for both functional and non functional water points.\n\nShow the codeDV <- scale(nigeria_localMI_dw$pct_nonfunctional)\nDV_Fun <- scale(nigeria_localMI_dw$pct_functional)   \n\n\nThis is follow by finding the lag of the scaled percentage of functional and non-functional water point.\n\nShow the codeC_mI <- lag.listw(bwm_fd, DV)\nC_mI_Fun <- lag.listw(bwm_fd, DV_Fun)\nC_mI_adp <- lag.listw(bwm_apd, DV)\nC_mI_adp_Fun <- lag.listw(bwm_apd, DV_Fun)\n\n\nOnce we are done with the computation, we will plot the scatterplot using the variables above.\n\nShow the codeMIplot <- data.frame(cbind(DV,C_mI,localMI_dw[,5]))\nMIplot <- MIplot %>%\n  filter (X3 < 0.05)\n\nfd_MI_Plot <- ggplot(MIplot, aes(x = X1, y = X2)) +\n  geom_point() +\n  ylim(-2,1) +\n  xlim (-2,3) +\n  geom_hline(yintercept=0, linetype=\"dashed\", \n                color = \"red\", size=0.5) +\n  geom_vline(xintercept=0, linetype=\"dashed\", \n                color = \"red\", size=0.5) +\n  labs(title = \"Moran Scatterplot Fixed Distance (Non-Functional)\", x = \"scaled Non-Functional (%)\", y = \"Lag scaled Non-Functional (%)\") +\n  theme_classic() +\n  theme(plot.title = element_text(size=8))\n\n\n\nShow the codeMIplot <- data.frame(cbind(DV_Fun,C_mI,localMI_dw[,5]))\nMIplot <- MIplot %>%\n  filter (X3 < 0.05)\n\nfd_MI_Plot_fun <- ggplot(MIplot, aes(x = X1, y = X2)) +\n  geom_point() +\n  ylim(-2,1) +\n  xlim (-2,3) +\n  geom_hline(yintercept=0, linetype=\"dashed\", \n                color = \"red\", size=0.5) +\n  geom_vline(xintercept=0, linetype=\"dashed\", \n                color = \"red\", size=0.5) +\n  labs(title = \"Moran Scatterplot Fixed Distance (Functional)\", x = \"scaled Functional (%)\", y = \"Lag scaled Functional (%)\") +\n  theme_classic() + \n  theme(plot.title = element_text(size=8))\n\n\n\nShow the codeMIplot <- data.frame(cbind(DV,C_mI_adp,localMI_adp[,5]))\nMIplot <- MIplot %>%\n  filter (X3 < 0.05)\n\nad_MI_Plot <- ggplot(MIplot, aes(x = X1, y = X2)) +\n  geom_point() +\n  ylim(-2,1) +\n  xlim (-2,3) +\n  geom_hline(yintercept=0, linetype=\"dashed\", \n                color = \"red\", size=0.5) +\n  geom_vline(xintercept=0, linetype=\"dashed\", \n                color = \"red\", size=0.5) +\n  labs(title = \"Moran Scatterplot Adapative Distance (Non-Functional)\", x = \"scaled Non-Functional (%)\", y = \"Lag scaled Non-Functional (%)\") +\n  theme_classic() +\n  theme(plot.title = element_text(size=8))\n\n\n\nShow the codeMIplot <- data.frame(cbind(DV_Fun,C_mI_adp_Fun,localMI_adp_fun[,5]))\nMIplot <- MIplot %>%\n  filter (X3 < 0.05)\n\nad_MI_Plot_fun <- ggplot(MIplot, aes(x = X1, y = X2)) +\n  geom_point() +\n  ylim(-2,1) +\n  xlim (-2,3) +\n  geom_hline(yintercept=0, linetype=\"dashed\", \n                color = \"red\", size=0.5) +\n  geom_vline(xintercept=0, linetype=\"dashed\", \n                color = \"red\", size=0.5) +\n  labs(title = \"Moran Scatterplot Adapative Distance (Functional)\", x = \"scaled Functional (%)\", y = \"Lag scaled Functional (%)\") +\n  theme_classic() +\n  theme(plot.title = element_text(size=8))\n\n\n\nShow the code(fd_MI_Plot + fd_MI_Plot_fun)/(ad_MI_Plot + ad_MI_Plot_fun)\n\n\n\n\nDue to the number of areas that are significant, it is hard to draw any statistical conclusion or inference from the scatterplot. Therefore, we will now visual the Moran I results on a chloropleth map instead.\nMapping local Moran’s I values\nTo better understand which area are outliers/clusters, we will visualise the Moran I values of each area using tmap. Firstly, we will filter all the areas that are not statistically significant (p value >= 0.05). We will then plot the base nigeria map and overlay with the filtered map.\nUsing choropleth mapping functions of tmap package, we can plot the local Moran’s I values by using the code chunk below.\n\nShow the codefd_moran <- tm_shape(nigeria_localMI_dw) +\n  tm_fill(\"white\") +\n  tm_borders(\"grey\", lwd = 0.5, alpha = 0.5) +\n  tm_shape(nigeria_localMI_dw[nigeria_localMI_dw$Pr.Ii < 0.05,]) +\n  tm_fill (col = \"Ii\",\n           style = \"pretty\",\n           palette = \"RdBu\",\n           title = \"Local Moran I value\") +\n  tm_borders(\"grey\", lwd = 0.5, alpha = 0.5) +\n  tm_layout(main.title = \"Local Moran I value for p < 0.05 (Non Functional)\",\n            main.title.size = 0.7,\n            main.title.fontface = \"bold\",\n            title = \"Fixed Distance Weight\",\n            title.size = 0.6)\n\nfd_moran_fun <- tm_shape(nigeria_localMI_dw) +\n  tm_fill(\"white\") +\n  tm_borders(\"grey\", lwd = 0.5, alpha = 0.5) +\n  tm_shape(nigeria_localMI_dw[nigeria_localMI_dw$Pr.Ii.fun < 0.05,]) +\n  tm_fill (col = \"Ii.1\",\n           style = \"pretty\",\n           palette = \"RdBu\",\n           title = \"Local Moran I value\") +\n  tm_borders(\"grey\", lwd = 0.5, alpha = 0.5) +\n  tm_layout(main.title = \"Local Moran I value for p < 0.05 (Functional)\",\n            main.title.size = 0.7,\n            main.title.fontface = \"bold\",\n            title = \"Fixed Distance Weight\",\n            title.size = 0.6)\n\ntmap_arrange (fd_moran, fd_moran_fun, asp = 1, ncol = 2)\n\n\n\n\n\n\ntmap of Fixed Distance Moran I (Functional & Non-Functional)\n\n\n\n\n\nShow the codeadp_moran <- tm_shape(nigeria_localMI_adp) +\n  tm_fill(\"white\") +\n  tm_borders(\"grey\", lwd = 0.5, alpha = 0.5) +\n  tm_shape(nigeria_localMI_adp[nigeria_localMI_adp$Pr.Ii < 0.05,]) +\n  tm_fill (col = \"Ii\",\n           style = \"pretty\",\n           palette = \"RdBu\",\n           title = \"Local Moran I value\") +\n  tm_borders(\"grey\", lwd = 0.5, alpha = 0.5) +\n  tm_layout(main.title = \"Local Moran I value for p < 0.05 (Non Functional)\",\n            main.title.size = 0.7,\n            main.title.fontface = \"bold\",\n            title = \"Adaptive Distance Weight\",\n            title.size = 0.6)\n\nadp_moran_fun <- tm_shape(nigeria_localMI_adp) +\n  tm_fill(\"white\") +\n  tm_borders(\"grey\", lwd = 0.5, alpha = 0.5) +\n  tm_shape(nigeria_localMI_adp[nigeria_localMI_adp$Pr.Ii.fun < 0.05,]) +\n  tm_fill (col = \"Ii.1\",\n           style = \"pretty\",\n           palette = \"RdBu\",\n           title = \"Local Moran I value\") +\n  tm_borders(\"grey\", lwd = 0.5, alpha = 0.5) +\n  tm_layout(main.title = \"Local Moran I value for p < 0.05 (Functional)\",\n            main.title.size = 0.7,\n            main.title.fontface = \"bold\",\n            title = \"Adaptive Distance Weight\",\n            title.size = 0.6)\n\ntmap_arrange (adp_moran, adp_moran_fun, asp = 1, ncol = 2)\n\n\n\n\n\n\ntmap of Adaptive Distance Moran I (Functional & Non-Functional)"
  },
  {
    "objectID": "posts/Geo/Geospatial_Exercise/index.html#future-work",
    "href": "posts/Geo/Geospatial_Exercise/index.html#future-work",
    "title": "Geospatial Analytics for Social Good - Understanding Nigeria Water functional and non-functional water point rate",
    "section": "Future Work",
    "text": "Future Work\nTo enhance the quality of research and analysis, I proposed to enhance the result of this analysis by conducting these research:\n\nPopulation - Instead of purely looking at the proportion of functional and non-functional water point, population of area can be considered to understand the needs of the region based on it’s population size. Tap per population must be considered to provide a better understanding of what the region is lacking and which region should be an area of focus to increase water access.\nk-nearest neighbour - Due to the short timeframe I have to conduct this analysis, I proposed to have more samples for different neighbours and compare the results. This will better justify the regions that are indeed an area of concern."
  },
  {
    "objectID": "posts/Geo/Geospatial_Exercise/index.html#hotspot-and-coldspot-analysis",
    "href": "posts/Geo/Geospatial_Exercise/index.html#hotspot-and-coldspot-analysis",
    "title": "Geospatial Analytics for Social Good - Understanding Nigeria Water functional and non-functional water point rate",
    "section": "Hotspot and Coldspot Analysis",
    "text": "Hotspot and Coldspot Analysis\nGetis-Ord Gi* (G-I-star), another name for hotspot analysis, operates by examining each feature in the dataset in the context of nearby features in the same dataset. Despite having a high value, a feature can not be a statistically significant hotspot. A feature with a high value must be surrounded by other features with high values in order to constitute a significant hotspot.\nGenerating Z score for Gertis-Ord Gi\nTo generate the Z score for each region, we will use the localG() function that takes in the attribute and the neighbour list. Since the output of the localG() function is in a list, we will convert it to a data frame by using the as.matrix() function. We will do the following steps on both distance weight methods and for both functional and non functional percentage attribute.\n\nShow the codegi.adaptive <- as.matrix(localG(nigeria$pct_nonfunctional, bwm_apd))\ngi.adaptive_fun <- as.matrix(localG(nigeria$pct_functional, bwm_apd))\nnigeria.gi <- cbind(nigeria, gi.adaptive) %>%\n  rename(gstat_adaptive = gi.adaptive) \nnigeria.gi <- cbind(nigeria.gi, gi.adaptive_fun) %>%\n  rename(gstat_adaptive_fun = gi.adaptive_fun) \n\ngi.fixed <- as.matrix(localG(nigeria$pct_nonfunctional, bwm_fd))\ngi.fixed_fun <- as.matrix(localG(nigeria$pct_functional, bwm_fd))\nnigeria.gi <- cbind(nigeria.gi, gi.fixed) %>%\n  rename(gstat_fixed = gi.fixed) \nnigeria.gi <- cbind(nigeria.gi, gi.fixed_fun) %>%\n  rename(gstat_fixed_fun = gi.fixed_fun) \n\n\nVisualising Z score for Gertis-Ord Gi\nOnce we have attained the Z score for each area, we will visualise the variable using tmap. The code chunk below will generate the map:\n\nShow the code#Fixed Distance Gi Map\nGimap_fixed <-tm_shape(nigeria.gi) +\n  tm_fill(col = \"gstat_fixed\", \n          style = \"pretty\",\n          palette=\"-RdBu\",\n          title = \"local Gi\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"Gi Map (Functional)\",\n            main.title.size = 0.6,\n            main.title.fontface = \"bold\",\n            main.title.position = \"center\",\n            title = \"Fixed Distance Weight\",\n            title.size = 0.6)\n\nGimap_fixed_fun <-tm_shape(nigeria.gi) +\n  tm_fill(col = \"gstat_fixed_fun\", \n          style = \"pretty\",\n          palette=\"-RdBu\",\n          title = \"local Gi\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"Gi Map (Functional)\",\n            main.title.size = 0.6,\n            main.title.fontface = \"bold\",\n            main.title.position = \"center\",\n            title = \"Fixed Distance Weight\",\n            title.size = 0.6)\n\ntmap_arrange(Gimap_fixed,Gimap_fixed_fun)\n\n#Adaptive Distance Gi Map\nGimap_adaptive <-tm_shape(nigeria.gi) +\n  tm_fill(col = \"gstat_adaptive\", \n          style = \"pretty\",\n          palette=\"-RdBu\",\n          title = \"local Gi\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"Gi Map (Non-Functional)\",\n            main.title.size = 0.6,\n            main.title.fontface = \"bold\",\n            main.title.position = \"center\",\n            title = \"Adaptive Distance Weight\",\n            title.size = 0.6)\n\nGimap_adaptive_fun <-tm_shape(nigeria.gi) +\n  tm_fill(col = \"gstat_adaptive_fun\", \n          style = \"pretty\",\n          palette=\"-RdBu\",\n          title = \"local Gi\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"Gi Map (Functional)\",\n            main.title.size = 0.6,\n            main.title.fontface = \"bold\",\n            main.title.position = \"center\",\n            title = \"Adaptive Distance Weight\",\n            title.size = 0.6)\n\ntmap_arrange(Gimap_adaptive,Gimap_adaptive_fun)\n\n\n\n\n\n\nGi map of Fixed Distance (Non-Functional & Functional)\n\n\n\n\n\n\n\n\nGi map of Adaptive Distance (Non-Functional & Functional)\n\n\n\n\nAnalysis of Gi map result\nThe results from the Gi map seems to align to what the LISA map have shown. From the Gi Map, we can see the southern area having more clustered non-functional water point and dispersed functional water point. This would likely be the priority of repair if resources are limited.\nSimilarly, the map also shows more clustered functional water point and dispersed non-functional water point at the northern regions.\nThe result of both LISA map and Gi map shows consistency in the area of concerns and area of focused. This is align to the recent initiatives by ENI that focus on ensuring water access to the North-East region of Nigeria that shows why these regions are not an area of concern. The study by Adebayo Oluwole Eludoyin also substantiate our claim on the importance of focusing on the South-West region of Nigeria to improve the water access for it’s population."
  },
  {
    "objectID": "posts/Geo/Geospatial_Exercise/index.html#conclusion",
    "href": "posts/Geo/Geospatial_Exercise/index.html#conclusion",
    "title": "Geospatial Analytics for Social Good - Understanding Nigeria Water functional and non-functional water point rate",
    "section": "Conclusion",
    "text": "Conclusion\nGeospatial Autocorrelation remains an important tool for organization to better allocate limited resources to solve any issues. The use of LISA and Gi map will allow organisation to better understand the geographical relationship based on the attribute analysed and enhance the quality of decision-making."
  }
]