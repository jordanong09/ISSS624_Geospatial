[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Ong Zhi Rong Jordan",
    "section": "",
    "text": "Currently a full-time servicemen, assuming an appointment in Training & Doctrine Command (TRADOC)."
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "Ong Zhi Rong Jordan",
    "section": "Education",
    "text": "Education\nNanyang Technological University - NTU | Singapore Bachelor of Engineering in Computer Engineering | Aug 2013 - Aug 2016\nSingapore Management University - SMU | Singapore Master of IT in Business (MITB)| Jan 2022 - Nov 2022 (Ongoing)"
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "Ong Zhi Rong Jordan",
    "section": "Experience",
    "text": "Experience\nNil"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Bringing Data to Life",
    "section": "",
    "text": "Geospatial analytics hold tremendous potential to address complex problems facing society. In this study, you are tasked to apply appropriate global and local measures of spatial Association techniques to reveals the spatial patterns of Not Functional water points. For the purpose of this study, Nigeria will be used as the study country.\n\n\n\ngeospatial\n\n\nsf\n\n\nspdep\n\n\ntmap\n\n\n\n\nOng Zhi Rong Jordan\n\n\nNovember 25, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDescribing the presence of systematic spatial variation in a variable. “The first law of geography Everything is related to everything else, but near things are more related than distant things.” Waldo R. Tobler (Tobler, Waldo R. 1970)\n\n\n\ngeospatial\n\n\nsf\n\n\nspdep\n\n\ntmap\n\n\n\n\nOng Zhi Rong Jordan\n\n\nNovember 24, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nComputation of spatial weights using R. Understanding the spatial relationships that exist among the features in the dataset.\n\n\n\ngeospatial\n\n\nsf\n\n\nspdep\n\n\ntmap\n\n\n\n\nOng Zhi Rong Jordan\n\n\nNovember 24, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUtilising the different libraries such as ggplot, tmap and leaflet to visualise geographical data.\n\n\n\ngeospatial\n\n\nsf\n\n\nggplot\n\n\ntmap\n\n\nleaflet\n\n\n\n\nOng Zhi Rong Jordan\n\n\nNovember 23, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUtilising the sf and tidyverse packages to tidy geospatial data.\n\n\n\ngeospatial\n\n\nsf\n\n\n\n\nOng Zhi Rong Jordan\n\n\nNovember 19, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLeveraging on simple data wrangling techniques to create a RFM model. Subsequently, leveraging on unsupervised classification to conduct customer segmentation for targeted marketing.\n\n\n\ntibble\n\n\nclustering\n\n\nunsupervised\n\n\n\n\nOng Zhi Rong Jordan\n\n\nJune 25, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUsing Static, Interactive and Statistical (SIS) Graphs to reveal the demographics and relationships of a city.\n\n\n\ntibble\n\n\nggstatsplot\n\n\n\n\nOng Zhi Rong Jordan\n\n\nJune 8, 2022\n\n\n\n\n\n\n\n\nNo matching items\n\n\n  \n\n\n See all"
  },
  {
    "objectID": "index.html#posts-of-my-own-works",
    "href": "index.html#posts-of-my-own-works",
    "title": "Bringing Data to Life",
    "section": "Posts of my own works",
    "text": "Posts of my own works\n\n\n\n\n  \n\n\n\n\nRFM Model\n\n\nLeveraging on simple data wrangling techniques to create a RFM model. Subsequently, leveraging on unsupervised classification to conduct customer segmentation for targeted marketing.\n\n\n\n\ntibble\n\n\nclustering\n\n\nunsupervised\n\n\n \n\n\n\n\nJune 25, 2022\n\n\nOng Zhi Rong Jordan\n\n\n\n\n\n\n  \n\n\n\n\nSIS Visual Representation\n\n\nUsing Static, Interactive and Statistical (SIS) Graphs to reveal the demographics and relationships of a city.\n\n\n\n\ntibble\n\n\nggstatsplot\n\n\n \n\n\n\n\nJune 8, 2022\n\n\nOng Zhi Rong Jordan\n\n\n\n\n\n\nNo matching items\n\n\n See all posts of my work"
  },
  {
    "objectID": "index.html#posts-from-geospatial-analytics",
    "href": "index.html#posts-from-geospatial-analytics",
    "title": "Bringing Data to Life",
    "section": "Posts from Geospatial Analytics",
    "text": "Posts from Geospatial Analytics\n\n\n\n\n  \n\n\n\n\nGeospatial Analytics for Social Good - Understanding Nigeria Water functional and non-functional water point rate\n\n\nGeospatial analytics hold tremendous potential to address complex problems facing society. In this study, you are tasked to apply appropriate global and local measures of spatial Association techniques to reveals the spatial patterns of Not Functional water points. For the purpose of this study, Nigeria will be used as the study country.\n\n\n\n\ngeospatial\n\n\nsf\n\n\nspdep\n\n\ntmap\n\n\n \n\n\n\n\nNovember 25, 2022\n\n\nOng Zhi Rong Jordan\n\n\n\n\n\n\n  \n\n\n\n\nIntroduction to Global and Local Measures of Spatial Autocorrelation\n\n\nDescribing the presence of systematic spatial variation in a variable. “The first law of geography Everything is related to everything else, but near things are more related than distant things.” Waldo R. Tobler (Tobler, Waldo R. 1970)\n\n\n\n\ngeospatial\n\n\nsf\n\n\nspdep\n\n\ntmap\n\n\n \n\n\n\n\nNovember 24, 2022\n\n\nOng Zhi Rong Jordan\n\n\n\n\n\n\n  \n\n\n\n\nIntroduction to Spatial Weights and Application\n\n\nComputation of spatial weights using R. Understanding the spatial relationships that exist among the features in the dataset.\n\n\n\n\ngeospatial\n\n\nsf\n\n\nspdep\n\n\ntmap\n\n\n \n\n\n\n\nNovember 24, 2022\n\n\nOng Zhi Rong Jordan\n\n\n\n\n\n\n  \n\n\n\n\nIntroduction to Choropleth Mapping\n\n\nUtilising the different libraries such as ggplot, tmap and leaflet to visualise geographical data.\n\n\n\n\ngeospatial\n\n\nsf\n\n\nggplot\n\n\ntmap\n\n\nleaflet\n\n\n \n\n\n\n\nNovember 23, 2022\n\n\nOng Zhi Rong Jordan\n\n\n\n\n\n\n  \n\n\n\n\nData Wrangling of Geospatial Data\n\n\nUtilising the sf and tidyverse packages to tidy geospatial data.\n\n\n\n\ngeospatial\n\n\nsf\n\n\n \n\n\n\n\nNovember 19, 2022\n\n\nOng Zhi Rong Jordan\n\n\n\n\n\n\nNo matching items\n\n\n  \n\n\n See all posts of Geospatial"
  },
  {
    "objectID": "index.html#posts-from-visual-analytics",
    "href": "index.html#posts-from-visual-analytics",
    "title": "Bringing Data to Life",
    "section": "Posts from Visual Analytics",
    "text": "Posts from Visual Analytics\n\n\n\n\n\nNo matching items\n\n\n See all posts of Visual"
  },
  {
    "objectID": "posts/Geo/Geospatial_HOE1/index.html",
    "href": "posts/Geo/Geospatial_HOE1/index.html",
    "title": "Introduction to Choropleth Mapping",
    "section": "",
    "text": "For this analysis, we will use the following packages from CRAN.\nsf - Support for simple features, a standardized way to encode spatial vector data. Binds to ‘GDAL’ for reading and writing data, to ‘GEOS’ for geometrical operations, and to ‘PROJ’ for projection conversions and datum transformations. Uses by default the ‘s2’ package for spherical geometry operations on ellipsoidal (long/lat) coordinates. tidyverse - Loading the core tidyverse packages which will be used for data wrangling and visualisation. tmap - Thematic maps are geographical maps in which spatial data distributions are visualized. This package offers a flexible, layer-based, and easy to use approach to create thematic maps, such as choropleths and bubble maps.\n\npacman::p_load(sf, tidyverse, tmap)"
  },
  {
    "objectID": "posts/Geo/Geospatial_HOE1/index.html#dataset",
    "href": "posts/Geo/Geospatial_HOE1/index.html#dataset",
    "title": "Introduction to Choropleth Mapping",
    "section": "Dataset",
    "text": "Dataset\nFor this analysis, we will extract data that is available from the web.\n\nMaster Plan 2014 Subzone Boundary (Web) from data.gov.sg\nPre-Schools Location from data.gov.sg\nCycling Path from LTADataMall\nLatest version of Singapore Airbnb listing data from Inside Airbnb\n\nExtracting geographical information from Dataset\nWe will leverage on the st_read function to retrieve polygon, line and point feature in both ESRI shapefile and KML format.\n\nmpsz = st_read(dsn = \"data/geospatial\", \n                  layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `C:\\jordanong09\\ISSS624_Geospatial\\posts\\Geo\\Geospatial_HOE1\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\ncyclingpath = st_read(dsn = \"data/geospatial\", \n                      layer = \"CyclingPath\")\n\nReading layer `CyclingPath' from data source \n  `C:\\jordanong09\\ISSS624_Geospatial\\posts\\Geo\\Geospatial_HOE1\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 1625 features and 2 fields\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: 12711.19 ymin: 28711.33 xmax: 42626.09 ymax: 48948.15\nProjected CRS: SVY21\n\npreschool = st_read(\"data/geospatial/pre-schools-location-kml.kml\")\n\nReading layer `PRESCHOOLS_LOCATION' from data source \n  `C:\\jordanong09\\ISSS624_Geospatial\\posts\\Geo\\Geospatial_HOE1\\data\\geospatial\\pre-schools-location-kml.kml' \n  using driver `KML'\nSimple feature collection with 1359 features and 2 fields\nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 103.6824 ymin: 1.248403 xmax: 103.9897 ymax: 1.462134\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84\n\nst_geometry(mpsz)\n\nGeometry set for 323 features \nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\nFirst 5 geometries:\n\nglimpse(mpsz)\n\nRows: 323\nColumns: 16\n$ OBJECTID   <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, …\n$ SUBZONE_NO <int> 1, 1, 3, 8, 3, 7, 9, 2, 13, 7, 12, 6, 1, 5, 1, 1, 3, 2, 2, …\n$ SUBZONE_N  <chr> \"MARINA SOUTH\", \"PEARL'S HILL\", \"BOAT QUAY\", \"HENDERSON HIL…\n$ SUBZONE_C  <chr> \"MSSZ01\", \"OTSZ01\", \"SRSZ03\", \"BMSZ08\", \"BMSZ03\", \"BMSZ07\",…\n$ CA_IND     <chr> \"Y\", \"Y\", \"Y\", \"N\", \"N\", \"N\", \"N\", \"Y\", \"N\", \"N\", \"N\", \"N\",…\n$ PLN_AREA_N <chr> \"MARINA SOUTH\", \"OUTRAM\", \"SINGAPORE RIVER\", \"BUKIT MERAH\",…\n$ PLN_AREA_C <chr> \"MS\", \"OT\", \"SR\", \"BM\", \"BM\", \"BM\", \"BM\", \"SR\", \"QT\", \"QT\",…\n$ REGION_N   <chr> \"CENTRAL REGION\", \"CENTRAL REGION\", \"CENTRAL REGION\", \"CENT…\n$ REGION_C   <chr> \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\",…\n$ INC_CRC    <chr> \"5ED7EB253F99252E\", \"8C7149B9EB32EEFC\", \"C35FEFF02B13E0E5\",…\n$ FMEL_UPD_D <date> 2014-12-05, 2014-12-05, 2014-12-05, 2014-12-05, 2014-12-05…\n$ X_ADDR     <dbl> 31595.84, 28679.06, 29654.96, 26782.83, 26201.96, 25358.82,…\n$ Y_ADDR     <dbl> 29220.19, 29782.05, 29974.66, 29933.77, 30005.70, 29991.38,…\n$ SHAPE_Leng <dbl> 5267.381, 3506.107, 1740.926, 3313.625, 2825.594, 4428.913,…\n$ SHAPE_Area <dbl> 1630379.27, 559816.25, 160807.50, 595428.89, 387429.44, 103…\n$ geometry   <MULTIPOLYGON [m]> MULTIPOLYGON (((31495.56 30..., MULTIPOLYGON (…\n\nhead(mpsz, n=5)  \n\nSimple feature collection with 5 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 25867.68 ymin: 28369.47 xmax: 32362.39 ymax: 30435.54\nProjected CRS: SVY21\n  OBJECTID SUBZONE_NO      SUBZONE_N SUBZONE_C CA_IND      PLN_AREA_N\n1        1          1   MARINA SOUTH    MSSZ01      Y    MARINA SOUTH\n2        2          1   PEARL'S HILL    OTSZ01      Y          OUTRAM\n3        3          3      BOAT QUAY    SRSZ03      Y SINGAPORE RIVER\n4        4          8 HENDERSON HILL    BMSZ08      N     BUKIT MERAH\n5        5          3        REDHILL    BMSZ03      N     BUKIT MERAH\n  PLN_AREA_C       REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR\n1         MS CENTRAL REGION       CR 5ED7EB253F99252E 2014-12-05 31595.84\n2         OT CENTRAL REGION       CR 8C7149B9EB32EEFC 2014-12-05 28679.06\n3         SR CENTRAL REGION       CR C35FEFF02B13E0E5 2014-12-05 29654.96\n4         BM CENTRAL REGION       CR 3775D82C5DDBEFBD 2014-12-05 26782.83\n5         BM CENTRAL REGION       CR 85D9ABEF0A40678F 2014-12-05 26201.96\n    Y_ADDR SHAPE_Leng SHAPE_Area                       geometry\n1 29220.19   5267.381  1630379.3 MULTIPOLYGON (((31495.56 30...\n2 29782.05   3506.107   559816.2 MULTIPOLYGON (((29092.28 30...\n3 29974.66   1740.926   160807.5 MULTIPOLYGON (((29932.33 29...\n4 29933.77   3313.625   595428.9 MULTIPOLYGON (((27131.28 30...\n5 30005.70   2825.594   387429.4 MULTIPOLYGON (((26451.03 30..."
  },
  {
    "objectID": "posts/Geo/Geospatial_HOE1/index.html#plotting-of-map",
    "href": "posts/Geo/Geospatial_HOE1/index.html#plotting-of-map",
    "title": "Introduction to Choropleth Mapping",
    "section": "Plotting of map",
    "text": "Plotting of map\n\nplot(mpsz)\n\n\n\nplot(st_geometry(mpsz))\n\n\n\nplot(mpsz[\"PLN_AREA_N\"])"
  },
  {
    "objectID": "posts/Geo/Geospatial_HOE1/index.html#changing-of-projection",
    "href": "posts/Geo/Geospatial_HOE1/index.html#changing-of-projection",
    "title": "Introduction to Choropleth Mapping",
    "section": "Changing of Projection",
    "text": "Changing of Projection\n\nmpsz3414 <- st_set_crs(mpsz, 3414)\n\npreschool3414 <- st_transform(preschool, \n                              crs = 3414)"
  },
  {
    "objectID": "posts/Geo/Geospatial_HOE1/index.html#importing-and-converting-an-aspatial-data",
    "href": "posts/Geo/Geospatial_HOE1/index.html#importing-and-converting-an-aspatial-data",
    "title": "Introduction to Choropleth Mapping",
    "section": "Importing and Converting An Aspatial Data",
    "text": "Importing and Converting An Aspatial Data\n\nlistings <- read_csv(\"data/aspatial/listings.csv\")\nlist(listings) \n\n[[1]]\n# A tibble: 4,252 × 16\n       id name         host_id host_name neighbourhood_g… neighbourhood latitude\n    <dbl> <chr>          <dbl> <chr>     <chr>            <chr>            <dbl>\n 1  50646 Pleasant Ro…  227796 Sujatha   Central Region   Bukit Timah       1.33\n 2  71609 Ensuite Roo…  367042 Belinda   East Region      Tampines          1.35\n 3  71896 B&B  Room 1…  367042 Belinda   East Region      Tampines          1.35\n 4  71903 Room 2-near…  367042 Belinda   East Region      Tampines          1.35\n 5 275343 Convenientl… 1439258 Joyce     Central Region   Bukit Merah       1.29\n 6 275344 15 mins to … 1439258 Joyce     Central Region   Bukit Merah       1.29\n 7 294281 5 mins walk… 1521514 Elizabeth Central Region   Newton            1.31\n 8 301247 Nice room w… 1552002 Rahul     Central Region   Geylang           1.32\n 9 324945 20 Mins to … 1439258 Joyce     Central Region   Bukit Merah       1.29\n10 330089 Accomo@ RED… 1439258 Joyce     Central Region   Bukit Merah       1.29\n# … with 4,242 more rows, and 9 more variables: longitude <dbl>,\n#   room_type <chr>, price <dbl>, minimum_nights <dbl>,\n#   number_of_reviews <dbl>, last_review <date>, reviews_per_month <dbl>,\n#   calculated_host_listings_count <dbl>, availability_365 <dbl>\n\n\n\nlistings_sf <- st_as_sf(listings, \n                       coords = c(\"longitude\", \"latitude\"),\n                       crs=4326) %>%\n  st_transform(crs = 3414)\n\nglimpse(listings_sf)\n\nRows: 4,252\nColumns: 15\n$ id                             <dbl> 50646, 71609, 71896, 71903, 275343, 275…\n$ name                           <chr> \"Pleasant Room along Bukit Timah\", \"Ens…\n$ host_id                        <dbl> 227796, 367042, 367042, 367042, 1439258…\n$ host_name                      <chr> \"Sujatha\", \"Belinda\", \"Belinda\", \"Belin…\n$ neighbourhood_group            <chr> \"Central Region\", \"East Region\", \"East …\n$ neighbourhood                  <chr> \"Bukit Timah\", \"Tampines\", \"Tampines\", …\n$ room_type                      <chr> \"Private room\", \"Private room\", \"Privat…\n$ price                          <dbl> 80, 178, 81, 81, 52, 40, 72, 41, 49, 49…\n$ minimum_nights                 <dbl> 90, 90, 90, 90, 14, 14, 90, 8, 14, 14, …\n$ number_of_reviews              <dbl> 18, 20, 24, 48, 20, 13, 133, 105, 14, 1…\n$ last_review                    <date> 2014-07-08, 2019-12-28, 2014-12-10, 20…\n$ reviews_per_month              <dbl> 0.22, 0.28, 0.33, 0.67, 0.20, 0.16, 1.2…\n$ calculated_host_listings_count <dbl> 1, 4, 4, 4, 50, 50, 7, 1, 50, 50, 50, 4…\n$ availability_365               <dbl> 365, 365, 365, 365, 353, 364, 365, 90, …\n$ geometry                       <POINT [m]> POINT (22646.02 35167.9), POINT (…\n\n\n\nbuffer_cycling <- st_buffer(cyclingpath, \n                               dist=5, nQuadSegs = 30)\n\nbuffer_cycling$AREA <- st_area(buffer_cycling)\n\nsum(buffer_cycling$AREA)\n\n773143.9 [m^2]\n\n\n\nmpsz3414$`PreSch Count`<- lengths(st_intersects(mpsz3414, preschool3414))\n\nsummary(mpsz3414$`PreSch Count`)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  0.000   0.000   2.000   4.207   6.000  37.000 \n\ntop_n(mpsz3414, 1, `PreSch Count`)\n\nSimple feature collection with 1 feature and 16 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 23449.05 ymin: 46001.23 xmax: 25594.22 ymax: 47996.47\nProjected CRS: SVY21 / Singapore TM\n  OBJECTID SUBZONE_NO      SUBZONE_N SUBZONE_C CA_IND PLN_AREA_N PLN_AREA_C\n1      290          3 WOODLANDS EAST    WDSZ03      N  WOODLANDS         WD\n      REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR   Y_ADDR\n1 NORTH REGION       NR C90769E43EE6B0F2 2014-12-05 24506.64 46991.63\n  SHAPE_Leng SHAPE_Area                       geometry PreSch Count\n1   6603.608    2553464 MULTIPOLYGON (((24786.75 46...           37"
  },
  {
    "objectID": "posts/Geo/Geospatial_HOE1/index.html#plotting-of-geographical-data",
    "href": "posts/Geo/Geospatial_HOE1/index.html#plotting-of-geographical-data",
    "title": "Introduction to Choropleth Mapping",
    "section": "Plotting of geographical data",
    "text": "Plotting of geographical data\n\nmpsz3414$Area <- mpsz3414 %>%\n  st_area()\n\nmpsz3414 <- mpsz3414 %>%\n  mutate(`PreSch Density` = `PreSch Count`/Area * 1000000)\n\nhist(mpsz3414$`PreSch Density`)"
  },
  {
    "objectID": "posts/Geo/Geospatial_HOE1/index.html#visualising-of-geographical-data-using-ggplot2",
    "href": "posts/Geo/Geospatial_HOE1/index.html#visualising-of-geographical-data-using-ggplot2",
    "title": "Introduction to Choropleth Mapping",
    "section": "Visualising of geographical data using ggplot2",
    "text": "Visualising of geographical data using ggplot2\n\nggplot(data=mpsz3414, \n       aes(x= as.numeric(`PreSch Density`)))+\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"light blue\") +\n  labs(title = \"Are pre-school even distributed in Singapore?\",\n       subtitle= \"There are many planning sub-zones with a single pre-school, on the other hand, \\nthere are two planning sub-zones with at least 20 pre-schools\",\n      x = \"Pre-school density (per km sq)\",\n      y = \"Frequency\")\n\n\n\n\n\nggplot(data=mpsz3414, \n       aes(x= as.numeric(`PreSch Density`), y = as.numeric(`PreSch Count`)))+\n  geom_point() +\n  labs(title = \"Are pre-school even distributed in Singapore?\",\n      x = \"Pre-school density (per km sq)\",\n      y = \"Pre-school Count\")"
  },
  {
    "objectID": "posts/Geo/Geospatial_HOE1/index.html#choropleth-mapping-with-r",
    "href": "posts/Geo/Geospatial_HOE1/index.html#choropleth-mapping-with-r",
    "title": "Introduction to Choropleth Mapping",
    "section": "Choropleth Mapping with R",
    "text": "Choropleth Mapping with R\n\npopdata <- read_csv(\"data/aspatial/respopagesextod2011to2020.csv\")\n\n\npopdata2020 <- popdata %>%\n  filter(Time == 2020) %>%\n  group_by(PA, SZ, AG) %>%\n  summarise(`POP` = sum(`Pop`)) %>%\n  ungroup()%>%\n  pivot_wider(names_from=AG, \n              values_from=POP) %>%\n  mutate(YOUNG = rowSums(.[3:6])\n         +rowSums(.[12])) %>%\nmutate(`ECONOMY ACTIVE` = rowSums(.[7:11])+\nrowSums(.[13:15]))%>%\nmutate(`AGED`=rowSums(.[16:21])) %>%\nmutate(`TOTAL`=rowSums(.[3:21])) %>%  \nmutate(`DEPENDENCY` = (`YOUNG` + `AGED`)\n/`ECONOMY ACTIVE`) %>%\n  select(`PA`, `SZ`, `YOUNG`, \n       `ECONOMY ACTIVE`, `AGED`, \n       `TOTAL`, `DEPENDENCY`)\n\n\npopdata2020 <- popdata2020 %>%\n  mutate_at(.vars = vars(PA, SZ), \n          .funs = funs(toupper)) %>%\n  filter(`ECONOMY ACTIVE` > 0)\n\nmpsz_pop2020 <- left_join(mpsz, popdata2020,\n                          by = c(\"SUBZONE_N\" = \"SZ\"))\n\nwrite_rds(mpsz_pop2020, \"data/rds/mpszpop2020.rds\")\n\n\ntmap_mode(\"plot\")\nqtm(mpsz_pop2020, \n    fill = \"DEPENDENCY\")\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"Dependency ratio\") +\n  tm_layout(main.title = \"Distribution of Dependency Ratio by planning subzone\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar() +\n  tm_grid(alpha =0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\\n and Population data from Department of Statistics DOS\", \n             position = c(\"left\", \"bottom\"))\n\n\n\n\n\ntm_shape(mpsz_pop2020) +\n  tm_polygons()\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\")\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\") +\n  tm_borders(lwd = 0.1,  alpha = 1)\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"jenks\") +\n  tm_borders(alpha = 0.5)\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"equal\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\nDIY using quantile and 4 classes.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 4,\n          style = \"quantile\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\nsummary(mpsz_pop2020$DEPENDENCY)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n 0.0000  0.6519  0.7025  0.7742  0.7645 19.0000      92 \n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          breaks = c(0, 0.60, 0.70, 0.80, 0.90, 1.00)) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 6,\n          style = \"quantile\",\n          palette = \"Blues\") +\n  tm_borders(alpha = 0.5)\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          style = \"quantile\",\n          palette = \"-Greens\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"jenks\", \n          palette = \"Blues\", \n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(main.title = \"Distribution of Dependency Ratio by planning subzone \\n(Jenks classification)\",\n            main.title.position = \"center\",\n            main.title.size = 1,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            legend.outside = FALSE,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = FALSE) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"-Greens\") +\n  tm_borders(alpha = 0.5) +\n  tmap_style(\"classic\")\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"No. of persons\") +\n  tm_layout(main.title = \"Distribution of Dependency Ratio \\nby planning subzone\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar(width = 0.15) +\n  tm_grid(lwd = 0.1, alpha = 0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\\n and Population data from Department of Statistics DOS\", \n             position = c(\"left\", \"bottom\")) +\n  tmap_style(\"white\")\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(c(\"YOUNG\", \"AGED\"),\n          style = \"equal\", \n          palette = \"Blues\") +\n  tm_layout(legend.position = c(\"right\", \"bottom\")) +\n  tm_borders(alpha = 0.5) +\n  tmap_style(\"white\")\n\n\n\ntm_shape(mpsz_pop2020)+ \n  tm_polygons(c(\"DEPENDENCY\",\"AGED\"),\n          style = c(\"equal\", \"quantile\"), \n          palette = list(\"Blues\",\"Greens\")) +\n  tm_layout(legend.position = c(\"right\", \"bottom\"))\n\n\n\n\n\ntm_shape(mpsz_pop2020) +\n  tm_fill(\"DEPENDENCY\",\n          style = \"quantile\",\n          palette = \"Blues\",\n          thres.poly = 0) + \n  tm_facets(by=\"REGION_N\", \n            free.coords=TRUE, \n            drop.shapes=TRUE) +\n  tm_layout(legend.show = FALSE,\n            title.position = c(\"center\", \"center\"), \n            title.size = 20) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\nyoungmap <- tm_shape(mpsz_pop2020)+ \n  tm_polygons(\"YOUNG\", \n              style = \"quantile\", \n              palette = \"Blues\")\n\nagedmap <- tm_shape(mpsz_pop2020)+ \n  tm_polygons(\"AGED\", \n              style = \"quantile\", \n              palette = \"Blues\")\n\ntmap_arrange(youngmap, agedmap, asp=1, ncol=2)\n\n\n\n\n\ntm_shape(mpsz_pop2020[mpsz_pop2020$REGION_N==\"CENTRAL REGION\", ])+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\", \n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(legend.outside = TRUE,\n            legend.height = 0.45, \n            legend.width = 5.0,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = FALSE) +\n  tm_borders(alpha = 0.5)"
  },
  {
    "objectID": "posts/geo.html",
    "href": "posts/geo.html",
    "title": "Jordan OZR",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\n\nGeospatial analytics hold tremendous potential to address complex problems facing society. In this study, you are tasked to apply appropriate global and local measures of spatial Association techniques to reveals the spatial patterns of Not Functional water points. For the purpose of this study, Nigeria will be used as the study country.\n\n\n\n\ngeospatial\n\n\nsf\n\n\nspdep\n\n\ntmap\n\n\n \n\n\n\n\nNovember 25, 2022\n\n\nOng Zhi Rong Jordan\n\n\n\n\n\n\n  \n\n\n\n\n\nDescribing the presence of systematic spatial variation in a variable. “The first law of geography Everything is related to everything else, but near things are more related than distant things.” Waldo R. Tobler (Tobler, Waldo R. 1970)\n\n\n\n\ngeospatial\n\n\nsf\n\n\nspdep\n\n\ntmap\n\n\n \n\n\n\n\nNovember 24, 2022\n\n\nOng Zhi Rong Jordan\n\n\n\n\n\n\n  \n\n\n\n\n\nComputation of spatial weights using R. Understanding the spatial relationships that exist among the features in the dataset.\n\n\n\n\ngeospatial\n\n\nsf\n\n\nspdep\n\n\ntmap\n\n\n \n\n\n\n\nNovember 24, 2022\n\n\nOng Zhi Rong Jordan\n\n\n\n\n\n\n  \n\n\n\n\n\nUtilising the different libraries such as ggplot, tmap and leaflet to visualise geographical data.\n\n\n\n\ngeospatial\n\n\nsf\n\n\nggplot\n\n\ntmap\n\n\nleaflet\n\n\n \n\n\n\n\nNovember 23, 2022\n\n\nOng Zhi Rong Jordan\n\n\n\n\n\n\n  \n\n\n\n\n\nUtilising the sf and tidyverse packages to tidy geospatial data.\n\n\n\n\ngeospatial\n\n\nsf\n\n\n \n\n\n\n\nNovember 19, 2022\n\n\nOng Zhi Rong Jordan\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/index.html",
    "href": "posts/index.html",
    "title": "Jordan OZR",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\n\nGeospatial analytics hold tremendous potential to address complex problems facing society. In this study, you are tasked to apply appropriate global and local measures of spatial Association techniques to reveals the spatial patterns of Not Functional water points. For the purpose of this study, Nigeria will be used as the study country.\n\n\n\ngeospatial\n\n\nsf\n\n\nspdep\n\n\ntmap\n\n\n\n\nOng Zhi Rong Jordan\n\n\nNovember 25, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDescribing the presence of systematic spatial variation in a variable. “The first law of geography Everything is related to everything else, but near things are more related than distant things.” Waldo R. Tobler (Tobler, Waldo R. 1970)\n\n\n\ngeospatial\n\n\nsf\n\n\nspdep\n\n\ntmap\n\n\n\n\nOng Zhi Rong Jordan\n\n\nNovember 24, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nComputation of spatial weights using R. Understanding the spatial relationships that exist among the features in the dataset.\n\n\n\ngeospatial\n\n\nsf\n\n\nspdep\n\n\ntmap\n\n\n\n\nOng Zhi Rong Jordan\n\n\nNovember 24, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUtilising the different libraries such as ggplot, tmap and leaflet to visualise geographical data.\n\n\n\ngeospatial\n\n\nsf\n\n\nggplot\n\n\ntmap\n\n\nleaflet\n\n\n\n\nOng Zhi Rong Jordan\n\n\nNovember 23, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUtilising the sf and tidyverse packages to tidy geospatial data.\n\n\n\ngeospatial\n\n\nsf\n\n\n\n\nOng Zhi Rong Jordan\n\n\nNovember 19, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLeveraging on simple data wrangling techniques to create a RFM model. Subsequently, leveraging on unsupervised classification to conduct customer segmentation for targeted marketing.\n\n\n\ntibble\n\n\nclustering\n\n\nunsupervised\n\n\n\n\nOng Zhi Rong Jordan\n\n\nJune 25, 2022\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUsing Static, Interactive and Statistical (SIS) Graphs to reveal the demographics and relationships of a city.\n\n\n\ntibble\n\n\nggstatsplot\n\n\n\n\nOng Zhi Rong Jordan\n\n\nJune 8, 2022\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Me/RFM/index.html",
    "href": "posts/Me/RFM/index.html",
    "title": "RFM Model",
    "section": "",
    "text": "The RFM model has become essential for businesses to identify high value customers and possible churn customers to conduct targeted marketing. Businesses have leverage RFM model to better understand customer behaviours and also calculate Customer Life Time Value (LTV). This could also translate to better budgeting for marketing cost using the (3:1) ratio of LTV:CAC. In this article, I will demonstrate how we can leverage on existing libraries to conduct unsupervised classification and lastly potential future works to enhance the model.\n\nknitr::include_graphics(\"RFM.png\")"
  },
  {
    "objectID": "posts/Me/RFM/index.html#libraries",
    "href": "posts/Me/RFM/index.html#libraries",
    "title": "RFM Model",
    "section": "Libraries",
    "text": "Libraries\nFor this analysis, we will use the following packages from CRAN.\ncluster - Methods for Cluster analysis. Much extended the original from Peter Rousseeuw, Anja Struyf and Mia Hubert, based on Kaufman and Rousseeuw (1990) “Finding Groups in Data”.tidyverse - Loading the core tidyverse packages which will be used for data wrangling and visualisation.factoextra - Extract and Visualize the Results of Multivariate Data Analyses. GGally Extension of ggplot2 by adding several functions to reduce the complexity of combining geometric objects with transformed data.\n\npacman::p_load(cluster, tidyverse, factoextra,lubridate,patchwork, GGally, moments,bestNormalize) #refer to 1st post to understand the usage of pacman\n\npackage 'estimability' successfully unpacked and MD5 sums checked\npackage 'ellipse' successfully unpacked and MD5 sums checked\npackage 'emmeans' successfully unpacked and MD5 sums checked\npackage 'flashClust' successfully unpacked and MD5 sums checked\npackage 'leaps' successfully unpacked and MD5 sums checked\npackage 'scatterplot3d' successfully unpacked and MD5 sums checked\npackage 'FactoMineR' successfully unpacked and MD5 sums checked\npackage 'factoextra' successfully unpacked and MD5 sums checked\n\nThe downloaded binary packages are in\n    C:\\Users\\jorda\\AppData\\Local\\Temp\\RtmpesZQVK\\downloaded_packages\npackage 'GGally' successfully unpacked and MD5 sums checked\n\nThe downloaded binary packages are in\n    C:\\Users\\jorda\\AppData\\Local\\Temp\\RtmpesZQVK\\downloaded_packages\npackage 'lamW' successfully unpacked and MD5 sums checked\npackage 'rngtools' successfully unpacked and MD5 sums checked\npackage 'lobstr' successfully unpacked and MD5 sums checked\npackage 'LambertW' successfully unpacked and MD5 sums checked\npackage 'nortest' successfully unpacked and MD5 sums checked\npackage 'doRNG' successfully unpacked and MD5 sums checked\npackage 'butcher' successfully unpacked and MD5 sums checked\npackage 'bestNormalize' successfully unpacked and MD5 sums checked\n\nThe downloaded binary packages are in\n    C:\\Users\\jorda\\AppData\\Local\\Temp\\RtmpesZQVK\\downloaded_packages"
  },
  {
    "objectID": "posts/Me/RFM/index.html#data-set",
    "href": "posts/Me/RFM/index.html#data-set",
    "title": "RFM Model",
    "section": "Data Set",
    "text": "Data Set\n\nWe will use a customer data set that consist of 6 columns.\n\nCustomer_ID: Identification of Customer\nCategoryGroup: Category group of the item purchased\nCategory: Category of the item purchased\nInvoiceDate: The date of purchased\nQuantity: The number of items purchased\nTotalPrice: The total amount spend on that item"
  },
  {
    "objectID": "posts/Me/RFM/index.html#data-wrangling",
    "href": "posts/Me/RFM/index.html#data-wrangling",
    "title": "RFM Model",
    "section": "Data Wrangling",
    "text": "Data Wrangling\n\ncustomer <- readRDS(\"data/customer.rds\")\n\nLet’s examine the data!\nFrom the summary, we can identify a few potential problems!\n\nCustomer_ID is in numeric not character. # I prefer IDs to be in character form since it is for representation of customer instead of number of customers.\n\nInvoiceDate is not in date time format!\n\nTotalPrice is not in numeric (the symbol was attached to the number)\n\n\nsummary (customer)\n\n  Customer_ID    CategoryGroup        Category         InvoiceDate       \n Min.   :12348   Length:395888      Length:395888      Length:395888     \n 1st Qu.:14132   Class :character   Class :character   Class :character  \n Median :15535   Mode  :character   Mode  :character   Mode  :character  \n Mean   :15462                                                           \n 3rd Qu.:16841                                                           \n Max.   :18287                                                           \n    Quantity        TotalPrice       \n Min.   :   1.00   Length:395888     \n 1st Qu.:   2.00   Class :character  \n Median :   4.00   Mode  :character  \n Mean   :   8.29                     \n 3rd Qu.:  12.00                     \n Max.   :1500.00                     \n\nhead(customer$TotalPrice)\n\n[1] \"£8\" \"£2\" \"£1\" \"£3\" \"£3\" \"£5\"\n\n\nChange of data class\nFor the date time format, we will leverage on lubridate functions to convert our exisiting date to date time format. Since the format is Month/Day/Year, we will use the function mdy. For TotalPrice, there are two symbols found, £ and ,. We will use the gsub function and replace all symbols to an empty space. Lastly, using as.numeric to convert it to a numeric class. For CustomerID, simply use as.character to convert it to character class.\n\ncustomer$InvoiceDate <- mdy(customer$InvoiceDate)\ncustomer$TotalPrice <- as.numeric(gsub(\"[£]|[,]\",\"\",customer$TotalPrice, perl=TRUE))\ncustomer$Customer_ID <- as.character(customer$Customer_ID)\n\nExtracting Recency, Frequency and Monetary\nRecency\nTo extract how recent the customer purchase an item from the store, we will use the last InvoiceDate to substract all the dates a customer purchase from the store and retrieve the minimum number. Since the format of Recency will be in datetime format, we will convert it using the as.numeric function.\n\ncustomer_recency <- customer %>%\n  mutate(recency = (max(InvoiceDate) + 1) - InvoiceDate) %>%\n  group_by(Customer_ID) %>%\n  summarise (Recency = as.numeric(min(recency)))\n\nFrequency\nTo extract how frequent the customer purchase an item from the store, we will use the n() function to find out how many different dates the customer visited the store.\n\ncustomer_frequency <- customer %>%\n  group_by(Customer_ID,InvoiceDate) %>%\n  summarise (count = n()) %>%\n  ungroup() %>%\n  group_by (Customer_ID) %>%\n  summarise (Frequency = n()) %>%\n  ungroup()\n\n\ncustomer_monetary <- customer %>%\n  group_by(Customer_ID) %>%\n  summarise (Monetary = sum(TotalPrice))\n\n\ncustomer_RFM <- customer_recency %>%\n  left_join (customer_frequency, by = \"Customer_ID\") %>%\n  left_join (customer_monetary, by = \"Customer_ID\")\n\nExamining the distribution of the RFM model\n\n# Histogram overlaid with kernel density curve\nrdplot <- ggplot(customer_RFM, aes(x=Recency)) + \n    geom_histogram(aes(y=..density..),      # Histogram with density instead of count on y-axis\n                   binwidth=10,\n                   colour=\"black\", fill=\"white\") +\n    geom_density(alpha=.2, fill=\"#ff9285\") + \n  ylab(\"Density\") +\n  theme_classic() +\n  theme(axis.title.y= element_text(angle=0), axis.ticks.x= element_blank(),\n        axis.line= element_line(color= 'grey'))\n\nfdplot <- ggplot(customer_RFM, aes(x=Frequency)) + \n    geom_histogram(aes(y=..density..),      # Histogram with density instead of count on y-axis\n                   binwidth=2,\n                   colour=\"black\", fill=\"white\") +\n    geom_density(alpha=.2, fill=\"#906efa\") + \n  ylab(\"Density\") +\n  theme_classic() +\n  theme(axis.title.y= element_text(angle=0), axis.ticks.x= element_blank(),\n        axis.line= element_line(color= 'grey'))\n\nmdplot <- ggplot(customer_RFM, aes(x=Monetary)) + \n    geom_histogram(aes(y=..density..),      # Histogram with density instead of count on y-axis\n                   binwidth=250,\n                   colour=\"black\", fill=\"white\") +\n    geom_density(alpha=.2, fill=\"#d18500\") + \n  ylab(\"Density\") +\n  theme_classic() +\n  theme(axis.title.y= element_text(angle=0), axis.ticks.x= element_blank(),\n        axis.line= element_line(color= 'grey'))\n\n\nrdplot + fdplot + mdplot\n\n\n\n\nThrough the skewness and the histogrm, we can conclude that the attributes does not conform to normal distribution. Since all three attributes does not conform to a normal distribution and K-means would perform better with a normal distributed data, we will conduct data transformation. Utilizing the bestNormalise library, we can identify which normalization techniques best suits each attributes based on their distribution.\n\nskewness(customer_RFM$Recency)\n\n[1] 0.7240294\n\nskewness(customer_RFM$Frequency)\n\n[1] 3.781094\n\nskewness(customer_RFM$Monetary)\n\n[1] 1.435198\n\n\nWe can\n\nbestNormalize(customer_RFM$Recency)\n\nBest Normalizing transformation with 5010 Observations\n Estimated Normality Statistics (Pearson P / df, lower => more normal):\n - arcsinh(x): 11.2249\n - Box-Cox: 7.8746\n - Center+scale: 27.8779\n - Exp(x): 22.7727\n - Log_b(x+a): 11.2725\n - orderNorm (ORQ): 1.1695\n - sqrt(x + a): 10.1741\n - Yeo-Johnson: 8.0219\nEstimation method: Out-of-sample via CV with 10 folds and 5 repeats\n \nBased off these, bestNormalize chose:\norderNorm Transformation with 5010 nonmissing obs and ties\n - 552 unique values \n - Original quantiles:\n  0%  25%  50%  75% 100% \n   1   32  121  382  676 \n\nbestNormalize(customer_RFM$Frequency)\n\nBest Normalizing transformation with 5010 Observations\n Estimated Normality Statistics (Pearson P / df, lower => more normal):\n - arcsinh(x): 82.5826\n - Box-Cox: 83.172\n - Center+scale: 82.3083\n - Exp(x): 74.932\n - Log_b(x+a): 82.5978\n - orderNorm (ORQ): 82.2105\n - sqrt(x + a): 82.6232\n - Yeo-Johnson: 83.3307\nEstimation method: Out-of-sample via CV with 10 folds and 5 repeats\n \nBased off these, bestNormalize chose:\nStandardized exp(x) Transformation with 5010 nonmissing obs.:\n Relevant statistics:\n - mean (before standardization) = 2.741283e+28 \n - sd (before standardization) = 1.940317e+30 \n\nbestNormalize(customer_RFM$Monetary)\n\nBest Normalizing transformation with 5010 Observations\n Estimated Normality Statistics (Pearson P / df, lower => more normal):\n - arcsinh(x): 2.3416\n - Box-Cox: 1.8263\n - Center+scale: 20.5024\n - Log_b(x+a): 2.3412\n - orderNorm (ORQ): 1.199\n - sqrt(x + a): 5.5771\n - Yeo-Johnson: 1.815\nEstimation method: Out-of-sample via CV with 10 folds and 5 repeats\n \nBased off these, bestNormalize chose:\norderNorm Transformation with 5010 nonmissing obs and ties\n - 2312 unique values \n - Original quantiles:\n     0%     25%     50%     75%    100% \n   4.00  313.25  713.50 1555.00 5004.00 \n\n\n\ncustomer_RFM_dt <- customer_RFM %>%\n  select(Recency, Frequency, Monetary)\n\nRecency <- orderNorm(customer_RFM_dt$Recency)\nFrequency <- boxcox (customer_RFM_dt$Frequency)\nMonetary <- orderNorm(customer_RFM_dt$Monetary)\n\ncustomer_RFM_dt$Recency <- Recency$x.t\ncustomer_RFM_dt$Frequency <- Frequency$x.t\ncustomer_RFM_dt$Monetary <- Monetary$x.t\n\nskewness(customer_RFM_dt$Recency)\n\n[1] 0.01507482\n\nskewness(customer_RFM_dt$Frequency)\n\n[1] 0.08897177\n\nskewness(customer_RFM_dt$Monetary)\n\n[1] 0.0006851529\n\n\n\n# Histogram overlaid with kernel density curve\nrdplot_dt <- ggplot(customer_RFM_dt, aes(x=Recency)) + \n    geom_histogram(aes(y=..density..),      # Histogram with density instead of count on y-axis\n                   colour=\"black\", fill=\"white\") +\n    geom_density(alpha=.2, fill=\"#ff9285\") + \n  ylab(\"Density\") +\n  theme_classic() +\n  theme(axis.title.y= element_text(angle=0), axis.ticks.x= element_blank(),\n        axis.line= element_line(color= 'grey'))\n\nfdplot_dt <- ggplot(customer_RFM_dt, aes(x=Frequency)) + \n    geom_histogram(aes(y=..density..),      # Histogram with density instead of count on y-axis\n                   colour=\"black\", fill=\"white\") +\n    geom_density(alpha=.2, fill=\"#906efa\") + \n  ylab(\"Density\") +\n  theme_classic() +\n  theme(axis.title.y= element_text(angle=0), axis.ticks.x= element_blank(),\n        axis.line= element_line(color= 'grey'))\n\nmdplot_dt <- ggplot(customer_RFM_dt, aes(x=Monetary)) + \n    geom_histogram(aes(y=..density..),      # Histogram with density instead of count on y-axis\n                   colour=\"black\", fill=\"white\") +\n    geom_density(alpha=.2, fill=\"#d18500\") + \n  ylab(\"Density\") +\n  theme_classic() +\n  theme(axis.title.y= element_text(angle=0), axis.ticks.x= element_blank(),\n        axis.line= element_line(color= 'grey'))\n\n\nrdplot_dt + fdplot_dt + mdplot_dt\n\n\n\n\n\ncustomer_RFM_cluster <- customer_RFM_dt %>%\n  select(Recency, Frequency, Monetary)\n\nK-means Clustering\nTo identify the optimal number of clusters using K means clustering, we will use the fviz_nbclust function and the silhouette and wss. Based on the silhouette score, the optimal cluster is 2 while the WSS score shows either 2 or 3. We will now explore both cluster size.\n\nset.seed(1234)\n\nfviz_nbclust(customer_RFM_cluster, kmeans, method = \"silhouette\")\n\n\n\nfviz_nbclust(customer_RFM_cluster, kmeans, method = \"wss\")\n\n\n\n\nInsights from Cluster\n\nkm_cluster2 <- kmeans(customer_RFM_cluster, \n                     2, \n                     nstart = 25)\n\n\n\nkm_cluster3 <- kmeans(customer_RFM_cluster, \n                     3, \n                     nstart = 25)\n\ncustomer_RFM$km_cluster2 <- as.character(km_cluster2$cluster)\n\ncustomer_RFM$km_cluster3 <- as.character(km_cluster3$cluster)\n\nFrom the table, we can identify that cluster 1 consist of customers on average made a purchase within 94 days, frequent the store 5 times and spend 1.7k. Whereas for cluster 2, the customers recency period on average is about 325 days, frequent on average 1 time and spend about $392. We can say that cluster 1 consist of our high value customers and cluster 2 consist of potential churn customers.\n\ncustomer_RFM %>%\n  group_by(km_cluster2) %>%\n  summarise(mean_recency = mean(Recency),\n            mean_frequency = mean(Frequency),\n            mean_monetary = mean(Monetary),\n            members = n()) \n\n# A tibble: 2 × 5\n  km_cluster2 mean_recency mean_frequency mean_monetary members\n  <chr>              <dbl>          <dbl>         <dbl>   <int>\n1 1                   94.1           5.45         1774.    2595\n2 2                  326.            1.39          393.    2415\n\ncustomer_RFM %>%\n  group_by(km_cluster3) %>%\n  summarise(mean_recency = mean(Recency),\n            mean_frequency = mean(Frequency),\n            mean_monetary = mean(Monetary),\n            members = n()) \n\n# A tibble: 3 × 5\n  km_cluster3 mean_recency mean_frequency mean_monetary members\n  <chr>              <dbl>          <dbl>         <dbl>   <int>\n1 1                  364.            1.11          295.    1681\n2 2                   54.6           7.32         2431.    1391\n3 3                  177.            2.80          865.    1938\n\n\nTo better visualise the distribution of our customers based on their cluster, we will leverage on the ggparcoord to visualise the distribution using a parallel coordinates plot.\n\n# Plot\nggparcoord(customer_RFM,\n    columns = 2:4, groupColumn = 5,\n    showPoints = TRUE,\n    scale=\"uniminmax\",\n    title = \"Parallel Coordinate Plot for the Customer Data\",\n    alphaLines = 0.3\n    ) + \n  theme_classic()+\n  theme(\n    plot.title = element_text(size=10)\n  )  + scale_color_brewer(palette = \"Set2\") + \n  guides(color=guide_legend(title=\"Cluster\"))"
  },
  {
    "objectID": "posts/Me/SIS Representation/index.html",
    "href": "posts/Me/SIS Representation/index.html",
    "title": "SIS Visual Representation",
    "section": "",
    "text": "In this article, I will share how we can leverage on Static, Interactive and Statistical (SIS) graphs to conduct appropriate data visualisation and draw statistical conclusion from the data set. In this article, we will explore varios libraries such as parsetR, ggstatsplot and ggplot."
  },
  {
    "objectID": "posts/Me/SIS Representation/index.html#libraries",
    "href": "posts/Me/SIS Representation/index.html#libraries",
    "title": "SIS Visual Representation",
    "section": "Libraries",
    "text": "Libraries\nInstead of using the base R function such as library() or install.packages(),we will use the p_load function from the pacman package that combine these functions together. Before using the package, you will need to install the package from CRAN.\n\ninstall.packages(\"pacman\")\n\nFor this analysis, we will use the following packages from CRAN.\nparsetR - Visualize your data with interactive d3.js parallel sets with the power and convenience of an htmlwidget.tidyverse - Loading the core tidyverse packages which will be used for data wrangling and visualisation.ggstatsplot - An extension of ggplot2 package for creating graphics with details from statistical tests included in the information-rich plots themselves.patchwork - Combine separate ggplots into the same graphic.\n\npacman::p_load(parsetR, tidyverse, ggstatsplot, patchwork, hrbrthemes)"
  },
  {
    "objectID": "posts/Me/SIS Representation/index.html#data-set",
    "href": "posts/Me/SIS Representation/index.html#data-set",
    "title": "SIS Visual Representation",
    "section": "Data Set",
    "text": "Data Set\n\nTwo different data set for this analysis:\n\n\nParticipants.csv - Information of all participants.\n\nFinancialJournal.csv- Input of the participant’s wages and expenses."
  },
  {
    "objectID": "posts/Me/SIS Representation/index.html#data-wrangling",
    "href": "posts/Me/SIS Representation/index.html#data-wrangling",
    "title": "SIS Visual Representation",
    "section": "Data Wrangling",
    "text": "Data Wrangling\n\nknitr::include_graphics(\"qn1_concept.png\")\n\n\n\n\n\n\n\n\nparticipants <- read_csv(\"rawdata/Participants.csv\")\nfinance <- read_csv(\"rawdata/FinancialJournal.csv\")\n\nReducing of File Size uploading to Git\nTo reduce the requirement to upload the original data set, I will use the saveRDS function to convert my working tibble dataframe to a R data format namely .rds. We will subsequently use the readRDS function to read the data files in R.\n\nsaveRDS(participants, \"participants.rds\")\nsaveRDS(finance, \"finance.rds\")\n\n\nparticipants <- readRDS(\"data/participants.rds\")\nfinance <- readRDS(\"data/finance.rds\")"
  },
  {
    "objectID": "posts/Me/SIS Representation/index.html#data-preparation",
    "href": "posts/Me/SIS Representation/index.html#data-preparation",
    "title": "SIS Visual Representation",
    "section": "Data Preparation",
    "text": "Data Preparation\nThrough the data from the participants, we can identify a total of 1011 participants ad 6 different attributes. The finance data shows the timestamp of the participants log and a category column. It seems like the data is in the long format and therefore we will subsequently pivot the data table to a wide format. We can also see that household size should be a categorical data rather than a numerical data. We address these issues using the dplyr package.\n\nsummary(participants)\n\n participantId    householdSize    haveKids            age       \n Min.   :   0.0   Min.   :1.000   Mode :logical   Min.   :18.00  \n 1st Qu.: 252.5   1st Qu.:1.000   FALSE:710       1st Qu.:29.00  \n Median : 505.0   Median :2.000   TRUE :301       Median :39.00  \n Mean   : 505.0   Mean   :1.964                   Mean   :39.07  \n 3rd Qu.: 757.5   3rd Qu.:3.000                   3rd Qu.:50.00  \n Max.   :1010.0   Max.   :3.000                   Max.   :60.00  \n educationLevel     interestGroup        joviality       \n Length:1011        Length:1011        Min.   :0.000204  \n Class :character   Class :character   1st Qu.:0.240074  \n Mode  :character   Mode  :character   Median :0.477539  \n                                       Mean   :0.493794  \n                                       3rd Qu.:0.746819  \n                                       Max.   :0.999234  \n\nsummary(finance)\n\n participantId      timestamp                        category        \n Min.   :   0.0   Min.   :2022-03-01 00:00:00.00   Length:1856330    \n 1st Qu.: 222.0   1st Qu.:2022-06-14 12:30:00.00   Class :character  \n Median : 464.0   Median :2022-10-06 16:20:00.00   Mode  :character  \n Mean   : 480.8   Mean   :2022-10-07 12:36:41.13                     \n 3rd Qu.: 726.0   3rd Qu.:2023-01-29 19:10:00.00                     \n Max.   :1010.0   Max.   :2023-05-25 00:05:00.00                     \n     amount         \n Min.   :-1562.726  \n 1st Qu.:   -5.594  \n Median :   -4.000  \n Mean   :   19.922  \n 3rd Qu.:   22.856  \n Max.   : 4096.526  \n\n\nAs part of Data Preparation, I prefer to ensure my columns are well worded. This would reduce the need to reword the X and Y axis subsequently for all the plots.\n\nparticipants <- participants %>%\n  rename('Participant Id' = 'participantId', \n         'Household Size' = 'householdSize', \n         'Have Kids' = 'haveKids', \n         'Age' = 'age', \n         'Education Level' = 'educationLevel', \n         'Interest Group' = 'interestGroup', \n         'Joviality' = 'joviality')\n\n\ncolnames(participants) # verify if the columns have been renamed correctly \n\n[1] \"Participant Id\"  \"Household Size\"  \"Have Kids\"       \"Age\"            \n[5] \"Education Level\" \"Interest Group\"  \"Joviality\"      \n\n#rename value \nparticipants$`Education Level` <- sub('HighSchoolOrCollege', \n                                    'High School or College',\n                                    participants$`Education Level`)\n\nparticipants$`Household Size` <- as.factor(participants$`Household Size`)\nparticipants$`Education Level` <- factor(participants$`Education Level`, levels = c(\n  \"Low\", \"High School or College\", \"Bachelors\", \"Graduate\"), ordered = TRUE) #create factor data object to categorise the Education Level by levels.\n\nWe will now examine how many different input categories are there. There are 6 different categories and 1011 participants throughout the period of 1 year and 2 months based on the timestamp. There should be a total of 2,547,720 financial records but the total recorded data was only 1,856,330. This shows some participants might not have recorded their finance throughout the period. We will now identify participants that are not consistent in their input.\n\nunique(finance$category)\n\n[1] \"Wage\"           \"Shelter\"        \"Education\"      \"RentAdjustment\"\n[5] \"Food\"           \"Recreation\"    \n\n\nBased on our analysis of the data, there are 131 participants who have only logged in less than 12 times throughout the period of analysis. We will identify these participants as inactive and will exclude them during our analysis of the the population demographics.\n\nincome <- finance %>% \n  filter(category == 'Wage') %>% # extract only wage data\n  select(participantId, amount) %>%\n  group_by(participantId) %>%\n  summarise(count = n()) %>%\n  ungroup()\n\ninactive <- finance %>% \n  filter(category == 'Wage') %>% # extract only wage data\n  select(participantId, amount) %>%\n  group_by(participantId) %>%\n  summarise(count = n()) %>%\n  filter (count < 13) %>%\n  ungroup()\n\n\ninactivepart <- inactive$participantId\n\nactive_participants <- subset(participants, !(`Participant Id` %in% inactivepart))\n\nSince the period of study is 15 months, we will extract the average monthly wage of each active participants using the summarise function and rounding the answer to 2 decimal place.\n\nactive_finance <- subset(finance, !(participantId %in% inactivepart))\n\nactive_finance <- active_finance %>% \n  filter(category == 'Wage') %>% # extract only wage data\n  select(participantId, amount) %>%\n  group_by(participantId) %>%\n  summarise (Income = round(sum(amount)/15,2)) %>%\n  ungroup()\n\n\nactive_participants <- active_participants %>%\n  left_join (active_finance, by = c(\"Participant Id\" = \"participantId\")) %>%\n  mutate(Joviality = Joviality * 100)"
  },
  {
    "objectID": "posts/Me/SIS Representation/index.html#visualisation-and-insights",
    "href": "posts/Me/SIS Representation/index.html#visualisation-and-insights",
    "title": "SIS Visual Representation",
    "section": "Visualisation and Insights",
    "text": "Visualisation and Insights\nVisualising using Static Graph\nWe will first visualise the distribution of the different attributes.\n\n\ngeom_text() is used to add annotations of the count and % values for geom_bar()\n\nGrids and background color are removed for a cleaner look as annotations are included.\nTo choose the different colours for the graph, I use medialab to decide on the Hue colors based on the number of graphs.\n\n\nage <- ggplot (active_participants, aes (x=Age)) +\n  geom_histogram(binwidth=5, fill=\"#c96d44\", color=\"#e9ecef\", alpha=0.9) +\n    labs(title = \"Age Distribution of Active Participants\", subtitle = \"Bin Size 5\") +\n    theme_ipsum() +\n    theme(\n      plot.title = element_text(size=15), axis.title.y= element_text(angle=0)\n    )\n\nhKids <- active_participants %>%\n  ggplot(aes(x = `Have Kids`)) +\n  geom_bar(fill= '#777acd') +\n  geom_text(stat = 'count',\n           aes(label= paste0(stat(count), ', ', \n                             round(stat(count)/sum(stat(count))*100, \n                             1), '%')), vjust= -0.5, size= 3) +\n  labs(y= 'No. of\\nParticipants', title = \"Distribution of Participants \\nwith/without Kids\") +\n  theme(axis.title.y= element_text(angle=0), axis.ticks.x= element_blank(),\n        panel.background= element_blank(), axis.line= element_line(color= 'grey'))\n\n\nhousehold <- active_participants %>%\n  ggplot(aes(x = `Household Size`)) +\n   geom_bar(fill= '#7aa456') +\n  geom_text(stat = 'count',\n           aes(label= paste0(stat(count), ', ', \n                             round(stat(count)/sum(stat(count))*100, \n                             1), '%')), vjust= -0.5, size= 3) +\n  labs(y= 'No. of\\nParticipants', title = \"Distribution of Participants \\nbased on Household Size\") +\n  theme(axis.title.y= element_text(angle=0), axis.ticks.x= element_blank(),\n        panel.background= element_blank(), axis.line= element_line(color= 'grey'))\n\neducation <- active_participants %>%\n  ggplot(aes(x = `Education Level`)) +\n   geom_bar(fill= '#c65999') +\n  geom_text(stat = 'count',\n           aes(label= paste0(stat(count), ', ', \n                             round(stat(count)/sum(stat(count))*100, \n                             1), '%')), vjust= -0.5, size= 3) +\n  labs(y= 'No. of\\nParticipants', title = \"Distribution of Participants \\nbased on Education Level\") +\n  theme(axis.title.y= element_text(angle=0), axis.ticks.x= element_blank(),\n        panel.background= element_blank(), axis.line= element_line(color= 'grey'), title = element_text(size = 10))\n\n\n(age + hKids)/(household + education) #using patchwork to stitch the different graphs together\n\n\n\n\n\njoy <- ggplot (active_participants, aes (x=Joviality)) +\n  geom_histogram(binwidth=5, fill=\"#9c954d\", color=\"#e9ecef\", alpha=0.9) +\n    labs(title = \"Joviality Distribution of Active Participants\", subtitle = \"Bin Size 5\") +\n    theme_ipsum() +\n    theme(\n      plot.title = element_text(size=15), axis.title.y= element_text(angle=0)\n    )\n\nincome <- ggplot (active_participants, aes (x=Income)) +\n  geom_histogram(binwidth=1000, fill=\"#b067a3\", color=\"#e9ecef\", alpha=0.9) +\n    labs(title = \"Income Distribution of Active Participants\", subtitle = \"Bin Size 1000\") +\n    theme_ipsum() +\n    theme(\n      plot.title = element_text(size=15), axis.title.y= element_text(angle=0)\n    )\n\nincome + joy\n\n\n\n\nWe will conduct binning on our numerical data such as Age, Income and Joviality. We use the ntile function to break the values and case_when() to change the group labels accordingly.\n\nactive_participants_grouped <- active_participants %>%\n  mutate (Income_group = ntile(Income, 4)) %>%\n  mutate (Joviality_group = ntile(Joviality, 4)) %>%\n  mutate (Income_group = case_when(\n    Income_group == 1 ~ \"Low Income\",\n    Income_group == 2 ~ \"Mid-Low Income\",\n    Income_group == 3 ~ \"Mid-High Income\",\n    Income_group == 4 ~ \"High Income\"\n  )) %>%\n  mutate (Joviality_group = case_when(\n    Joviality_group == 1 ~ \"Low Joy\",\n    Joviality_group == 2 ~ \"Mid-Low Joy\",\n    Joviality_group == 3 ~ \"Mid-High Joy\",\n    Joviality_group == 4 ~ \"High Joy\"\n  ))\n\nVisualising using Interactive Graph\nWe will now analyse the data using interactive graphs such as parallel set plot. We will leverage on the parset library to provide interactive function. The interesting feature about the parset function is that it allows the user to dynamically shift the levels of the attributes (top-bottom and left-right), providing the user a more interactive visualisation of the data set.\n\nactive_participants_parset <- active_participants_grouped %>%\n  select (`Household Size`, `Have Kids`, `Education Level`, `Interest Group`, Income_group, Joviality_group)\n\nparset(active_participants_parset)\n\n\n\n\n\nVisualising using Statistical Graph\nFrom the Parset plot, we identify a few probable relationship such as Education Level to Income Level etc. We will now use statistical plot to verify our claim. The ggstatsplot library provides a suite of statistical plot to allow user to choose the plot based on its data set. For this study, since our attributes are in categorical form, I will leverage on the ggbarstats.\nInsights\nPearson’s \\(x^2\\)-test of independence revealed that, across 880 participants,there was a significant association between Income Level, Education Level and Joviality Level. (p-value below alpha value of 0.05). The Bayes Factor for the left analysis revealed that the data were \\(8e^{66}\\) times more probable under the alternative hypothesis as compared to the null hypothesis. This can be considered extreme evidence (Sandra Andraszewicz, 2015) in favor of the alternative hypothesis. The Bayes Factor for the right analysis revealed that the data were 23968348874 times more probable under the alternative hypothesis as compared to the null hypothesis. This can also be considered extreme evidence in favor of the alternative hypothesis.\n\nactive_participants_parset$Income_group <- factor(active_participants_parset$Income_group, levels = c(\n  \"Low Income\", \"Mid-Low Income\", \"Mid-High Income\", \"High Income\"), ordered = TRUE) #create factor data object to segment the Education Level by levels.\n\nincome <- ggbarstats(\n  data = active_participants_parset,\n  x = `Education Level`,\n  y = Income_group,\n  type = \"np\",\n  xlab = \"Income Group\"\n)\n\njoy <- ggbarstats(\n  data = active_participants_parset,\n  x = Joviality_group,\n  y = Income_group,\n  type = \"np\",\n  xlab = \"Income Group\"\n)\n\n\nincome + joy"
  },
  {
    "objectID": "posts/Me/SIS Representation/index.html#conclusion",
    "href": "posts/Me/SIS Representation/index.html#conclusion",
    "title": "SIS Visual Representation",
    "section": "Conclusion",
    "text": "Conclusion\nIt is important for data analyst to understand the importance of static and interactive graphs, how we should leverage these tools to provide appropriate data visualisation and subsequently use statistical graphs to draw statistical conclusion to support the hypothesis."
  },
  {
    "objectID": "posts/me.html",
    "href": "posts/me.html",
    "title": "Jordan OZR",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n  \n\n\n\n\n\nLeveraging on simple data wrangling techniques to create a RFM model. Subsequently, leveraging on unsupervised classification to conduct customer segmentation for targeted marketing.\n\n\n\n\ntibble\n\n\nclustering\n\n\nunsupervised\n\n\n \n\n\n\n\nJune 25, 2022\n\n\nOng Zhi Rong Jordan\n\n\n\n\n\n\n  \n\n\n\n\n\nUsing Static, Interactive and Statistical (SIS) Graphs to reveal the demographics and relationships of a city.\n\n\n\n\ntibble\n\n\nggstatsplot\n\n\n \n\n\n\n\nJune 8, 2022\n\n\nOng Zhi Rong Jordan\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/visual.html",
    "href": "posts/visual.html",
    "title": "Jordan OZR",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Geo/Geospatial_Choropleth/index.html",
    "href": "posts/Geo/Geospatial_Choropleth/index.html",
    "title": "Introduction to Choropleth Mapping",
    "section": "",
    "text": "For this analysis, we will use the following packages from CRAN.\n\n\nsf - Support for simple features, a standardized way to encode spatial vector data. Binds to ‘GDAL’ for reading and writing data, to ‘GEOS’ for geometrical operations, and to ‘PROJ’ for projection conversions and datum transformations. Uses by default the ‘s2’ package for spherical geometry operations on ellipsoidal (long/lat) coordinates.\n\ntidyverse - Loading the core tidyverse packages which will be used for data wrangling and visualisation.\n\ntmap - Thematic maps are geographical maps in which spatial data distributions are visualized. This package offers a flexible, layer-based, and easy to use approach to create thematic maps, such as choropleths and bubble maps.\n\nleaflet - Create and customize interactive maps using the ‘Leaflet’ JavaScript library and the ‘htmlwidgets’ package.\n\nscales - Graphical scales map data to aesthetics, and provide methods for automatically determining breaks and labels for axes and legends.\n\nviridis - color scales in this package to make plots that are pretty, better represent your data, easier to read by those with colorblindness, and print well in gray scale\n\n\npacman::p_load(sf, tidyverse, tmap, leaflet, scales, viridis)"
  },
  {
    "objectID": "posts/Geo/Geospatial_Choropleth/index.html#libraries",
    "href": "posts/Geo/Geospatial_Choropleth/index.html#libraries",
    "title": "Introduction to Choropleth Mapping",
    "section": "Libraries",
    "text": "Libraries\nFor this analysis, we will use the following packages from CRAN.\n\n\nsf - Support for simple features, a standardized way to encode spatial vector data. Binds to ‘GDAL’ for reading and writing data, to ‘GEOS’ for geometrical operations, and to ‘PROJ’ for projection conversions and datum transformations. Uses by default the ‘s2’ package for spherical geometry operations on ellipsoidal (long/lat) coordinates.\n\ntidyverse - Loading the core tidyverse packages which will be used for data wrangling and visualisation.\n\ntmap - Thematic maps are geographical maps in which spatial data distributions are visualized. This package offers a flexible, layer-based, and easy to use approach to create thematic maps, such as choropleths and bubble maps.\n\nleaflet - Create and customize interactive maps using the ‘Leaflet’ JavaScript library and the ‘htmlwidgets’ package.\n\nscales - Graphical scales map data to aesthetics, and provide methods for automatically determining breaks and labels for axes and legends.\n\nviridis - Color scales in this package to make plots that are pretty, better represent your data, easier to read by those with colorblindness, and print well in gray scale.\n\n\npacman::p_load(sf, tidyverse, tmap, leaflet, scales, viridis)"
  },
  {
    "objectID": "posts/Geo/Geospatial_Choropleth/index.html#data-preparation",
    "href": "posts/Geo/Geospatial_Choropleth/index.html#data-preparation",
    "title": "Introduction to Choropleth Mapping",
    "section": "Data Preparation",
    "text": "Data Preparation\nTwo data set will be used to create the choropleth map. They are:\n\nMaster Plan 2014 Subzone Boundary (Web) (i.e. MP14_SUBZONE_WEB_PL) in ESRI shapefile format. It can be downloaded at data.gov.sg This is a geospatial data. It consists of the geographical boundary of Singapore at the planning subzone level. The data is based on URA Master Plan 2014.\nSingapore Residents by Planning Area / Subzone, Age Group, Sex and Type of Dwelling, June 2011-2020 in csv format (i.e. respopagesextod2011to2020.csv). This is an aspatial data fie. It can be downloaded at Department of Statistics, Singapore Although it does not contain any coordinates values, but it’s PA and SZ fields can be used as unique identifiers to geocode to MP14_SUBZONE_WEB_PL shapefile.\n\nImporting of Geospatial Data\nThe code chunk below uses the st_read() function of sf package to import MP14_SUBZONE_WEB_PL shapefile into R as a simple feature data frame called mpsz. To view the tibble data frame, we can simply call the tibble file name mpsz. When you print a tibble, it only shows the first ten rows and all the columns that fit on one screen. It also prints an abbreviated description of the column type.\n\nmpsz <- st_read(dsn = \"data/geospatial\", \n                layer = \"MP14_SUBZONE_WEB_PL\") %>%\n  st_transform(crs = 3414)\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `C:\\jordanong09\\ISSS624_Geospatial\\posts\\Geo\\Geospatial_Choropleth\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\nmpsz\n\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21 / Singapore TM\nFirst 10 features:\n   OBJECTID SUBZONE_NO       SUBZONE_N SUBZONE_C CA_IND      PLN_AREA_N\n1         1          1    MARINA SOUTH    MSSZ01      Y    MARINA SOUTH\n2         2          1    PEARL'S HILL    OTSZ01      Y          OUTRAM\n3         3          3       BOAT QUAY    SRSZ03      Y SINGAPORE RIVER\n4         4          8  HENDERSON HILL    BMSZ08      N     BUKIT MERAH\n5         5          3         REDHILL    BMSZ03      N     BUKIT MERAH\n6         6          7  ALEXANDRA HILL    BMSZ07      N     BUKIT MERAH\n7         7          9   BUKIT HO SWEE    BMSZ09      N     BUKIT MERAH\n8         8          2     CLARKE QUAY    SRSZ02      Y SINGAPORE RIVER\n9         9         13 PASIR PANJANG 1    QTSZ13      N      QUEENSTOWN\n10       10          7       QUEENSWAY    QTSZ07      N      QUEENSTOWN\n   PLN_AREA_C       REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR\n1          MS CENTRAL REGION       CR 5ED7EB253F99252E 2014-12-05 31595.84\n2          OT CENTRAL REGION       CR 8C7149B9EB32EEFC 2014-12-05 28679.06\n3          SR CENTRAL REGION       CR C35FEFF02B13E0E5 2014-12-05 29654.96\n4          BM CENTRAL REGION       CR 3775D82C5DDBEFBD 2014-12-05 26782.83\n5          BM CENTRAL REGION       CR 85D9ABEF0A40678F 2014-12-05 26201.96\n6          BM CENTRAL REGION       CR 9D286521EF5E3B59 2014-12-05 25358.82\n7          BM CENTRAL REGION       CR 7839A8577144EFE2 2014-12-05 27680.06\n8          SR CENTRAL REGION       CR 48661DC0FBA09F7A 2014-12-05 29253.21\n9          QT CENTRAL REGION       CR 1F721290C421BFAB 2014-12-05 22077.34\n10         QT CENTRAL REGION       CR 3580D2AFFBEE914C 2014-12-05 24168.31\n     Y_ADDR SHAPE_Leng SHAPE_Area                       geometry\n1  29220.19   5267.381  1630379.3 MULTIPOLYGON (((31495.56 30...\n2  29782.05   3506.107   559816.2 MULTIPOLYGON (((29092.28 30...\n3  29974.66   1740.926   160807.5 MULTIPOLYGON (((29932.33 29...\n4  29933.77   3313.625   595428.9 MULTIPOLYGON (((27131.28 30...\n5  30005.70   2825.594   387429.4 MULTIPOLYGON (((26451.03 30...\n6  29991.38   4428.913  1030378.8 MULTIPOLYGON (((25899.7 297...\n7  30230.86   3275.312   551732.0 MULTIPOLYGON (((27746.95 30...\n8  30222.86   2208.619   290184.7 MULTIPOLYGON (((29351.26 29...\n9  29893.78   6571.323  1084792.3 MULTIPOLYGON (((20996.49 30...\n10 30104.18   3454.239   631644.3 MULTIPOLYGON (((24472.11 29...\n\n\nImporting Attribute Data into R\nNext, we will import respopagsex2000to2018.csv file into RStudio and save the file into an R dataframe called popdata.\nThe task will be performed by using read_csv() function of readr package as shown in the code chunk below.\n\npopdata <- read_csv(\"data/respopagesextod2011to2020.csv\")\n\nData Wrangling\nThe following data wrangling and transformation functions will be used:\n\npivot_wider() of tidyr package, and\nmutate(), filter(), group_by() and select() of dplyr package\n\n\npopdata2020 <- popdata %>%\n  filter(Time == 2020) %>%\n  group_by(PA, SZ, AG) %>%\n  summarise(`POP` = sum(`Pop`)) %>%\n  ungroup()%>%\n  pivot_wider(names_from=AG, \n              values_from=POP) %>%\n  mutate(YOUNG = rowSums(.[3:6])\n         +rowSums(.[12])) %>%\nmutate(`ECONOMY ACTIVE` = rowSums(.[7:11])+\nrowSums(.[13:15]))%>%\nmutate(`AGED`=rowSums(.[16:21])) %>%\nmutate(`TOTAL`=rowSums(.[3:21])) %>%  \nmutate(`DEPENDENCY` = (`YOUNG` + `AGED`)\n/`ECONOMY ACTIVE`) %>%\n  select(`PA`, `SZ`, `YOUNG`, \n       `ECONOMY ACTIVE`, `AGED`, \n       `TOTAL`, `DEPENDENCY`)\n\nJoining of attribute and geospatial data frame\nBefore we can perform the georelational join, one extra step is required to convert the values in PA and SZ fields to uppercase. This is because the values of PA and SZ fields are made up of upper- and lowercase. On the other, hand the SUBZONE_N and PLN_AREA_N are in uppercase.\nNext, left_join() of dplyr is used to join the geographical data and attribute table using planning subzone name e.g. SUBZONE_N and SZ as the common identifier.\n\npopdata2020 <- popdata2020 %>%\n  mutate_at(.vars = vars(PA, SZ), \n          .funs = funs(toupper)) %>%\n  filter(`ECONOMY ACTIVE` > 0)\n\nmpsz_pop2020 <- left_join(mpsz, popdata2020,\n                          by = c(\"SUBZONE_N\" = \"SZ\"))"
  },
  {
    "objectID": "posts/Geo/Geospatial_Choropleth/index.html#choropleth-mapping-of-geospatial-data",
    "href": "posts/Geo/Geospatial_Choropleth/index.html#choropleth-mapping-of-geospatial-data",
    "title": "Introduction to Choropleth Mapping",
    "section": "Choropleth Mapping of Geospatial Data",
    "text": "Choropleth Mapping of Geospatial Data\nPlotting using TMap\ntmap has similar syntax to the popular ggplot2 but will also produce a reasonable map with only a couple of lines of code. A default colour scheme will be given where this is not specified and passed to tm_polygons and a legend will also be created by default.\ntmap also offer the user two views, static (plot) or interactive (view).\nTwo approaches can be used to prepare thematic map using tmap, they are:\n\nPlotting a thematic map quickly by using qtm().\nPlotting highly customisable thematic map by using tmap elements.\n\nPlotting a choropleth map quickly by using qtm()\nqtm is termed as quick thematic mode allow users to quickly draw a choropleth with a single line of code. It is concise and provides a good default visualisation in many cases. We will explore the different view that tmap provides.\nThe code chunk below will draw an interactive cartographic standard choropleth map as shown below. The fill argument is used to map the attribute. (i.e. DEPENDENCY)\nThe interactive mode uses the leaflet library. Since the leaflet library require the sf object to be in WGS84, we need to set the tmap_options to true to allow our data set which is SVY21 to be plotted on the leaflet map.\n\ntmap_mode(\"view\")\ntmap_options(check.and.fix = TRUE)\nqtm(mpsz_pop2020, \n    fill = \"DEPENDENCY\")\n\n\n\n\n\ntmap_mode(\"plot\")\n\nThe code chunk below will draw a static cartographic standard choropleth map as shown below.\n\nqtm(mpsz_pop2020, \n    fill = \"DEPENDENCY\")\n\n\n\n\n\nstatic map using qtm\n\n\n\n\nCreating a choropleth map by using tmap’s elements\nDespite its usefulness of drawing a choropleth map quickly and easily, the disadvantge of qtm() is that it makes aesthetics of individual layers harder to control. To draw a high quality cartographic choropleth map as shown in the figure below, tmap’s drawing elements should be used. In the following sub-section, we will share with you tmap functions that used to plot these elements.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"Dependency ratio\") +\n  tm_layout(main.title = \"Distribution of Dependency Ratio by planning subzone\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar() +\n  tm_grid(alpha =0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\\n and Population data from Department of Statistics DOS\", \n             position = c(\"left\", \"bottom\"))\n\n\n\n\n\nstatic tmap with tmap elements\n\n\n\n\nDrawing a base map\nThe basic building block of tmap is tm_shape() followed by one or more layer elemments such as tm_fill() and tm_polygons().\nIn the code chunk below, tm_shape() is used to define the input data (i.e mpsz_pop2020) and tm_polygons() is used to draw the planning subzone polygons\n\ntm_shape(mpsz_pop2020) +\n  tm_polygons()\n\n\n\n\n\nbase map without elements\n\n\n\n\nDrawing a choropleth map using tm_polygons()\nTo draw a choropleth map showing the geographical distribution of a selected variable by planning subzone, we just need to assign the target variable such as Dependency to tm_polygons(). This is similar to the qtm drawn earlier.\n\nThe default interval binning used to draw the choropleth map is called “pretty”.\nThe default colour scheme used is YlOrRd of ColorBrewer\nBy default, Missing value will be shaded in grey.\n\n\ntm_shape(mpsz_pop2020)+\n  tm_polygons(\"DEPENDENCY\")\n\n\n\n\n\nmap with polygons\n\n\n\n\nDrawing a choropleth map using tm_fill() and *tm_border()**\nActually, tm_polygons() is a wraper of tm_fill() and tm_border(). tm_fill() shades the polygons by using the default colour scheme and tm_borders() adds the borders of the shapefile onto the choropleth map.\nThe code chunk below draws a choropleth map by using tm_fill() alone.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\")\n\n\n\n\n\nmap with fill only\n\n\n\n\nTo add the boundary of the planning subzones, tm_borders will be used as shown in the code chunk below.\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\") +\n  tm_borders(lwd = 0.1,  alpha = 1)\n\n\n\n\n\nmap with fill and borders\n\n\n\n\nData classification methods of tmap\nMost choropleth maps employ some methods of data classification. The point of classification is to take a large number of observations and group them into data ranges or classes.\ntmap provides a total ten data classification methods, namely: fixed, sd, equal, pretty (default), quantile, kmeans, hclust, bclust, fisher, and jenks.\nThe code chunk below shows multiple data classification methods and classes to illustrate the difference. tmap_arrange is used to display the consolidated maps in grid form.\n\ntmap1 <-  tm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 4,\n          style = \"quantile\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(title = \"Quantile - 4 Class\", title.size = 0.7, legend.text.size = 0.4)\n\ntmap2 <-  tm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 4,\n          style = \"jenks\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(title = \"Jenks - 4 Class\", title.size = 0.7, legend.text.size = 0.4)\n\ntmap3 <-  tm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 4,\n          style = \"equal\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(title = \"Equal - 5 Class\", title.size = 0.7, legend.text.size = 0.4)\n\ntmap4 <-  tm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"hclust\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(title = \"Hclust - 5 Class\", title.size = 0.7, legend.text.size = 0.4)\n\ntmap5 <-  tm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"sd\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(title = \"Sd - 5 Class\", title.size = 0.7, legend.text.size = 0.4)\n\n\ntmap6 <-  tm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 5,\n          style = \"kmeans\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(title = \"Kmeans - 5 Class\", title.size = 0.7, legend.text.size = 0.4)\n\ntmap_arrange(tmap1, tmap2, tmap3, tmap4, tmap5, tmap6, ncol = 3)\n\n\n\n\n\ntmap with multiple data classification methods and classes\n\n\n\n\n\nsummary(mpsz_pop2020$DEPENDENCY)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max.    NA's \n 0.0000  0.6519  0.7025  0.7742  0.7645 19.0000      92 \n\ntmap_mode(\"plot\")\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          breaks = c(0, 0.60, 0.70, 0.80, 0.90, 1.00)) +\n  tm_borders(alpha = 0.5) \n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          n = 6,\n          style = \"quantile\",\n          palette = \"Blues\") +\n  tm_borders(alpha = 0.5)\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\",\n          style = \"quantile\",\n          palette = \"-Greens\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"jenks\", \n          palette = \"Blues\", \n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(main.title = \"Distribution of Dependency Ratio by planning subzone \\n(Jenks classification)\",\n            main.title.position = \"center\",\n            main.title.size = 1,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            legend.outside = FALSE,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = FALSE) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"-Greens\") +\n  tm_borders(alpha = 0.5) +\n  tmap_style(\"classic\")\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\",\n          title = \"No. of persons\") +\n  tm_layout(main.title = \"Distribution of Dependency Ratio \\nby planning subzone\",\n            main.title.position = \"center\",\n            main.title.size = 1.2,\n            legend.height = 0.45, \n            legend.width = 0.35,\n            frame = TRUE) +\n  tm_borders(alpha = 0.5) +\n  tm_compass(type=\"8star\", size = 2) +\n  tm_scale_bar(width = 0.15) +\n  tm_grid(lwd = 0.1, alpha = 0.2) +\n  tm_credits(\"Source: Planning Sub-zone boundary from Urban Redevelopment Authorithy (URA)\\n and Population data from Department of Statistics DOS\", \n             position = c(\"left\", \"bottom\")) +\n  tmap_style(\"white\")\n\n\n\n\n\ntm_shape(mpsz_pop2020)+\n  tm_fill(c(\"YOUNG\", \"AGED\"),\n          style = \"equal\", \n          palette = \"Blues\") +\n  tm_layout(legend.position = c(\"right\", \"bottom\")) +\n  tm_borders(alpha = 0.5) +\n  tmap_style(\"white\")\n\n\n\ntm_shape(mpsz_pop2020)+ \n  tm_polygons(c(\"DEPENDENCY\",\"AGED\"),\n          style = c(\"equal\", \"quantile\"), \n          palette = list(\"Blues\",\"Greens\")) +\n  tm_layout(legend.position = c(\"right\", \"bottom\"))\n\n\n\n\n\ntm_shape(mpsz_pop2020) +\n  tm_fill(\"DEPENDENCY\",\n          style = \"quantile\",\n          palette = \"Blues\",\n          thres.poly = 0) + \n  tm_facets(by=\"REGION_N\", \n            free.coords=TRUE, \n            drop.shapes=TRUE) +\n  tm_layout(legend.show = FALSE,\n            title.position = c(\"center\", \"center\"), \n            title.size = 20) +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\nyoungmap <- tm_shape(mpsz_pop2020)+ \n  tm_polygons(\"YOUNG\", \n              style = \"quantile\", \n              palette = \"Blues\")\n\nagedmap <- tm_shape(mpsz_pop2020)+ \n  tm_polygons(\"AGED\", \n              style = \"quantile\", \n              palette = \"Blues\")\n\ntmap_arrange(youngmap, agedmap, asp=1, ncol=2)\n\n\n\n\n\ntm_shape(mpsz_pop2020[mpsz_pop2020$REGION_N==\"CENTRAL REGION\", ])+\n  tm_fill(\"DEPENDENCY\", \n          style = \"quantile\", \n          palette = \"Blues\", \n          legend.hist = TRUE, \n          legend.is.portrait = TRUE,\n          legend.hist.z = 0.1) +\n  tm_layout(legend.outside = TRUE,\n            legend.height = 0.45, \n            legend.width = 5.0,\n            legend.position = c(\"right\", \"bottom\"),\n            frame = FALSE) +\n  tm_borders(alpha = 0.5)\n\n\n\n\nPlotting using ggplot\nggplot provides the user much more flexibility in the layers required on the map. Since our object is an sf object, we will use geom_sf which will automatically detect a geometry column and map it. coord_sf is also used to govern the map projection.\n\nggmap <- ggplot(data = mpsz_pop2020) +\n  geom_sf(aes(fill = YOUNG)) +\n  geom_text(aes(x = X_ADDR, y = Y_ADDR, label = PLN_AREA_C), size = 1) + #input the planning area labels\n  xlab(\"Longitude\") + ylab(\"Latitude\") + #x and y axis name\n  ggtitle(\"Dependency level across Planning Area\") + #title\n  theme_bw() + #theme chosen\n  theme(panel.grid.major = element_line(color = gray(.5), linetype = \"dashed\", size = 0.5),\n        panel.background = element_rect(fill = \"aliceblue\")) + \n  coord_sf(crs = st_crs(3414)) \n\nggmap\n\n\n\n\nThe viridis package also allow the user to improve the colour scaling on the plot. Since we use fill to fill the map with the Young attribute, we will use scale_fill_viridis to scale the variable based on the viridis palette.\n\nggmap + scale_fill_viridis(option = \"magma\", direction = -1)\n\n\n\n\nPlotting using leaflet\nLeaflet is one of the most popular open-source JavaScript libraries for interactive maps. This package has grown significantly in popularity in recent years and has fast become common currency amongst companies wishing to dynamically visualize its data. It is an excellent option to consider where the patterns in your data are large and complex and where you have constituent polygons of varying sizes.\nFeatures\n\nInteractive panning/zooming\n\nCompose maps using arbitrary combinations of:\n\nMap tiles\nMarkers\nPolygons\nLines\nPopups\nGeoJSON\n\n\nCreate maps right from the R console or RStudio\nEmbed maps in knitr/R Markdown documents and Shiny apps\nEasily render spatial objects from the sp or sf packages, or data frames with latitude/longitude columns\nUse map bounds and mouse events to drive Shiny logic\nDisplay maps in non spherical mercator projections\nAugment map features using chosen plugins from leaflet plugins repository\n\nData Preparation for leaflet mapping\nFirstly, we create a new column and scale the Young attribute from 0 - 100. We use the colorBin function to maps numeric input data to a fixed number of output colors using the bin created. We then create the interactive labels using the sprintf function.\n\nmpsz_pop2020_leaflet <- mpsz_pop2020 %>%\n  mutate (youngpct = rescale(YOUNG, to = c(0,100)))\n\nmpsz_pop2020_leaflet$youngpct[is.nan(mpsz_pop2020_leaflet$youngpct)]<-0\n\nbins <- c(0, 20, 30, 40, 50, 60, 70, 80, 90, Inf)\npal <- colorBin(\"YlOrRd\", domain = mpsz_pop2020_leaflet$youngpct, bins = bins)\n\nlabels <- sprintf(\n  \"<strong>%s</strong><br/>%g Young Pct\",\n  mpsz_pop2020_leaflet$PLN_AREA_N, mpsz_pop2020_leaflet$youngpct\n) %>% lapply(htmltools::HTML)\n\nCRS projection for leaflet mapping\nThe Leaflet package expects all point, line, and shape data to be specified in latitude and longitude using WGS 84 (a.k.a. EPSG:4326). By default, when displaying this data it projects everything to EPSG:3857 and expects that any map tiles are also displayed in EPSG:3857.\nTherefore, we will need to transform our sf object to the correct crs using st_transform.\n\nmpsz_pop2020_leaflet <- mpsz_pop2020_leaflet %>%\n  st_transform(crs = 4326)\n\nPlotting of leaflet map\nThe easiest way to add tiles is by calling addTiles() with no arguments; by default, OpenStreetMap tiles are used. But many popular free third-party basemaps can be added using the addProviderTiles() function, which is implemented using the leaflet-providers plugin.\nAs a convenience, leaflet also provides a named list of all the third-party tile providers that are supported by the plugin. This enables you to use auto-completion feature of your favorite R IDE (like RStudio) and not have to remember or look up supported tile providers; just type providers$ and choose from one of the options. You can also use names(providers) to view all of the options. For this visualisation, I will use the CartoDB.Positron tiles.\n\nleaflet(mpsz_pop2020_leaflet) %>%\n  addProviderTiles(providers$CartoDB.Positron) %>%\n  addPolygons(\n  fillColor = ~pal(youngpct),\n  weight = 1,\n  opacity = 1,\n  color = \"white\",\n  dashArray = \"3\",\n  fillOpacity = 0.7,\n  highlight = highlightOptions(\n    weight = 5,\n    color = \"#666\",\n    dashArray = \"\",\n    fillOpacity = 0.7,\n    bringToFront = TRUE),\n  label = labels,\n  labelOptions = labelOptions(\n    style = list(\"font-weight\" = \"normal\", padding = \"3px 8px\"),\n    textsize = \"15px\",\n    direction = \"auto\")) %>%\naddLegend(pal = pal, values = ~youngpct, opacity = 0.7, title = NULL,\n                position = \"bottomright\")"
  },
  {
    "objectID": "posts/Geo/Geospatial_Datawrangling/index.html",
    "href": "posts/Geo/Geospatial_Datawrangling/index.html",
    "title": "Data Wrangling of Geospatial Data",
    "section": "",
    "text": "For this analysis, we will use the following packages from CRAN.\n\n\nsf - Support for simple features, a standardized way to encode spatial vector data. Binds to ‘GDAL’ for reading and writing data, to ‘GEOS’ for geometrical operations, and to ‘PROJ’ for projection conversions and datum transformations. Uses by default the ‘s2’ package for spherical geometry operations on ellipsoidal (long/lat) coordinates.\n\ntidyverse - Loading the core tidyverse packages which will be used for data wrangling and visualisation.\n\n\npacman::p_load(sf, tidyverse)"
  },
  {
    "objectID": "posts/Geo/Geospatial_Datawrangling/index.html#dataset",
    "href": "posts/Geo/Geospatial_Datawrangling/index.html#dataset",
    "title": "Data Wrangling of Geospatial Data",
    "section": "Dataset",
    "text": "Dataset\nFor this analysis, we will extract data that is available from the web.\n\nMaster Plan 2014 Subzone Boundary (Web) from data.gov.sg\nPre-Schools Location from data.gov.sg\nCycling Path from LTADataMall\nLatest version of Singapore Airbnb listing data from Inside Airbnb\n\nExtracting geographical information from Dataset\nWe will leverage on the st_read function to retrieve polygon, line and point feature in both ESRI shapefile and KML format.\nThe st_geometry function returns an object of class sfc whereas the glimpse function from dplyr act as a transposed version of the print function that shows the values of the different columns.\nTo check all the classes within the dataset, we use the sapply function to run the class function through all the columns within the data set and return their classes.\n\nmpsz <- st_read(dsn = \"data/geospatial\", \n                  layer = \"MP14_SUBZONE_WEB_PL\")\n\nReading layer `MP14_SUBZONE_WEB_PL' from data source \n  `C:\\jordanong09\\ISSS624_Geospatial\\posts\\Geo\\Geospatial_Datawrangling\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 323 features and 15 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\n\ncyclingpath <- st_read(dsn = \"data/geospatial\", \n                      layer = \"CyclingPath\")\n\nReading layer `CyclingPath' from data source \n  `C:\\jordanong09\\ISSS624_Geospatial\\posts\\Geo\\Geospatial_Datawrangling\\data\\geospatial' \n  using driver `ESRI Shapefile'\nSimple feature collection with 1625 features and 2 fields\nGeometry type: LINESTRING\nDimension:     XY\nBounding box:  xmin: 12711.19 ymin: 28711.33 xmax: 42626.09 ymax: 48948.15\nProjected CRS: SVY21\n\npreschool <- st_read(\"data/geospatial/pre-schools-location-kml.kml\")\n\nReading layer `PRESCHOOLS_LOCATION' from data source \n  `C:\\jordanong09\\ISSS624_Geospatial\\posts\\Geo\\Geospatial_Datawrangling\\data\\geospatial\\pre-schools-location-kml.kml' \n  using driver `KML'\nSimple feature collection with 1359 features and 2 fields\nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 103.6824 ymin: 1.248403 xmax: 103.9897 ymax: 1.462134\nz_range:       zmin: 0 zmax: 0\nGeodetic CRS:  WGS 84\n\nst_geometry(mpsz)\n\nGeometry set for 323 features \nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21\nFirst 5 geometries:\n\nglimpse(mpsz)\n\nRows: 323\nColumns: 16\n$ OBJECTID   <int> 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, …\n$ SUBZONE_NO <int> 1, 1, 3, 8, 3, 7, 9, 2, 13, 7, 12, 6, 1, 5, 1, 1, 3, 2, 2, …\n$ SUBZONE_N  <chr> \"MARINA SOUTH\", \"PEARL'S HILL\", \"BOAT QUAY\", \"HENDERSON HIL…\n$ SUBZONE_C  <chr> \"MSSZ01\", \"OTSZ01\", \"SRSZ03\", \"BMSZ08\", \"BMSZ03\", \"BMSZ07\",…\n$ CA_IND     <chr> \"Y\", \"Y\", \"Y\", \"N\", \"N\", \"N\", \"N\", \"Y\", \"N\", \"N\", \"N\", \"N\",…\n$ PLN_AREA_N <chr> \"MARINA SOUTH\", \"OUTRAM\", \"SINGAPORE RIVER\", \"BUKIT MERAH\",…\n$ PLN_AREA_C <chr> \"MS\", \"OT\", \"SR\", \"BM\", \"BM\", \"BM\", \"BM\", \"SR\", \"QT\", \"QT\",…\n$ REGION_N   <chr> \"CENTRAL REGION\", \"CENTRAL REGION\", \"CENTRAL REGION\", \"CENT…\n$ REGION_C   <chr> \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\", \"CR\",…\n$ INC_CRC    <chr> \"5ED7EB253F99252E\", \"8C7149B9EB32EEFC\", \"C35FEFF02B13E0E5\",…\n$ FMEL_UPD_D <date> 2014-12-05, 2014-12-05, 2014-12-05, 2014-12-05, 2014-12-05…\n$ X_ADDR     <dbl> 31595.84, 28679.06, 29654.96, 26782.83, 26201.96, 25358.82,…\n$ Y_ADDR     <dbl> 29220.19, 29782.05, 29974.66, 29933.77, 30005.70, 29991.38,…\n$ SHAPE_Leng <dbl> 5267.381, 3506.107, 1740.926, 3313.625, 2825.594, 4428.913,…\n$ SHAPE_Area <dbl> 1630379.27, 559816.25, 160807.50, 595428.89, 387429.44, 103…\n$ geometry   <MULTIPOLYGON [m]> MULTIPOLYGON (((31495.56 30..., MULTIPOLYGON (…\n\nsapply(mpsz, class)\n\n$OBJECTID\n[1] \"integer\"\n\n$SUBZONE_NO\n[1] \"integer\"\n\n$SUBZONE_N\n[1] \"character\"\n\n$SUBZONE_C\n[1] \"character\"\n\n$CA_IND\n[1] \"character\"\n\n$PLN_AREA_N\n[1] \"character\"\n\n$PLN_AREA_C\n[1] \"character\"\n\n$REGION_N\n[1] \"character\"\n\n$REGION_C\n[1] \"character\"\n\n$INC_CRC\n[1] \"character\"\n\n$FMEL_UPD_D\n[1] \"Date\"\n\n$X_ADDR\n[1] \"numeric\"\n\n$Y_ADDR\n[1] \"numeric\"\n\n$SHAPE_Leng\n[1] \"numeric\"\n\n$SHAPE_Area\n[1] \"numeric\"\n\n$geometry\n[1] \"sfc_MULTIPOLYGON\" \"sfc\""
  },
  {
    "objectID": "posts/Geo/Geospatial_Datawrangling/index.html#plotting-of-geospatial-data",
    "href": "posts/Geo/Geospatial_Datawrangling/index.html#plotting-of-geospatial-data",
    "title": "Data Wrangling of Geospatial Data",
    "section": "Plotting of geospatial data",
    "text": "Plotting of geospatial data\nUnlike non-geospatial dataset where we plot the data using charts, we will leverage on map-based visualisation to draw insights from our geospatial data. The plot function uses the geometry data, contained primarily in the polygons slot. plot is one of the most useful functions in R, as it changes its behaviour depending on the input data. From the example below, we can see how we manipulate the plot based on how we subset the dataset.\n\nplot(mpsz) #plot based on the different column attributes\n\n\n\nplot(mpsz[\"PLN_AREA_N\"]) #colour plot based on column `PLN_AREA_N`\n\n\n\nplot(st_geometry(mpsz)) #only plot the basic geometry of the polygon data\ncondition <- mpsz$SUBZONE_NO > 5 #set a condition\nplot(mpsz[condition, ], col = \"turquoise\", add = TRUE) #layer the condition above the initial plot"
  },
  {
    "objectID": "posts/Geo/Geospatial_Datawrangling/index.html#changing-of-projection",
    "href": "posts/Geo/Geospatial_Datawrangling/index.html#changing-of-projection",
    "title": "Data Wrangling of Geospatial Data",
    "section": "Changing of Projection",
    "text": "Changing of Projection\nThe Coordinate Reference System (CRS) of spatial objects defines where they are placed on the Earth’s surface. We need to ensure the CRS of our sf objects are correct. Since Singapore uses EPSG:3414 - SVY21 / Singapore TM and from the above details, we understand that all the sf object does not conform to the correct CRS (WGS 84 or SVY21). We will utilise two different function, st_set_crs or st_transform to manually change the CRS of our sp object to the desired value.\n\nmpsz3414 <- st_set_crs(mpsz, 3414)\n\npreschool3414 <- st_transform(preschool, \n                              crs = 3414)\n\n\nst_geometry(mpsz3414)\n\nGeometry set for 323 features \nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 2667.538 ymin: 15748.72 xmax: 56396.44 ymax: 50256.33\nProjected CRS: SVY21 / Singapore TM\nFirst 5 geometries:\n\nst_geometry(preschool3414)\n\nGeometry set for 1359 features \nGeometry type: POINT\nDimension:     XYZ\nBounding box:  xmin: 11203.01 ymin: 25667.6 xmax: 45404.24 ymax: 49300.88\nz_range:       zmin: 0 zmax: 0\nProjected CRS: SVY21 / Singapore TM\nFirst 5 geometries:"
  },
  {
    "objectID": "posts/Geo/Geospatial_Datawrangling/index.html#importing-and-converting-an-aspatial-data",
    "href": "posts/Geo/Geospatial_Datawrangling/index.html#importing-and-converting-an-aspatial-data",
    "title": "Data Wrangling of Geospatial Data",
    "section": "Importing and Converting an Aspatial Data",
    "text": "Importing and Converting an Aspatial Data\nR provides the function to convert any foreign object to an sf object using the st_as_sf function. This will allow user to provide a data table that consist of the longitude and latitude and select the correct CRS to transform it to the approriate sf object.\nAfter importing the data, we will examine the dataframe using the list function.\n\nlistings <- read_csv(\"data/aspatial/listings.csv\")\nlist(listings) \n\n[[1]]\n# A tibble: 4,252 × 16\n       id name         host_id host_name neighbourhood_g… neighbourhood latitude\n    <dbl> <chr>          <dbl> <chr>     <chr>            <chr>            <dbl>\n 1  50646 Pleasant Ro…  227796 Sujatha   Central Region   Bukit Timah       1.33\n 2  71609 Ensuite Roo…  367042 Belinda   East Region      Tampines          1.35\n 3  71896 B&B  Room 1…  367042 Belinda   East Region      Tampines          1.35\n 4  71903 Room 2-near…  367042 Belinda   East Region      Tampines          1.35\n 5 275343 Convenientl… 1439258 Joyce     Central Region   Bukit Merah       1.29\n 6 275344 15 mins to … 1439258 Joyce     Central Region   Bukit Merah       1.29\n 7 294281 5 mins walk… 1521514 Elizabeth Central Region   Newton            1.31\n 8 301247 Nice room w… 1552002 Rahul     Central Region   Geylang           1.32\n 9 324945 20 Mins to … 1439258 Joyce     Central Region   Bukit Merah       1.29\n10 330089 Accomo@ RED… 1439258 Joyce     Central Region   Bukit Merah       1.29\n# … with 4,242 more rows, and 9 more variables: longitude <dbl>,\n#   room_type <chr>, price <dbl>, minimum_nights <dbl>,\n#   number_of_reviews <dbl>, last_review <date>, reviews_per_month <dbl>,\n#   calculated_host_listings_count <dbl>, availability_365 <dbl>\n\n\nThe output reveals that the data frame consists of 4252 rows and 16 columns. The column longtitude and latitude will be required for to transform this data frame to a sf object.\n\nlistings_sf <- st_as_sf(listings, \n                       coords = c(\"longitude\", \"latitude\"),\n                       crs=4326) %>%\n  st_transform(crs = 3414)\n\nst_crs(listings_sf)\n\nCoordinate Reference System:\n  User input: EPSG:3414 \n  wkt:\nPROJCRS[\"SVY21 / Singapore TM\",\n    BASEGEOGCRS[\"SVY21\",\n        DATUM[\"SVY21\",\n            ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n                LENGTHUNIT[\"metre\",1]]],\n        PRIMEM[\"Greenwich\",0,\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        ID[\"EPSG\",4757]],\n    CONVERSION[\"Singapore Transverse Mercator\",\n        METHOD[\"Transverse Mercator\",\n            ID[\"EPSG\",9807]],\n        PARAMETER[\"Latitude of natural origin\",1.36666666666667,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8801]],\n        PARAMETER[\"Longitude of natural origin\",103.833333333333,\n            ANGLEUNIT[\"degree\",0.0174532925199433],\n            ID[\"EPSG\",8802]],\n        PARAMETER[\"Scale factor at natural origin\",1,\n            SCALEUNIT[\"unity\",1],\n            ID[\"EPSG\",8805]],\n        PARAMETER[\"False easting\",28001.642,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8806]],\n        PARAMETER[\"False northing\",38744.572,\n            LENGTHUNIT[\"metre\",1],\n            ID[\"EPSG\",8807]]],\n    CS[Cartesian,2],\n        AXIS[\"northing (N)\",north,\n            ORDER[1],\n            LENGTHUNIT[\"metre\",1]],\n        AXIS[\"easting (E)\",east,\n            ORDER[2],\n            LENGTHUNIT[\"metre\",1]],\n    USAGE[\n        SCOPE[\"Cadastre, engineering survey, topographic mapping.\"],\n        AREA[\"Singapore - onshore and offshore.\"],\n        BBOX[1.13,103.59,1.47,104.07]],\n    ID[\"EPSG\",3414]]"
  },
  {
    "objectID": "posts/Geo/Geospatial_Datawrangling/index.html#joining-sf-and-tibble-dataframe",
    "href": "posts/Geo/Geospatial_Datawrangling/index.html#joining-sf-and-tibble-dataframe",
    "title": "Data Wrangling of Geospatial Data",
    "section": "Joining sf and tibble dataframe",
    "text": "Joining sf and tibble dataframe\nOne way to manipulate a dataframe is to combine two different sets of data frame together to combine the information retrieved. We will now aggregate the room price of the apartment based on the planning area.\n\n\nmutate: Adds new variables and preserves existing ones. If the new column is referencing an exisiting column, it will replace the variable. Since all the planning area are in uppercase in the mpsz data frame, we will use toupper to convert all the variables inside neighbourhood to uppercase.\n\nfilter: To remove irrelevant rows that are not required for the join.\n\nrename: Rename the column. I will be changing the neighbourhood to PLN_AREA_N to allow both data frame to identify the keys for the join.\n\nsummarise: After grouping the variables through the group_by function, we will summarise it to one row with the average price using the mean function.\n\n\nlistings_tidy <- listings %>%\n  mutate (neighbourhood = toupper(neighbourhood)) %>%\n  filter ((neighbourhood %in% unique(mpsz$PLN_AREA_N))) %>%\n  rename(\"PLN_AREA_N\" = \"neighbourhood\") %>%\n  group_by(PLN_AREA_N) %>%\n  summarise (avgprice = mean(price)) %>%\n  ungroup()\n\n\nmpsz3414 <- mpsz3414 %>%\n  left_join(listings_tidy)"
  },
  {
    "objectID": "posts/Geo/Geospatial_Datawrangling/index.html#geoprocessing-with-sf-package",
    "href": "posts/Geo/Geospatial_Datawrangling/index.html#geoprocessing-with-sf-package",
    "title": "Data Wrangling of Geospatial Data",
    "section": "Geoprocessing with sf package",
    "text": "Geoprocessing with sf package\nBuffering\nIn some cases, there is a need to create a buffering zone along the linestring object. An example would be to expand 5m along a road and understanding the total area increased through the expansion. One way we can do this is to use the st_buffer function that computes a buffer around this geometry/each geometry. To find out the overall area, st_area will be used. If the coordinates are in degrees longtitude/latitude, st_geod_area is used for area calculation.\n\nbuffer_cycling <- st_buffer(cyclingpath, \n                               dist=5, nQuadSegs = 30)\n\nbuffer_cycling$AREA <- st_area(buffer_cycling)\n\nsum(buffer_cycling$AREA)\n\n773143.9 [m^2]\n\n\nVisualising of buffering\nFrom the below visualisation, we are able to better understand how the buffer distance is being calculated and the different endCapStyle to be use for the buffer.\n\ncyclingpath_buffer <- cyclingpath[1,] %>%\n  select (-CYL_PATH_1)\n\nop = par(mfrow=c(2,3))\nplot(st_buffer(cyclingpath_buffer, dist = 1, endCapStyle=\"ROUND\"), reset = FALSE, main = \"endCapStyle: ROUND, distance 1\")\nplot(cyclingpath_buffer,col='blue',add=TRUE)\nplot(st_buffer(cyclingpath_buffer, dist = 2, endCapStyle=\"FLAT\"), reset = FALSE, main = \"endCapStyle: FLAT, distance 2\")\nplot(cyclingpath_buffer,col='blue',add=TRUE)\nplot(st_buffer(cyclingpath_buffer, dist = 3, endCapStyle=\"SQUARE\"), reset = FALSE, main = \"endCapStyle: SQUARE, distance 3\")\nplot(cyclingpath_buffer,col='blue',add=TRUE)\n\n\n\n\n\nmpsz3414$`PreSch Count`<- lengths(st_intersects(mpsz3414, preschool3414))\n\nsummary(mpsz3414$`PreSch Count`)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  0.000   0.000   2.000   4.207   6.000  37.000 \n\ntop_n(mpsz3414, 1, `PreSch Count`)\n\nSimple feature collection with 1 feature and 17 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 23449.05 ymin: 46001.23 xmax: 25594.22 ymax: 47996.47\nProjected CRS: SVY21 / Singapore TM\n  OBJECTID SUBZONE_NO      SUBZONE_N SUBZONE_C CA_IND PLN_AREA_N PLN_AREA_C\n1      290          3 WOODLANDS EAST    WDSZ03      N  WOODLANDS         WD\n      REGION_N REGION_C          INC_CRC FMEL_UPD_D   X_ADDR   Y_ADDR\n1 NORTH REGION       NR C90769E43EE6B0F2 2014-12-05 24506.64 46991.63\n  SHAPE_Leng SHAPE_Area avgprice                       geometry PreSch Count\n1   6603.608    2553464 74.53191 MULTIPOLYGON (((24786.75 46...           37"
  },
  {
    "objectID": "posts/Geo/Geospatial_Datawrangling/index.html#plotting-of-geographical-data",
    "href": "posts/Geo/Geospatial_Datawrangling/index.html#plotting-of-geographical-data",
    "title": "Data Wrangling of Geospatial Data",
    "section": "Plotting of geographical data",
    "text": "Plotting of geographical data\n\nmpsz3414$Area <- mpsz3414 %>%\n  st_area()\n\nmpsz3414 <- mpsz3414 %>%\n  mutate(`PreSch Density` = `PreSch Count`/Area * 1000000)\n\nhist(mpsz3414$`PreSch Density`)"
  },
  {
    "objectID": "posts/Geo/Geospatial_Datawrangling/index.html#visualising-of-geographical-data-using-ggplot2",
    "href": "posts/Geo/Geospatial_Datawrangling/index.html#visualising-of-geographical-data-using-ggplot2",
    "title": "Data Wrangling of Geospatial Data",
    "section": "Visualising of geographical data using ggplot2",
    "text": "Visualising of geographical data using ggplot2\n\nggplot(data=mpsz3414, \n       aes(x= as.numeric(`PreSch Density`)))+\n  geom_histogram(bins=20, \n                 color=\"black\", \n                 fill=\"light blue\") +\n  labs(title = \"Are pre-school even distributed in Singapore?\",\n       subtitle= \"There are many planning sub-zones with a single pre-school, on the other hand, \\nthere are two planning sub-zones with at least 20 pre-schools\",\n      x = \"Pre-school density (per km sq)\",\n      y = \"Frequency\")\n\n\n\n\n\nggplot(data=mpsz3414, \n       aes(x= as.numeric(`PreSch Density`), y = as.numeric(`PreSch Count`)))+\n  geom_point() +\n  labs(title = \"Are pre-school even distributed in Singapore?\",\n      x = \"Pre-school density (per km sq)\",\n      y = \"Pre-school Count\")"
  },
  {
    "objectID": "posts/Geo/Geospatial_Choropleth/index.html#conclusion",
    "href": "posts/Geo/Geospatial_Choropleth/index.html#conclusion",
    "title": "Introduction to Choropleth Mapping",
    "section": "Conclusion",
    "text": "Conclusion"
  },
  {
    "objectID": "posts/Geo/Geospatial_SpatialWeight/index.html",
    "href": "posts/Geo/Geospatial_SpatialWeight/index.html",
    "title": "Introduction to Spatial Weights and Application",
    "section": "",
    "text": "For this analysis, we will use the following packages from CRAN.\n\n\nsf - Support for simple features, a standardized way to encode spatial vector data. Binds to ‘GDAL’ for reading and writing data, to ‘GEOS’ for geometrical operations, and to ‘PROJ’ for projection conversions and datum transformations. Uses by default the ‘s2’ package for spherical geometry operations on ellipsoidal (long/lat) coordinates.\n\ntidyverse - Loading the core tidyverse packages which will be used for data wrangling and visualisation.\n\ntmap - Thematic maps are geographical maps in which spatial data distributions are visualized. This package offers a flexible, layer-based, and easy to use approach to create thematic maps, such as choropleths and bubble maps.\n\nspdep - A collection of functions to create spatial weights matrix objects from polygon ‘contiguities’, from point patterns by distance and tessellations, for summarizing these objects, and for permitting their use in spatial data analysis, including regional aggregation by minimum spanning tree; a collection of tests for spatial ‘autocorrelation’\n\n\npacman::p_load(sf, spdep, tmap, tidyverse)"
  },
  {
    "objectID": "posts/Geo/Geospatial_SpatialWeight/index.html#data-preparation",
    "href": "posts/Geo/Geospatial_SpatialWeight/index.html#data-preparation",
    "title": "Introduction to Spatial Weights and Application",
    "section": "Data Preparation",
    "text": "Data Preparation\nTwo dataset will be used for this study:\n\nHunan.shp: A shapefile of the Hunan Province that consist of all the capital\nHunan.csv: A csv file containing multiple attributes of each capital within Hunan\n\nImporting of data\nWe will use the st_read to import the shape file and read_csv to import the aspatial data into the R environment. We will then use a relational join left_join to combine the spatial and aspatial data together.\n\nhunan <- st_read(dsn = \"data/geospatial\", \n                 layer = \"Hunan\")\n\nhunan2012 <- read_csv(\"data/aspatial/Hunan_2012.csv\")\n\n\nhunan <- left_join(hunan,hunan2012)"
  },
  {
    "objectID": "posts/Geo/Geospatial_SpatialWeight/index.html#visualisation-of-spatial-data",
    "href": "posts/Geo/Geospatial_SpatialWeight/index.html#visualisation-of-spatial-data",
    "title": "Introduction to Spatial Weights and Application",
    "section": "Visualisation of spatial data",
    "text": "Visualisation of spatial data\nFor the visualisation, we will only be using tmap. We will prepare a basemap anbd a choropleth map to visualise the distribution of GDP per capita among the capital.\n\nbasemap <- tm_shape(hunan) +\n  tm_polygons() +\n  tm_text(\"NAME_3\", size=0.4)\n\ngdppc <- qtm(hunan, \"GDPPC\")\n\ntmap_arrange(basemap, gdppc, asp=1, ncol=2)\n\n\n\n\n\nbase map and choropleth map"
  },
  {
    "objectID": "posts/Geo/Geospatial_SpatialWeight/index.html#computing-contiguity-spatial-weights",
    "href": "posts/Geo/Geospatial_SpatialWeight/index.html#computing-contiguity-spatial-weights",
    "title": "Introduction to Spatial Weights and Application",
    "section": "Computing Contiguity Spatial Weights",
    "text": "Computing Contiguity Spatial Weights\nContiguity means that two spatial units share a common border of non-zero length. There are multiple criterion of contiguity such as:\n\n\nRook: When only common sides of the polygons are considered to define the neighbor relation (common vertices are ignored).\n\nQueen: The difference between the rook and queen criterion to determine neighbors is that the latter also includes common vertices.\n\nBishop: Is based on the existence of common vertices between two spatial units.\n\n\n\n\n\nContiguity Weights\n\n\n\n\nExcept in the simplest of circumstances, visual examination or manual calculation cannot be used to create the spatial weights from the geometry of the data. It is necessary to utilize explicit spatial data structures to deal with the placement and layout of the polygons in order to determine whether two polygons are contiguous.\nWe will use the poly2nb function to construct neighbours list based on the regions with contiguous boundaries. Based on the documentation, user will be able to pass a queen argument that takes in True or False. The argument the default is set to TRUE, that is, if you don’t specify queen = FALSE this function will return a list of first order neighbours using the Queen criteria.\nComputing (QUEEN) contiguity based neighbour\nThe code chunk below is used to compute Queen contiguity weight matrix.\n\nwm_q <- poly2nb(hunan)\nsummary(wm_q)\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \nLink number distribution:\n\n 1  2  3  4  5  6  7  8  9 11 \n 2  2 12 16 24 14 11  4  2  1 \n2 least connected regions:\n30 65 with 1 link\n1 most connected region:\n85 with 11 links\n\n\nbased on the summary report above,the report shows that there are 88 area units in Hunan. The most connected area unit has 11 neighbours. There are two area units with only one neighbours.\nFor each polygon in our polygon object, wm_q lists all neighboring polygons. For example, to see the neighbors for the first polygon in the object, type:\n\nwm_q[[1]]\n\n[1]  2  3  4 57 85\n\n\nPolygon 1 has 5 neighbors. The numbers represent the polygon IDs as stored in hunan SpatialPolygonsDataFrame class.\nTo reveal the county names of the five neighboring polygons, the code chunk will be used:\n\nhunan$NAME_3[c(2,3,4,57,85)]\n\n[1] \"Hanshou\" \"Jinshi\"  \"Li\"      \"Nan\"     \"Taoyuan\"\n\n\nWe can retrieve the GDPPC of these five countries by using the code chunk below.\n\nhunan$GDPPC[wm_q[[1]]]\n\n[1] 20981 34592 24473 21311 22879\n\n\nThe printed output above shows that the GDPPC of the five nearest neighbours based on Queen’s method are 20981, 34592, 24473, 21311 and 22879 respectively.\nYou can display the complete weight matrix by using str().\n\nstr(wm_q)\n\nList of 88\n $ : int [1:5] 2 3 4 57 85\n $ : int [1:5] 1 57 58 78 85\n $ : int [1:4] 1 4 5 85\n $ : int [1:4] 1 3 5 6\n $ : int [1:4] 3 4 6 85\n $ : int [1:5] 4 5 69 75 85\n $ : int [1:4] 67 71 74 84\n $ : int [1:7] 9 46 47 56 78 80 86\n $ : int [1:6] 8 66 68 78 84 86\n $ : int [1:8] 16 17 19 20 22 70 72 73\n $ : int [1:3] 14 17 72\n $ : int [1:5] 13 60 61 63 83\n $ : int [1:4] 12 15 60 83\n $ : int [1:3] 11 15 17\n $ : int [1:4] 13 14 17 83\n $ : int [1:5] 10 17 22 72 83\n $ : int [1:7] 10 11 14 15 16 72 83\n $ : int [1:5] 20 22 23 77 83\n $ : int [1:6] 10 20 21 73 74 86\n $ : int [1:7] 10 18 19 21 22 23 82\n $ : int [1:5] 19 20 35 82 86\n $ : int [1:5] 10 16 18 20 83\n $ : int [1:7] 18 20 38 41 77 79 82\n $ : int [1:5] 25 28 31 32 54\n $ : int [1:5] 24 28 31 33 81\n $ : int [1:4] 27 33 42 81\n $ : int [1:3] 26 29 42\n $ : int [1:5] 24 25 33 49 54\n $ : int [1:3] 27 37 42\n $ : int 33\n $ : int [1:8] 24 25 32 36 39 40 56 81\n $ : int [1:8] 24 31 50 54 55 56 75 85\n $ : int [1:5] 25 26 28 30 81\n $ : int [1:3] 36 45 80\n $ : int [1:6] 21 41 47 80 82 86\n $ : int [1:6] 31 34 40 45 56 80\n $ : int [1:4] 29 42 43 44\n $ : int [1:4] 23 44 77 79\n $ : int [1:5] 31 40 42 43 81\n $ : int [1:6] 31 36 39 43 45 79\n $ : int [1:6] 23 35 45 79 80 82\n $ : int [1:7] 26 27 29 37 39 43 81\n $ : int [1:6] 37 39 40 42 44 79\n $ : int [1:4] 37 38 43 79\n $ : int [1:6] 34 36 40 41 79 80\n $ : int [1:3] 8 47 86\n $ : int [1:5] 8 35 46 80 86\n $ : int [1:5] 50 51 52 53 55\n $ : int [1:4] 28 51 52 54\n $ : int [1:5] 32 48 52 54 55\n $ : int [1:3] 48 49 52\n $ : int [1:5] 48 49 50 51 54\n $ : int [1:3] 48 55 75\n $ : int [1:6] 24 28 32 49 50 52\n $ : int [1:5] 32 48 50 53 75\n $ : int [1:7] 8 31 32 36 78 80 85\n $ : int [1:6] 1 2 58 64 76 85\n $ : int [1:5] 2 57 68 76 78\n $ : int [1:4] 60 61 87 88\n $ : int [1:4] 12 13 59 61\n $ : int [1:7] 12 59 60 62 63 77 87\n $ : int [1:3] 61 77 87\n $ : int [1:4] 12 61 77 83\n $ : int [1:2] 57 76\n $ : int 76\n $ : int [1:5] 9 67 68 76 84\n $ : int [1:4] 7 66 76 84\n $ : int [1:5] 9 58 66 76 78\n $ : int [1:3] 6 75 85\n $ : int [1:3] 10 72 73\n $ : int [1:3] 7 73 74\n $ : int [1:5] 10 11 16 17 70\n $ : int [1:5] 10 19 70 71 74\n $ : int [1:6] 7 19 71 73 84 86\n $ : int [1:6] 6 32 53 55 69 85\n $ : int [1:7] 57 58 64 65 66 67 68\n $ : int [1:7] 18 23 38 61 62 63 83\n $ : int [1:7] 2 8 9 56 58 68 85\n $ : int [1:7] 23 38 40 41 43 44 45\n $ : int [1:8] 8 34 35 36 41 45 47 56\n $ : int [1:6] 25 26 31 33 39 42\n $ : int [1:5] 20 21 23 35 41\n $ : int [1:9] 12 13 15 16 17 18 22 63 77\n $ : int [1:6] 7 9 66 67 74 86\n $ : int [1:11] 1 2 3 5 6 32 56 57 69 75 ...\n $ : int [1:9] 8 9 19 21 35 46 47 74 84\n $ : int [1:4] 59 61 62 88\n $ : int [1:2] 59 87\n - attr(*, \"class\")= chr \"nb\"\n - attr(*, \"region.id\")= chr [1:88] \"1\" \"2\" \"3\" \"4\" ...\n - attr(*, \"call\")= language poly2nb(pl = hunan)\n - attr(*, \"type\")= chr \"queen\"\n - attr(*, \"sym\")= logi TRUE\n\n\nComputing (ROOK) contiguity based neighbour\nThe code chunk below is used to compute Rook contiguity weight matrix.\n\nwm_r <- poly2nb(hunan, queen=FALSE)\nsummary(wm_r)\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 440 \nPercentage nonzero weights: 5.681818 \nAverage number of links: 5 \nLink number distribution:\n\n 1  2  3  4  5  6  7  8  9 10 \n 2  2 12 20 21 14 11  3  2  1 \n2 least connected regions:\n30 65 with 1 link\n1 most connected region:\n85 with 10 links\n\n\nThe summary report above shows that there are 88 area units in Hunan. The most connect area unit has 10 neighbours. There are two area units with only one heighbours.\nVisualising the weights matrix\nA connectivity graph takes a point and displays a line to each neighboring point. We are working with polygons at the moment, so we will need to get points in order to make our connectivity graphs. The most typically method for this will be polygon centroids. To retrieve the centroid of each area, we will use the st_centroid function.\n\ncoords <- st_centroid(st_geometry(hunan))\n\nPlotting Queen and Rook contiguity based neighbours map\n\npar(mfrow=c(1,2))\n\nplot(st_geometry(hunan), border=\"grey\", main = \"Queen Contiguity\")\nplot(wm_q, coords,pch = 19, cex = 0.6, add = TRUE, col= \"red\")\n\nplot(st_geometry(hunan), border=\"grey\", main = \"Rook Contiguity\")\nplot(wm_r, coords,pch = 19, cex = 0.6, add = TRUE, col= \"blue\")"
  },
  {
    "objectID": "posts/Geo/Geospatial_SpatialWeight/index.html#computing-distance-based-neighbours",
    "href": "posts/Geo/Geospatial_SpatialWeight/index.html#computing-distance-based-neighbours",
    "title": "Introduction to Spatial Weights and Application",
    "section": "Computing distance based neighbours",
    "text": "Computing distance based neighbours\nIn this section, you will learn how to derive distance-based weight matrices by using dnearneigh() of spdep package. The function identifies neighbours of region points by Euclidean distance in the metric of the points between lower (greater than or equal to and upper (less than or equal to) bounds.\nDetermine the cut-off distance\nFirstly, we need to determine the upper limit for distance band by using the steps below:\n\nReturn a matrix with the indices of points belonging to the set of the k nearest neighbours of each other by using knearneigh() of spdep.\nConvert the knn object returned by knearneigh() into a neighbours list of class nb with a list of integer vectors containing neighbour region number ids by using knn2nb().\nReturn the length of neighbour relationship edges by using nbdists() of spdep. The function returns in the units of the coordinates if the coordinates are projected, in km otherwise.\nRemove the list structure of the returned object by using unlist().\n\n\nk1 <- knn2nb(knearneigh(coords))\nk1dists <- unlist(nbdists(k1, coords))\nsummary(k1dists)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  24.80   32.53   38.06   38.91   44.66   58.25 \n\n\nThe summary report shows that the largest first nearest neighbour distance is 58.25 km, so using this as the upper threshold gives certainty that all units will have at least one neighbour.\nComputing fixed distance weight matrix\nNow, we will compute the distance weight matrix by using dnearneigh() as shown in the code chunk below.\n\nwm_d59 <- dnearneigh(coords, 0, 59)\nwm_d59\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 298 \nPercentage nonzero weights: 3.84814 \nAverage number of links: 3.386364 \n\n\nThe report shows that on average, every area should have at least 3 neighbours (links).\nTo display the structure of the weight matrix is to combine table() and card() of spdep.\n\nhead(table(hunan$County, card(wm_d59)),10)\n\n           \n            1 2 3 4 5 6\n  Anhua     1 0 0 0 0 0\n  Anren     0 0 0 1 0 0\n  Anxiang   0 0 0 0 1 0\n  Baojing   0 0 0 1 0 0\n  Chaling   0 0 1 0 0 0\n  Changning 0 0 1 0 0 0\n  Changsha  0 0 0 1 0 0\n  Chengbu   1 0 0 0 0 0\n  Chenxi    0 0 0 1 0 0\n  Cili      1 0 0 0 0 0\n\n\nVisualising distance weight matrix\nThe left graph with the red lines show the links of 1st nearest neighbours and the right graph with the black lines show the links of neighbours within the cut-off distance of 59km.\n\npar(mfrow=c(1,2))\nplot(hunan$geometry, border=\"lightgrey\", main=\"1st nearest neighbours\")\nplot(k1, coords, add=TRUE, col=\"red\", length=0.08)\nplot(hunan$geometry, border=\"lightgrey\", main=\"Distance link\")\nplot(wm_d59, coords, add=TRUE, pch = 19, cex = 0.6)\n\n\n\n\nAdaptive distance weight matrix\nOther than using distance as a criteria to decide the neighbours, it is possible to control the numbers of neighbours directly using k-nearest neighbours, either accepting asymmetric neighbours or imposing symmetry as shown in the code chunk below:\n\nknn6 <- knn2nb(knearneigh(coords, k=6)) #k refers to the number of neighbours per area\nknn6\n\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 528 \nPercentage nonzero weights: 6.818182 \nAverage number of links: 6 \nNon-symmetric neighbours list\n\n\nPlotting adaptive distance weight\nWe can plot the adaptive distance weight matrix using the code chunk below:\n\nplot(hunan$geometry, border=\"lightgrey\", main = \"Adaptive Distance Weight\")\nplot(knn6, coords, pch = 19, cex = 0.6, add = TRUE, col = \"red\")\n\n\n\n\nWeights based on Inversed Distance Weighting (IDW)\nIn this section, you will learn how to derive a spatial weight matrix based on Inversed Distance method.\nFirst, we will compute the distances between areas by using nbdists() of spdep.\nWe will use the [lapply()] (https://www.rdocumentation.org/packages/base/versions/3.6.2/topics/lapply) to apply the inverse function through the list.\n\ndist <- nbdists(wm_q, coords, longlat = TRUE)\nids <- lapply(dist, function(x) 1/(x))\nhead(ids,5)\n\n[[1]]\n[1] 1.694446 3.805533 1.847048 2.867007 1.166097\n\n[[2]]\n[1] 1.694446 1.832614 1.889267 2.537233 1.681209\n\n[[3]]\n[1] 3.805533 3.007305 3.588446 1.468997\n\n[[4]]\n[1] 1.847048 3.007305 3.731278 1.490622\n\n[[5]]\n[1] 3.588446 3.731278 1.526472 1.775459\n\n\nNext, we will use the nb2listw to apply the weights list with values given by the coding scheme style chosen. There are multiple style to choose from:\n\nB (Basic Binary Coding)\nW (Row Standardised) - sums over all links to n\nC (Globally Standardised) - sums over all links to n\nU (Globally Standardised / No of neighbours) - sums over all links to unity\nS (Variance-Stabilizing Coding Scheme) - sums over all links to n\nminmax - divides the weights by the minimum of the maximum row sums and maximum column sums of the input weights\n\nFor the simplifed analysis, we will use the W (Row Standardised).\n\nrswm_q <- nb2listw(wm_q, style=\"W\", zero.policy = TRUE)\nrswm_q\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \n\nWeights style: W \nWeights constants summary:\n   n   nn S0       S1       S2\nW 88 7744 88 37.86334 365.9147\n\n\nFrom the earlier example, we know that the first Id has 5 neighbours. We take a look at the weight distribution of these 5 neighours. Since we are using Row Standardised, they should be equal.\n\nrswm_q$weights[1]\n\n[[1]]\n[1] 0.2 0.2 0.2 0.2 0.2\n\n\nEach neighbor is assigned a 0.2 of the total weight. This means that when R computes the average neighboring income values, each neighbor’s income will be multiplied by 0.2 before being tallied."
  },
  {
    "objectID": "posts/Geo/Geospatial_SpatialWeight/index.html#application-of-spatial-weight-matrix",
    "href": "posts/Geo/Geospatial_SpatialWeight/index.html#application-of-spatial-weight-matrix",
    "title": "Introduction to Spatial Weights and Application",
    "section": "Application of Spatial Weight Matrix",
    "text": "Application of Spatial Weight Matrix\nIn this section, you will learn how to create four different spatial lagged variables, they are:\n\nspatial lag with row-standardized weights\nspatial lag as a sum of neighbouring values\nspatial window average\nspatial window sum\n\nSpatial lag with row-standardized weights\nFirstly, we’ll compute the average neighbor GDPPC value for each polygon using the lag.listw() that can compute the lag of a vector. These values are often referred to as spatially lagged values.\n\nGDPPC.lag <- lag.listw(rswm_q, hunan$GDPPC)\nhead(GDPPC.lag)\n\n[1] 24847.20 22724.80 24143.25 27737.50 27270.25 21248.80\n\n\nIn the previous section, we retrieved the GDPPC of the neighbours of the first area by using the following code chunk:\n\nhunan$GDPPC[wm_q[[1]]]\n\n[1] 20981 34592 24473 21311 22879\n\n\nFrom this, we can understand that the spatial lag with row-standardized weights is actually the average GDPPC of its neighbours.\n\\[(20981+34592+24473+21311+22879/5 = 24847.20)\\]\nWe will now append these lagged values to our Hunan data frame.\n\nlag.df <- as.data.frame(list(hunan$NAME_3,GDPPC.lag))\ncolnames(lag.df) <- c(\"NAME_3\", \"lag GDPPC\")\nhunan <- left_join(hunan,lag.df)\n\nNext, we will plot both the GDPPC and spatial lag GDPPC for comparison using the code chunk below.\n\ngdppc <- tm_shape(hunan) +\n  tm_fill(\"GDPPC\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"GDPPC without lagged values\", main.title.size = 0.7, legend.text.size = 0.4,\n            main.title.fontface = \"bold\",main.title.position = \"center\")\n  \nlag_gdppc <- tm_shape(hunan) +\n  tm_fill(\"lag GDPPC\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"GDPPC with lagged values\", main.title.size = 0.7, legend.text.size = 0.4,\n            main.title.fontface = \"bold\",main.title.position = \"center\")\ntmap_arrange(gdppc, lag_gdppc, asp=1, ncol=2)\n\n\n\n\n\nGDPPC vs lag GDPPC\n\n\n\n\nSpatial lag as a sum of neighboring values\nWe can calculate spatial lag as a sum of neighboring values by assigning binary weights. This requires us to go back to our neighbors list, then apply a function that will assign binary weights, then we use glist = in the nb2listw function to explicitly assign these weights.\nWe start by applying a function that will assign a value of 1 per each neighbor. This is done with lapply, which we have been using to manipulate the neighbors structure throughout the past notebooks. Basically it applies a function across each value in the neighbors structure.\n\nb_weights <- lapply(wm_q, function(x) 0*x + 1)\n\nb_weights2 <- nb2listw(wm_q, \n                       glist = b_weights, \n                       style = \"B\")\nb_weights2\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \n\nWeights style: B \nWeights constants summary:\n   n   nn  S0  S1    S2\nB 88 7744 448 896 10224\n\n\nWith the proper weights assigned, we can use lag.listw to compute a lag variable from our weight and GDPPC.\n\nlag_df <- as.data.frame (list(hunan$NAME_3, lag.listw(b_weights2, hunan$GDPPC)))\ncolnames(lag_df) <- c(\"NAME_3\", \"lag_sum GDPPC\")\nhunan <- left_join(hunan, lag_df)\n\nlag_df\n\n          NAME_3 lag_sum GDPPC\n1        Anxiang        124236\n2        Hanshou        113624\n3         Jinshi         96573\n4             Li        110950\n5          Linli        109081\n6         Shimen        106244\n7        Liuyang        174988\n8      Ningxiang        235079\n9      Wangcheng        273907\n10         Anren        256221\n11       Guidong         98013\n12         Jiahe        104050\n13         Linwu        102846\n14       Rucheng         92017\n15       Yizhang        133831\n16      Yongxing        158446\n17        Zixing        141883\n18     Changning        119508\n19      Hengdong        150757\n20       Hengnan        153324\n21      Hengshan        113593\n22       Leiyang        129594\n23        Qidong        142149\n24        Chenxi        100119\n25     Zhongfang         82884\n26       Huitong         74668\n27      Jingzhou         43184\n28        Mayang         99244\n29       Tongdao         46549\n30      Xinhuang         20518\n31          Xupu        140576\n32      Yuanling        121601\n33      Zhijiang         92069\n34 Lengshuijiang         43258\n35    Shuangfeng        144567\n36        Xinhua        132119\n37       Chengbu         51694\n38        Dongan         59024\n39       Dongkou         69349\n40       Longhui         73780\n41      Shaodong         94651\n42       Suining        100680\n43        Wugang         69398\n44       Xinning         52798\n45       Xinshao        140472\n46      Shaoshan        118623\n47    Xiangxiang        180933\n48       Baojing         82798\n49     Fenghuang         83090\n50       Guzhang         97356\n51       Huayuan         59482\n52        Jishou         77334\n53      Longshan         38777\n54          Luxi        111463\n55      Yongshun         74715\n56         Anhua        174391\n57           Nan        150558\n58     Yuanjiang        122144\n59      Jianghua         68012\n60       Lanshan         84575\n61      Ningyuan        143045\n62     Shuangpai         51394\n63       Xintian         98279\n64       Huarong         47671\n65      Linxiang         26360\n66         Miluo        236917\n67     Pingjiang        220631\n68      Xiangyin        185290\n69          Cili         64640\n70       Chaling         70046\n71        Liling        126971\n72       Yanling        144693\n73           You        129404\n74       Zhuzhou        284074\n75       Sangzhi        112268\n76       Yueyang        203611\n77        Qiyang        145238\n78      Taojiang        251536\n79      Shaoyang        108078\n80      Lianyuan        238300\n81     Hongjiang        108870\n82      Hengyang        108085\n83       Guiyang        262835\n84      Changsha        248182\n85       Taoyuan        244850\n86      Xiangtan        404456\n87           Dao         67608\n88     Jiangyong         33860\n\n\nFrom the above data table and the GDPPC from the previous section, we know that the lagged sum is the addition of all the GDPPC of its neighbours.\n\\[(20981+34592+24473+21311+22879 = 124236)\\]\n\ngdppc <- tm_shape(hunan) +\n  tm_fill(\"GDPPC\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"GDPPC without lagged values\", main.title.size = 0.7, legend.text.size = 0.4,\n            main.title.fontface = \"bold\",main.title.position = \"center\")\n  \nlag_gdppc <- tm_shape(hunan) +\n  tm_fill(\"lag GDPPC\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"GDPPC with lagged values\", main.title.size = 0.7, legend.text.size = 0.4,\n            main.title.fontface = \"bold\",main.title.position = \"center\")\n\nlag_sum_gdppc <- tm_shape(hunan) +\n  tm_fill(\"lag_sum GDPPC\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"GDPPC with lagged sum values\", main.title.size = 0.7, legend.text.size = 0.4,\n            main.title.fontface = \"bold\",main.title.position = \"center\")\n\ntmap_arrange(gdppc, lag_gdppc, lag_sum_gdppc, asp=1, ncol=3)\n\n\n\n\n\nGDPPC vs lag GDPPC vs lag sum GDPPC\n\n\n\n\nSpatial Window Average\nThe spatial window average uses row-standardized weights and includes the diagonal element. (region itself) We will use the include.self().\n\nwm_q_self <- include.self(wm_q)\n\nWe will now obtain the weight and retrieve the new spatial window average and combine it with our exisiting Hunan dataframe.\n\nwm_q_self_list <- nb2listw(wm_q_self)\nlag_w_avg_gpdpc <- lag.listw(wm_q_self_list, \n                             hunan$GDPPC)\n\nlag_w_avg_df <- as.data.frame(list(hunan$NAME_3, lag_w_avg_gpdpc))\n\ncolnames(lag_w_avg_df) <- c(\"NAME_3\", \"lag_window_avg GDPPC\")\n\nhunan <- left_join(hunan, lag_w_avg_df)\n\n\ngdppc <- tm_shape(hunan) +\n  tm_fill(\"GDPPC\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"GDPPC without lagged values\", main.title.size = 0.7, legend.text.size = 0.4,\n            main.title.fontface = \"bold\",main.title.position = \"center\")\n  \nlag_gdppc <- tm_shape(hunan) +\n  tm_fill(\"lag GDPPC\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"GDPPC with lagged values\", main.title.size = 0.7, legend.text.size = 0.4,\n            main.title.fontface = \"bold\",main.title.position = \"center\")\n\nlag_sum_gdppc <- tm_shape(hunan) +\n  tm_fill(\"lag_sum GDPPC\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"GDPPC with lagged sum values\", main.title.size = 0.7, legend.text.size = 0.4,\n            main.title.fontface = \"bold\",main.title.position = \"center\")\n\nlag_sum_avg_gdppc <- tm_shape(hunan) +\n  tm_fill(\"lag_window_avg GDPPC\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"GDPPC with lagged average values\", main.title.size = 0.7, legend.text.size = 0.4,\n            main.title.fontface = \"bold\",main.title.position = \"center\")\n\ntmap_arrange(gdppc, lag_gdppc, lag_sum_gdppc, lag_sum_avg_gdppc, asp=1, ncol=2)\n\n\n\n\n\nGDPPC vs lag GDPPC vs lag sum GDPPC vs lag avg GDPPC\n\n\n\n\nSpatial Window Sum\nThe spatial Window sum is similar to the window average but using the binary weights. Therefore we will repeat the following steps of the Spatial lag as a sum of neighboring values and to include its own region.\n\nb_weights <- lapply(wm_q_self, function(x) 0*x + 1)\nb_weights[1]\n\n[[1]]\n[1] 1 1 1 1 1 1\n\n\nFrom the result, we can see now the first area instead of 5 neighbours, it has 6 neighbours which include itself. We will now retrieve the spatial window sum and combine it with our exisiting Hunan dataframe.\n\nb_weights2 <- nb2listw(wm_q_self, \n                       glist = b_weights, \n                       style = \"B\")\nw_sum_gdppc_df <- as.data.frame(list(hunan$NAME_3, lag.listw(b_weights2, hunan$GDPPC)))\ncolnames(w_sum_gdppc_df) <- c(\"NAME_3\", \"w_sum GDPPC\")\n\nhunan <- left_join(hunan, w_sum_gdppc_df)\n\nWe will now visualise all the plots we created and visualise the difference in each method (excluding the original GDPPC).\n\nlag_gdppc <- tm_shape(hunan) +\n  tm_fill(\"lag GDPPC\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"GDPPC with lagged values\", main.title.size = 0.7, legend.text.size = 0.4,\n            main.title.fontface = \"bold\",main.title.position = \"center\")\n\nlag_sum_gdppc <- tm_shape(hunan) +\n  tm_fill(\"lag_sum GDPPC\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"GDPPC with lagged sum values\", main.title.size = 0.7, legend.text.size = 0.4,\n            main.title.fontface = \"bold\",main.title.position = \"center\")\n\nlag_sum_avg_gdppc <- tm_shape(hunan) +\n  tm_fill(\"lag_window_avg GDPPC\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"GDPPC with lagged average values\", main.title.size = 0.7, legend.text.size = 0.4,\n            main.title.fontface = \"bold\",main.title.position = \"center\")\n\nlag_sum_window_gdppc <- tm_shape(hunan) +\n  tm_fill(\"w_sum GDPPC\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"GDPPC with lagged sum average values\", main.title.size = 0.7, legend.text.size = 0.4,\n            main.title.fontface = \"bold\",main.title.position = \"center\")\n\ntmap_arrange(lag_gdppc, lag_sum_gdppc, lag_sum_avg_gdppc, lag_sum_window_gdppc, asp=1, ncol=2)\n\n\n\n\n\nlag GDPPC vs lag sum GDPPC vs lag avg GDPPC vs lag sum avg GDPPC"
  },
  {
    "objectID": "posts/Geo/Geospatial_SpatialWeight/index.html#conclusion",
    "href": "posts/Geo/Geospatial_SpatialWeight/index.html#conclusion",
    "title": "Introduction to Spatial Weights and Application",
    "section": "Conclusion",
    "text": "Conclusion\nThis study allow us to understand the different contiguity spatial weights and different methods to utilise the neighbours information. There is no best method but which method suits your analysis of your work."
  },
  {
    "objectID": "posts/Geo/Geospatial_Autocorrelation/index.html",
    "href": "posts/Geo/Geospatial_Autocorrelation/index.html",
    "title": "Introduction to Global and Local Measures of Spatial Autocorrelation",
    "section": "",
    "text": "The Spatial Autocorrelation measures spatial autocorrelation based on feature locations and feature values simultaneously. Given a set of features and an associated attribute, it evaluates whether the expressed pattern is clustered, scattered, or random. The tool calculates the Moran’s I index value as well as a z-score and p-value to assess the significance of this index. P-values are numerical approximations of the area under the curve for a known distribution, bounded by the test statistic.\nIn this study we will explore the computation of Global and Local Measure of Spatial Autocorrelation (GLSA) by using spdep package."
  },
  {
    "objectID": "posts/Geo/Geospatial_Autocorrelation/index.html#libraries",
    "href": "posts/Geo/Geospatial_Autocorrelation/index.html#libraries",
    "title": "Introduction to Global and Local Measures of Spatial Autocorrelation",
    "section": "Libraries",
    "text": "Libraries\nFor this study, we will use the following packages from CRAN.\n\n\nsf - Support for simple features, a standardized way to encode spatial vector data. Binds to ‘GDAL’ for reading and writing data, to ‘GEOS’ for geometrical operations, and to ‘PROJ’ for projection conversions and datum transformations. Uses by default the ‘s2’ package for spherical geometry operations on ellipsoidal (long/lat) coordinates.\n\ntidyverse - Loading the core tidyverse packages which will be used for data wrangling and visualisation.\n\ntmap - Thematic maps are geographical maps in which spatial data distributions are visualized. This package offers a flexible, layer-based, and easy to use approach to create thematic maps, such as choropleths and bubble maps.\n\nspdep - A collection of functions to create spatial weights matrix objects from polygon ‘contiguities’, from point patterns by distance and tessellations, for summarizing these objects, and for permitting their use in spatial data analysis, including regional aggregation by minimum spanning tree; a collection of tests for spatial ‘autocorrelation’\n\n\npacman::p_load(sf, spdep, tmap, tidyverse)"
  },
  {
    "objectID": "posts/Geo/Geospatial_Autocorrelation/index.html#data-preparation",
    "href": "posts/Geo/Geospatial_Autocorrelation/index.html#data-preparation",
    "title": "Introduction to Global and Local Measures of Spatial Autocorrelation",
    "section": "Data Preparation",
    "text": "Data Preparation\nTwo dataset will be used for this study:\n\nHunan.shp: A shapefile of the Hunan Province that consist of all the capital\nHunan.csv: A csv file containing multiple attributes of each capital within Hunan\n\nImporting of data\nWe will use the st_read to import the shape file and read_csv to import the aspatial data into the R environment. We will then use a relational join left_join to combine the spatial and aspatial data together.\n\nhunan <- st_read(dsn = \"data/geospatial\", \n                 layer = \"Hunan\")\n\nhunan2012 <- read_csv(\"data/aspatial/Hunan_2012.csv\")\n\n\nhunan <- left_join(hunan,hunan2012)"
  },
  {
    "objectID": "posts/Geo/Geospatial_Autocorrelation/index.html#visualisation-of-spatial-data",
    "href": "posts/Geo/Geospatial_Autocorrelation/index.html#visualisation-of-spatial-data",
    "title": "Introduction to Global and Local Measures of Spatial Autocorrelation",
    "section": "Visualisation of spatial data",
    "text": "Visualisation of spatial data\nFor the visualisation, we will only be using tmap to show the distribution of GDPPC 2021.\n\ntm_shape(hunan) +\n  tm_fill(\"GDPPC\",\n          n = 5,\n          style = \"quantile\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"GDDPC using Quantile classification\", main.title.size = 0.7,\n            main.title.fontface = \"bold\", main.title.position = \"center\")"
  },
  {
    "objectID": "posts/Geo/Geospatial_Autocorrelation/index.html#global-spatial-autocorrelation",
    "href": "posts/Geo/Geospatial_Autocorrelation/index.html#global-spatial-autocorrelation",
    "title": "Introduction to Global and Local Measures of Spatial Autocorrelation",
    "section": "Global Spatial Autocorrelation",
    "text": "Global Spatial Autocorrelation\nBefore we commence with spatial autocorrelation, we need to construct the spatial weights of the study region. The spatial weights is used to define the neighbourhood relationships between the geographical units (i.e. county) in the study area. Refer to my previous post to understand the flow of construsting the spatial weight.\nComputing Contiguity Spatial Weights\nFor this study, we will be using the Queen contiguity weight matrix. The code chunk below will construct the weight matrix and subsequently implement the row-standardised weight matrix using the nb2listw() function.\n\nwm_q <- poly2nb(hunan, \n                queen=TRUE)\n\nrswm_q <- nb2listw(wm_q, \n                   style=\"W\", \n                   zero.policy = TRUE)\nrswm_q\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 88 \nNumber of nonzero links: 448 \nPercentage nonzero weights: 5.785124 \nAverage number of links: 5.090909 \n\nWeights style: W \nWeights constants summary:\n   n   nn S0       S1       S2\nW 88 7744 88 37.86334 365.9147\n\n\nGlobal Spatial Autocorrelation: Moran’s I\nWe will now perform Moran’s I statistics testing by using the moran.test() from spdep.\n\nmoran.test(hunan$GDPPC, \n           listw=rswm_q, \n           zero.policy = TRUE, \n           na.action=na.omit)\n\n\n    Moran I test under randomisation\n\ndata:  hunan$GDPPC  \nweights: rswm_q    \n\nMoran I statistic standard deviate = 4.7351, p-value = 1.095e-06\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n      0.300749970      -0.011494253       0.004348351 \n\n\nTo better understand the result, we will reference the following table.\n\n\n\n\n\n\n\nThe p-value is not statistically significant.\nYou cannot reject the null hypothesis. It is quite possible that the spatial distribution of feature values is the result of random spatial processes. The observed spatial pattern of feature values could very well be one of many, many possible versions of complete spatial randomness (CSR).\n\n\nThe p-value is statistically significant, and the z-score is positive.\nYou may reject the null hypothesis. The spatial distribution of high values and/or low values in the dataset is more spatially clustered than would be expected if underlying spatial processes were random.\n\n\nThe p-value is statistically significant, and the z-score is not positive.\nYou may reject the null hypothesis. The spatial distribution of high values and low values in the dataset is more spatially dispersed than would be expected if underlying spatial processes were random. A dispersed spatial pattern often reflects some type of competitive process—a feature with a high value repels other features with high values; similarly, a feature with a low value repels other features with low values.\n\n\n\nThe hypothesis:\nH0 : The attribute being analyzed is randomly distributed among the features in your study area.\nH1: The attribute being analyzed is not randomly distributed among the features in your study area.\nSince the above result has a p-value below 0.05 and a positive z-score, we can conclude with statistical evidence that the attribute a not randomly distributed and the spatial distribution of high values and/or low values in the dataset is more spatially clustered\nComputing Monte Carlo Moran’s I\nThe code chunk below performs permutation test for Moran’s I statistic by using moran.mc() of spdep. A total of 1000 simulation will be performed.\n\nset.seed(1234)\nbperm= moran.mc(hunan$GDPPC, \n                listw=rswm_q, \n                nsim=999, \n                zero.policy = TRUE, \n                na.action=na.omit)\nbperm\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  hunan$GDPPC \nweights: rswm_q  \nnumber of simulations + 1: 1000 \n\nstatistic = 0.30075, observed rank = 1000, p-value = 0.001\nalternative hypothesis: greater\n\n\nVisualising Monte Carlo Moran’s I\nWe will use a density plot to visualise the output of the Monte Carlo Moran’s I. First, we need to extract the res value and convert it into a dataframe. We then visualise the test statistic result using geom_density from the ggplot package.\n\nmonte_carlo <- as.data.frame(bperm[7])\n\nggplot(monte_carlo, aes(x=res)) + \n  geom_density(fill=\"lightblue\") +\n  geom_vline(aes(xintercept=0.30075),\n            color=\"blue\", linetype=\"dashed\", size=1) +\n  labs(title = \"Density plot of Monte Carlo Simulation of Moran's I\", x = \"Test Statistic\", y = \"Density\") +\n  theme_minimal() \n\n\n\n\nFrom the plot, we can see the actual Moran’s I statistic (blue line) is far outside the simulated data (shaded in blue), indicating a significant evidence of positive autocorrelation.\nGlobal Spatial Autocorrelation: Geary’s\nWe will now perform the Geary’c statistic testing by using the geary.test() function. We will also compute the Monte Carlo Geary’sc using the geary.mc.\n\ngeary.test(hunan$GDPPC, listw=rswm_q)\n\n\n    Geary C test under randomisation\n\ndata:  hunan$GDPPC \nweights: rswm_q \n\nGeary C statistic standard deviate = 3.6108, p-value = 0.0001526\nalternative hypothesis: Expectation greater than statistic\nsample estimates:\nGeary C statistic       Expectation          Variance \n        0.6907223         1.0000000         0.0073364 \n\nset.seed(1234)\ngperm=geary.mc(hunan$GDPPC, \n               listw=rswm_q, \n               nsim=999)\ngperm\n\n\n    Monte-Carlo simulation of Geary C\n\ndata:  hunan$GDPPC \nweights: rswm_q \nnumber of simulations + 1: 1000 \n\nstatistic = 0.69072, observed rank = 1, p-value = 0.001\nalternative hypothesis: greater\n\n\nBased on the result, the p-value is below the alpha value of 0.05 and therefore we can statistical evidence to reject the null hypothesis.\nVisualising the Monte Carlo Geary’s C\n\nmonte_carlo_geary <- as.data.frame(gperm[7])\n\nggplot(monte_carlo_geary, aes(x=res)) + \n  geom_density(fill=\"lightblue\") +\n  geom_vline(aes(xintercept=0.69072),\n            color=\"blue\", linetype=\"dashed\", size=1) +\n  labs(title = \"Density plot of Monte Carlo Simulation of Geary’s C\", x = \"Test Statistic\", y = \"Density\") +\n  theme_minimal() \n\n\n\n\nUnlike the Moran’s I where the statistical value is located on the right side of the density graph, the test statistic for Geary’s C is inversely related to Moran’s I where the value less than 1 indicates positive spatial autocorrelation, while a value larger than 1 points to negative spatial autocorrelation. Therefore, based on the plot, we can conclude there is significant evidence of positive autocorrelation."
  },
  {
    "objectID": "posts/Geo/Geospatial_Autocorrelation/index.html#spatial-correlogram",
    "href": "posts/Geo/Geospatial_Autocorrelation/index.html#spatial-correlogram",
    "title": "Introduction to Global and Local Measures of Spatial Autocorrelation",
    "section": "Spatial Correlogram",
    "text": "Spatial Correlogram\nA nonparametric spatial correlogram is another measure of overall spatial autocorrelation that does not rely on specifying a matrix of spatial weights. Instead, a local regression is fitted to the calculated covariances or correlations for all pairs of observations based on the distance between them. They show how correlated are pairs of spatial observations when you increase the distance (lag) between them - they are plots of some index of autocorrelation (Moran’s I or Geary’s c) against distance.Although correlograms are not as fundamental as variograms (a keystone concept of geostatistics), they are very useful as an exploratory and descriptive tool. For this purpose they actually provide richer information than variograms.\nCompute Moran’s I and Geary’C correlogram\nWe will utilise the sp.correlogram() function from the spdep package and compute a 6-lag spatial correlogram of GDPPC. We then use the plot() and print() function to visualise the output. The method function can take in three different inputs:\n\ncorr - correlation\nI - Moran’s I\nC - Geary’s C\n\nWe will illustrate the Moran’s I correlogram.\n\nMI_corr <- sp.correlogram(wm_q, \n                          hunan$GDPPC, \n                          order=6, \n                          method=\"I\", \n                          style=\"W\")\nplot(MI_corr)\n\n\n\nprint(MI_corr)\n\nSpatial correlogram for hunan$GDPPC \nmethod: Moran's I\n         estimate expectation   variance standard deviate Pr(I) two sided    \n1 (88)  0.3007500  -0.0114943  0.0043484           4.7351       2.189e-06 ***\n2 (88)  0.2060084  -0.0114943  0.0020962           4.7505       2.029e-06 ***\n3 (88)  0.0668273  -0.0114943  0.0014602           2.0496        0.040400 *  \n4 (88)  0.0299470  -0.0114943  0.0011717           1.2107        0.226015    \n5 (88) -0.1530471  -0.0114943  0.0012440          -4.0134       5.984e-05 ***\n6 (88) -0.1187070  -0.0114943  0.0016791          -2.6164        0.008886 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nNow, we will illustrate the Geary’C correlogram.\n\nGC_corr <- sp.correlogram(wm_q, \n                          hunan$GDPPC, \n                          order=6, \n                          method=\"C\", \n                          style=\"W\")\nplot(GC_corr)\n\n\n\nprint(GC_corr)\n\nSpatial correlogram for hunan$GDPPC \nmethod: Geary's C\n        estimate expectation  variance standard deviate Pr(I) two sided    \n1 (88) 0.6907223   1.0000000 0.0073364          -3.6108       0.0003052 ***\n2 (88) 0.7630197   1.0000000 0.0049126          -3.3811       0.0007220 ***\n3 (88) 0.9397299   1.0000000 0.0049005          -0.8610       0.3892612    \n4 (88) 1.0098462   1.0000000 0.0039631           0.1564       0.8757128    \n5 (88) 1.2008204   1.0000000 0.0035568           3.3673       0.0007592 ***\n6 (88) 1.0773386   1.0000000 0.0058042           1.0151       0.3100407    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nInterpretation of results\nFrom the results and plot shown above, we understand the relationship between Moran’s I and Geary’C (inverse). We also can identify lag 1 and 2 with statistical evidence to have a positive autocorrelation and lag 5 and 6 to have a negative correlation with p-values below 0.05."
  },
  {
    "objectID": "posts/Geo/Geospatial_Autocorrelation/index.html#cluster-and-outlier-analysis",
    "href": "posts/Geo/Geospatial_Autocorrelation/index.html#cluster-and-outlier-analysis",
    "title": "Introduction to Global and Local Measures of Spatial Autocorrelation",
    "section": "Cluster and Outlier Analysis",
    "text": "Cluster and Outlier Analysis\nLocal Indicators of Spatial Association (LISA) is a technique that allows analysts to identify areas on the map where data values are strongly positively or negatively correlated. We will now use techniques to detect clusters and/or outliers.\nComputing local Moran’s I\nTo compute local Moran’s I, the localmoran() function of spdep will be used. It computes Ii values, given a set of zi values and a listw object providing neighbour weighting information for the polygon associated with the zi values.\n\nfips <- order(hunan$County)\nlocalMI <- localmoran(hunan$GDPPC, rswm_q)\nhead(localMI)\n\n            Ii          E.Ii       Var.Ii        Z.Ii Pr(z != E(Ii))\n1 -0.001468468 -2.815006e-05 4.723841e-04 -0.06626904      0.9471636\n2  0.025878173 -6.061953e-04 1.016664e-02  0.26266425      0.7928094\n3 -0.011987646 -5.366648e-03 1.133362e-01 -0.01966705      0.9843090\n4  0.001022468 -2.404783e-07 5.105969e-06  0.45259801      0.6508382\n5  0.014814881 -6.829362e-05 1.449949e-03  0.39085814      0.6959021\n6 -0.038793829 -3.860263e-04 6.475559e-03 -0.47728835      0.6331568\n\n\nlocalmoran() function returns a matrix of values whose columns are:\n\nIi: the local Moran’s I statistics\nE.Ii: the expectation of local moran statistic under the randomisation hypothesis\nVar.Ii: the variance of local moran statistic under the randomisation hypothesis\nZ.Ii:the standard deviate of local moran statistic\nPr(): the p-value of local moran statistic\n\nThe z-scores and pseudo p-values represent the statistical significance of the computed index values.\nThe code chunk below list the content of the local Moran matrix derived by using printCoefmat().\n\nprintCoefmat(data.frame(localMI[fips,], row.names=hunan$County[fips]), check.names=FALSE)\n\n                       Ii        E.Ii      Var.Ii        Z.Ii Pr.z....E.Ii..\nAnhua         -2.2493e-02 -5.0048e-03  5.8235e-02 -7.2467e-02         0.9422\nAnren         -3.9932e-01 -7.0111e-03  7.0348e-02 -1.4791e+00         0.1391\nAnxiang       -1.4685e-03 -2.8150e-05  4.7238e-04 -6.6269e-02         0.9472\nBaojing        3.4737e-01 -5.0089e-03  8.3636e-02  1.2185e+00         0.2230\nChaling        2.0559e-02 -9.6812e-04  2.7711e-02  1.2932e-01         0.8971\nChangning     -2.9868e-05 -9.0010e-09  1.5105e-07 -7.6828e-02         0.9388\nChangsha       4.9022e+00 -2.1348e-01  2.3194e+00  3.3590e+00         0.0008\nChengbu        7.3725e-01 -1.0534e-02  2.2132e-01  1.5895e+00         0.1119\nChenxi         1.4544e-01 -2.8156e-03  4.7116e-02  6.8299e-01         0.4946\nCili           7.3176e-02 -1.6747e-03  4.7902e-02  3.4200e-01         0.7324\nDao            2.1420e-01 -2.0824e-03  4.4123e-02  1.0297e+00         0.3032\nDongan         1.5210e-01 -6.3485e-04  1.3471e-02  1.3159e+00         0.1882\nDongkou        5.2918e-01 -6.4461e-03  1.0748e-01  1.6338e+00         0.1023\nFenghuang      1.8013e-01 -6.2832e-03  1.3257e-01  5.1198e-01         0.6087\nGuidong       -5.9160e-01 -1.3086e-02  3.7003e-01 -9.5104e-01         0.3416\nGuiyang        1.8240e-01 -3.6908e-03  3.2610e-02  1.0305e+00         0.3028\nGuzhang        2.8466e-01 -8.5054e-03  1.4152e-01  7.7931e-01         0.4358\nHanshou        2.5878e-02 -6.0620e-04  1.0167e-02  2.6266e-01         0.7928\nHengdong       9.9964e-03 -4.9063e-04  6.7742e-03  1.2742e-01         0.8986\nHengnan        2.8064e-02 -3.2160e-04  3.7597e-03  4.6294e-01         0.6434\nHengshan      -5.8201e-03 -3.0437e-05  5.1076e-04 -2.5618e-01         0.7978\nHengyang       6.2997e-02 -1.3046e-03  2.1865e-02  4.3486e-01         0.6637\nHongjiang      1.8790e-01 -2.3019e-03  3.1725e-02  1.0678e+00         0.2856\nHuarong       -1.5389e-02 -1.8667e-03  8.1030e-02 -4.7503e-02         0.9621\nHuayuan        8.3772e-02 -8.5569e-04  2.4495e-02  5.4072e-01         0.5887\nHuitong        2.5997e-01 -5.2447e-03  1.1077e-01  7.9685e-01         0.4255\nJiahe         -1.2431e-01 -3.0550e-03  5.1111e-02 -5.3633e-01         0.5917\nJianghua       2.8651e-01 -3.8280e-03  8.0968e-02  1.0204e+00         0.3076\nJiangyong      2.4337e-01 -2.7082e-03  1.1746e-01  7.1800e-01         0.4728\nJingzhou       1.8270e-01 -8.5106e-04  2.4363e-02  1.1759e+00         0.2396\nJinshi        -1.1988e-02 -5.3666e-03  1.1334e-01 -1.9667e-02         0.9843\nJishou        -2.8680e-01 -2.6305e-03  4.4028e-02 -1.3543e+00         0.1756\nLanshan        6.3334e-02 -9.6365e-04  2.0441e-02  4.4972e-01         0.6529\nLeiyang        1.1581e-02 -1.4948e-04  2.5082e-03  2.3422e-01         0.8148\nLengshuijiang -1.7903e+00 -8.2129e-02  2.1598e+00 -1.1623e+00         0.2451\nLi             1.0225e-03 -2.4048e-07  5.1060e-06  4.5260e-01         0.6508\nLianyuan      -1.4672e-01 -1.8983e-03  1.9145e-02 -1.0467e+00         0.2952\nLiling         1.3774e+00 -1.5097e-02  4.2601e-01  2.1335e+00         0.0329\nLinli          1.4815e-02 -6.8294e-05  1.4499e-03  3.9086e-01         0.6959\nLinwu         -2.4621e-03 -9.0703e-06  1.9258e-04 -1.7676e-01         0.8597\nLinxiang       6.5904e-02 -2.9028e-03  2.5470e-01  1.3634e-01         0.8916\nLiuyang        3.3688e+00 -7.7502e-02  1.5180e+00  2.7972e+00         0.0052\nLonghui        8.0801e-01 -1.1377e-02  1.5538e-01  2.0787e+00         0.0376\nLongshan       7.5663e-01 -1.1100e-02  3.1449e-01  1.3690e+00         0.1710\nLuxi           1.8177e-01 -2.4855e-03  3.4249e-02  9.9561e-01         0.3194\nMayang         2.1852e-01 -5.8773e-03  9.8049e-02  7.1663e-01         0.4736\nMiluo          1.8704e+00 -1.6927e-02  2.7925e-01  3.5715e+00         0.0004\nNan           -9.5789e-03 -4.9497e-04  6.8341e-03 -1.0988e-01         0.9125\nNingxiang      1.5607e+00 -7.3878e-02  8.0012e-01  1.8274e+00         0.0676\nNingyuan       2.0910e-01 -7.0884e-03  8.2306e-02  7.5356e-01         0.4511\nPingjiang     -9.8964e-01 -2.6457e-03  5.6027e-02 -4.1698e+00         0.0000\nQidong         1.1806e-01 -2.1207e-03  2.4747e-02  7.6396e-01         0.4449\nQiyang         6.1966e-02 -7.3374e-04  8.5743e-03  6.7712e-01         0.4983\nRucheng       -3.6992e-01 -8.8999e-03  2.5272e-01 -7.1814e-01         0.4727\nSangzhi        2.5053e-01 -4.9470e-03  6.8000e-02  9.7972e-01         0.3272\nShaodong      -3.2659e-02 -3.6592e-05  5.0546e-04 -1.4510e+00         0.1468\nShaoshan       2.1223e+00 -5.0227e-02  1.3668e+00  1.8583e+00         0.0631\nShaoyang       5.9499e-01 -1.1253e-02  1.3012e-01  1.6807e+00         0.0928\nShimen        -3.8794e-02 -3.8603e-04  6.4756e-03 -4.7729e-01         0.6332\nShuangfeng     9.2835e-03 -2.2867e-03  3.1516e-02  6.5174e-02         0.9480\nShuangpai      8.0591e-02 -3.1366e-04  8.9838e-03  8.5358e-01         0.3933\nSuining        3.7585e-01 -3.5933e-03  4.1870e-02  1.8544e+00         0.0637\nTaojiang      -2.5394e-01 -1.2395e-03  1.4477e-02 -2.1002e+00         0.0357\nTaoyuan        1.4729e-02 -1.2039e-04  8.5103e-04  5.0903e-01         0.6107\nTongdao        4.6482e-01 -6.9870e-03  1.9879e-01  1.0582e+00         0.2900\nWangcheng      4.4220e+00 -1.1067e-01  1.3596e+00  3.8873e+00         0.0001\nWugang         7.1003e-01 -7.8144e-03  1.0710e-01  2.1935e+00         0.0283\nXiangtan       2.4530e-01 -3.6457e-04  3.2319e-03  4.3213e+00         0.0000\nXiangxiang     2.6271e-01 -1.2703e-03  2.1290e-02  1.8092e+00         0.0704\nXiangyin       5.4525e-01 -4.7442e-03  7.9236e-02  1.9539e+00         0.0507\nXinhua         1.1810e-01 -6.2649e-03  8.6001e-02  4.2409e-01         0.6715\nXinhuang       1.5725e-01 -4.1820e-03  3.6648e-01  2.6667e-01         0.7897\nXinning        6.8928e-01 -9.6674e-03  2.0328e-01  1.5502e+00         0.1211\nXinshao        5.7578e-02 -8.5932e-03  1.1769e-01  1.9289e-01         0.8470\nXintian       -7.4050e-03 -5.1493e-03  1.0877e-01 -6.8395e-03         0.9945\nXupu           3.2406e-01 -5.7468e-03  5.7735e-02  1.3726e+00         0.1699\nYanling       -6.9021e-02 -5.9211e-04  9.9306e-03 -6.8667e-01         0.4923\nYizhang       -2.6844e-01 -2.2463e-03  4.7588e-02 -1.2202e+00         0.2224\nYongshun       6.3064e-01 -1.1350e-02  1.8830e-01  1.4795e+00         0.1390\nYongxing       4.3411e-01 -9.0735e-03  1.5088e-01  1.1409e+00         0.2539\nYou            7.8750e-02 -7.2728e-03  1.2116e-01  2.4714e-01         0.8048\nYuanjiang      2.0004e-04 -1.7760e-04  2.9798e-03  6.9181e-03         0.9945\nYuanling       8.7298e-03 -2.2981e-06  2.3221e-05  1.8121e+00         0.0700\nYueyang        4.1189e-02 -1.9768e-04  2.3113e-03  8.6085e-01         0.3893\nZhijiang       1.0476e-01 -7.8123e-04  1.3100e-02  9.2214e-01         0.3565\nZhongfang     -2.2685e-01 -2.1455e-03  3.5927e-02 -1.1855e+00         0.2358\nZhuzhou        3.2864e-01 -5.2432e-04  7.2391e-03  3.8688e+00         0.0001\nZixing        -7.6849e-01 -8.8210e-02  9.4057e-01 -7.0144e-01         0.4830\n\n\nMapping the local Moran’s I\nWe have to combine the local Moran’s dataframe with the our exisiting Hunan spatialdataframe before plotting. We will use the cbind() function.\n\nhunan.localMI <- cbind(hunan,localMI) %>%\n  rename(Pr.Ii = Pr.z....E.Ii..)\n\nMapping local Moran’s I values\nUsing choropleth mapping functions of tmap package, we can plot the local Moran’s I values by using the code chinks below.\n\ntm_shape(hunan.localMI) +\n  tm_fill(col = \"Ii\", \n          style = \"pretty\",\n          palette = \"RdBu\",\n          title = \"Local Moran I value\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\nA positive value for I indicates that a feature has neighboring features with similarly high or low attribute values; this feature is part of a cluster. A negative value for I indicates that a feature has neighboring features with dissimilar values; this feature is an outlier. The choropleth shows there is evidence for both positive and negative Ii values. However, it is useful to consider the p-values for each of these values, as consider above.\nMapping local Moran’s I p-values\nThe code chunks below produce a choropleth map of Moran’s I p-values by using functions of tmap package.\n\ntm_shape(hunan.localMI) +\n  tm_fill(col = \"Pr.Ii\", \n          breaks=c(-Inf, 0.001, 0.01, 0.05, 0.1, Inf),\n          palette=\"-Blues\", \n          title = \"Local Moran's I p-values\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\nMapping Moran’s I values and p-values\nIt will be more useful to understand which area Moran’s I value is statistically significant. We will create another dataframe with only areas that are statistically significant with a p value < 0.05. We will then plot the overlay above the base map and identify the area with positive and negative I value.\n\nhunan.localMI.sub <- hunan.localMI %>%\n  filter (Pr.Ii <= 0.05)\n\nimap <- tm_shape(hunan.localMI) +\n  tm_fill(\"white\") +\n  tm_borders(\"grey\", lwd = 0.5, alpha = 0.5) +\n  tm_shape(hunan.localMI.sub) +\n  tm_fill (col = \"Ii\",\n           style = \"pretty\",\n           palette = \"RdBu\",\n           title = \"Local Moran I value\") +\n  tm_borders(\"grey\", lwd = 0.5, alpha = 0.5) +\n  tm_layout(main.title = \"Local Moran I value for p < 0.05\",\n            main.title.size = 0.7,\n            main.title.fontface = \"bold\")\n\nimap \n\n\n\n\nFrom our plot, we are able to derive that 11 areas have I values that are statically significant, and 2 areas have negative I value which means a dissimilar features."
  },
  {
    "objectID": "posts/Geo/Geospatial_Autocorrelation/index.html#creating-a-lisa-cluster-map",
    "href": "posts/Geo/Geospatial_Autocorrelation/index.html#creating-a-lisa-cluster-map",
    "title": "Introduction to Global and Local Measures of Spatial Autocorrelation",
    "section": "Creating a LISA Cluster Map",
    "text": "Creating a LISA Cluster Map\nThe LISA Cluster Map shows the significant locations color coded by type of spatial autocorrelation. The first step before we can generate the LISA cluster map is to plot the Moran scatterplot.\nPlotting Moran scatterplot\nThe Moran scatterplot is an illustration of the relationship between the values of the chosen attribute at each location and the average value of the same attribute at neighboring locations.\nThe code chunk below plots the Moran scatterplot of GDPPC 2012 by using moran.plot() of spdep.\n\nnci <- moran.plot(hunan$GDPPC, rswm_q,\n                  labels=as.character(hunan$County), \n                  xlab=\"GDPPC 2012\", \n                  ylab=\"Spatially Lag GDPPC 2012\")\n\n\n\n\nIn the upper right quadrant, there are cases where both the attribute value and the local average are greater than the global average. Similarly, in the lower left quadrant, there are cases where both the attribute value and the local mean are below the global mean. These conditions confirm positive spatial autocorrelation. Cases in the other two quadrants show negative spatial autocorrelation.\nPreparing LISA map classes\nWe will now prepare the LISA map classes. We first need to retrieve the quadrant for each area.\n\nquadrant <- vector(mode=\"numeric\",length=nrow(localMI))\n\nNext, we scale the GDPPC.\n\nDV <- scale(hunan.localMI$GDPPC)   \n\nThis is follow by finding the lag of the scaled GDPPC.\n\nC_mI <- lag.listw(rswm_q, DV)   \n\nUsing the Moran Scatterplot below, we filter all the area with p value < 0.05 and identify significant areas. We can see that the plot below is align with our Moran I plot where there are a total of 11 significant areas, 2 areas that are outliers (LH), and 9 areas that are clusters (7 HH and 2 LL).\n\nMIplot <- data.frame(cbind(DV,C_mI,localMI[,5]))\nMIplot <- MIplot %>%\n  filter (X3 < 0.05)\nplot(x = MIplot$X1, y = MIplot$X2, main = \"Moran Scatterplot PPOV\", xlab = \"scaled GDDPC\", ylab = \"Lag scaled GDPPC\")\nabline(h = 0, v = 0)\n\n\n\n\nWe will now then define the quadrant based on the following criteria and place non-significant Moran (p value < 0.05) in the category 0.:\n\nsignif <- 0.05 \nquadrant[DV >0 & C_mI>0] <- 4      \nquadrant[DV <0 & C_mI<0] <- 1      \nquadrant[DV <0 & C_mI>0] <- 2\nquadrant[DV >0 & C_mI<0] <- 3\nquadrant[localMI[,5]>signif] <- 0\n\nPlotting LISA map\nOnce the quadrant of each area has been decided, we will now plot the LISA map using tmap. We will plot both the base map with the GDDPC distribution and the LISA map to better understand the relationship.\n\ntmap_mode(\"plot\")\nhunan.localMI$quadrant <- quadrant\ncolors <- c(\"#ffffff\", \"#2c7bb6\", \"#abd9e9\", \"#fdae61\", \"#d7191c\")\nclusters <- c(\"insignificant\", \"low-low\", \"low-high\", \"high-low\", \"high-high\")\n\nlisamap <- tm_shape(hunan.localMI) +\n  tm_fill(col = \"quadrant\", \n          style = \"cat\", \n          palette = colors[c(sort(unique(quadrant)))+1], \n          labels = clusters[c(sort(unique(quadrant)))+1]) +\n  tm_view(set.zoom.limits = c(11,17)) +\n  tm_borders(alpha=0.5) +\n  tm_layout(main.title = \"LISA Map with Quadrant\", main.title.size = 0.7,\n            main.title.fontface = \"bold\", main.title.position = \"center\")\n\nbasemap <- tm_shape(hunan) +\n  tm_fill(\"GDPPC\",\n          n = 5,\n          style = \"quantile\") +\n  tm_borders(alpha = 0.5) +\n  tm_layout(main.title = \"GDDPC using Quantile classification\", main.title.size = 0.7,\n            main.title.fontface = \"bold\", main.title.position = \"center\")\n\ntmap_arrange (imap,lisamap,basemap)\n\n\n\n\nBased on the map plot above, we can see that the Moran I value map provide us with insights on which areas are consider outliers or clusters. The LISA map provide us with more in-depth information on whether the outliers are HL or LH and whether the clusters are HH or LL. These attributes can further be confirmed by referencing the base map on the right."
  },
  {
    "objectID": "posts/Geo/Geospatial_Autocorrelation/index.html#hot-spot-and-cold-spot-area-analysis",
    "href": "posts/Geo/Geospatial_Autocorrelation/index.html#hot-spot-and-cold-spot-area-analysis",
    "title": "Introduction to Global and Local Measures of Spatial Autocorrelation",
    "section": "Hot Spot and Cold Spot Area Analysis",
    "text": "Hot Spot and Cold Spot Area Analysis\nBy grouping points of occurrence into polygons or converging points that are close to one another based on a calculated distance, Hotspot Analysis uses vectors to locate statistically significant hot spots and cold spots in your data.\nGetis and Ord’s G-Statistics\nThe Getis and Ord’s G-statistics is used to measure the degree of clustering for either high or low values. The High/Low Clustering (Getis-Ord General G) statistic is an inferential statistic, which means that the null hypothesis is used to interpret the analysis’s findings. It is assumed that there is no spatial clustering of feature values when using the High/Low Clustering (General G) statistic.\nThe analysis consists of three steps:\n\nDeriving spatial weight matrix\nComputing Gi statistics\nMapping Gi statistics\nDeriving distance-based weight matrix\nFirst, we need to define a new set of neighbours. Whist the spatial autocorrelation considered units which shared borders, for Getis-Ord we are defining neighbours based on distance.\nThere are two type of distance-based proximity matrix, they are:\n\nfixed distance weight matrix\nadaptive distance weight matrix\n\nThese methods were explained on the previous post and therefore will not be elaborated here.\nThe code chunk below is to generate the fixed distance weight matrix:\n\ncoords <- st_centroid(st_geometry(hunan))\nk1 <- knn2nb(knearneigh(coords))\nk1dists <- unlist(nbdists(k1, coords))\nwm_d59 <- dnearneigh(coords, 0, 59)\nwm59_lw <- nb2listw(wm_d59, style = 'W')\n\nThe output spatial weights object is called wm59_lw.\nThe code chunk below is to generate the adaptive distance weight matrix:\n\nknn <- knn2nb(knearneigh(coords, k=8))\nknn_lw <- nb2listw(knn, style = 'B')"
  },
  {
    "objectID": "posts/Geo/Geospatial_Autocorrelation/index.html#computing-gi-statistics",
    "href": "posts/Geo/Geospatial_Autocorrelation/index.html#computing-gi-statistics",
    "title": "Introduction to Global and Local Measures of Spatial Autocorrelation",
    "section": "Computing Gi statistics",
    "text": "Computing Gi statistics"
  },
  {
    "objectID": "posts/Geo/Geospatial_Exercise/index.html",
    "href": "posts/Geo/Geospatial_Exercise/index.html",
    "title": "Geospatial Analytics for Social Good - Understanding Nigeria Water functional and non-functional water point rate",
    "section": "",
    "text": "The Spatial Autocorrelation measures spatial autocorrelation based on feature locations and feature values simultaneously. Given a set of features and an associated attribute, it evaluates whether the expressed pattern is clustered, scattered, or random. The tool calculates the Moran’s I index value as well as a z-score and p-value to assess the significance of this index. P-values are numerical approximations of the area under the curve for a known distribution, bounded by the test statistic.\nIn this study we will explore the computation of Global and Local Measure of Spatial Autocorrelation (GLSA) by using spdep package."
  },
  {
    "objectID": "posts/Geo/Geospatial_Exercise/index.html#libraries",
    "href": "posts/Geo/Geospatial_Exercise/index.html#libraries",
    "title": "Geospatial Analytics for Social Good - Understanding Nigeria Water functional and non-functional water point rate",
    "section": "Libraries",
    "text": "Libraries\nFor this study, we will use the following packages from CRAN.\n\n\nsf - Support for simple features, a standardized way to encode spatial vector data. Binds to ‘GDAL’ for reading and writing data, to ‘GEOS’ for geometrical operations, and to ‘PROJ’ for projection conversions and datum transformations. Uses by default the ‘s2’ package for spherical geometry operations on ellipsoidal (long/lat) coordinates.\n\ntidyverse - Loading the core tidyverse packages which will be used for data wrangling and visualisation.\n\ntmap - Thematic maps are geographical maps in which spatial data distributions are visualized. This package offers a flexible, layer-based, and easy to use approach to create thematic maps, such as choropleths and bubble maps.\n\nspdep - A collection of functions to create spatial weights matrix objects from polygon ‘contiguities’, from point patterns by distance and tessellations, for summarizing these objects, and for permitting their use in spatial data analysis, including regional aggregation by minimum spanning tree; a collection of tests for spatial ‘autocorrelation’\n\n\npacman::p_load(sf, spdep, tmap, tidyverse)"
  },
  {
    "objectID": "posts/Geo/Geospatial_Exercise/index.html#data-preparation",
    "href": "posts/Geo/Geospatial_Exercise/index.html#data-preparation",
    "title": "Geospatial Analytics for Social Good - Understanding Nigeria Water functional and non-functional water point rate",
    "section": "Data Preparation",
    "text": "Data Preparation\nTwo dataset will be used for this study:\n\nNigeria.shp: A shapefile of Nigeria that consist of all the Level-2 Administrative Boundary (also known as Local Government Area)\nNigeriaAttribute.csv: A csv file containing multiple attributes of each Level-2 Administrative Boundary\n\nImporting of data\nWe will use the st_read to import the shape file and read_csv to import the aspatial data into the R environment. We will then use a relational join left_join to combine the spatial and aspatial data together.\n\nnigeria <- st_read(dsn = \"data\", \n                 layer = \"geoBoundaries-NGA-ADM2\")\n\nnigeriaA <- read_csv(\"data/nigeriaattribute.csv\")\n\n\nnigeriaT <- nigeriaA %>%\n  rename (\"Country\" = \"#clean_country_name\",\n          \"clean_adm2\" = \"#clean_adm2\",\n          \"status\" = \"#status_clean\",\n          \"lat\" = \"#lat_deg\",\n          \"long\" = \"#lon_deg\") %>%\n  filter (Country == \"Nigeria\" & !is.na(status)) %>%\n  select (clean_adm2,status,lat,long)\n\nnigeria <- nigeria %>%\n  st_transform(crs = 26391)\n\nnigeriaT_sf <- st_as_sf(nigeriaT, coords = c(\"long\", \"lat\"),  crs = 4326)\nnigeriaT_sf <- st_transform(nigeriaT_sf, crs = 26391)\n\nst_crs (nigeria)\nst_crs (nigeriaT_sf)\n\n\nnigeriaT_sf$status <- gsub(\"([A-Za-z]+).*\", \"\\\\1\", nigeriaT_sf$status)\nunique(nigeriaT_sf$status)\n\n\nnigeria$functional <- lengths(st_intersects(nigeria, nigeriaT_sf[nigeriaT_sf$status == \"Functional\",]))\nnigeria$nonfunctional <- lengths(st_intersects(nigeria, nigeriaT_sf[nigeriaT_sf$status != \"Functional\",]))\n\n\n\n\n\nf <- qtm(nigeria, \"functional\")\nnf <- qtm(nigeria, \"nonfunctional\")\n\ntmap_arrange(f,nf,ncol = 2)\n\n\n\n\nComputing Contiguity Spatial Weights\nFor this study, we will be using the Queen contiguity weight matrix. The code chunk below will construct the weight matrix and subsequently implement the row-standardised weight matrix using the nb2listw() function.\n\nwm_q <- poly2nb(nigeria, \n                queen=TRUE)\n\nset.ZeroPolicyOption(TRUE)\n\n[1] FALSE\n\nrswm_q <- nb2listw(wm_q, \n                   style=\"W\", \n                   zero.policy = TRUE)\nrswm_q\n\nCharacteristics of weights list object:\nNeighbour list object:\nNumber of regions: 774 \nNumber of nonzero links: 4440 \nPercentage nonzero weights: 0.7411414 \nAverage number of links: 5.736434 \n1 region with no links:\n86\n\nWeights style: W \nWeights constants summary:\n    n     nn  S0       S1       S2\nW 773 597529 773 285.0658 3198.414\n\n\nGlobal Spatial Autocorrelation: Moran’s I\nWe will now perform Moran’s I statistics testing by using the moran.test() from spdep.\n\nmoran.test(nigeria$nonfunctional, \n           listw=rswm_q, \n           zero.policy = TRUE, \n           na.action=na.omit)\n\n\n    Moran I test under randomisation\n\ndata:  nigeria$nonfunctional  \nweights: rswm_q  n reduced by no-neighbour observations\n  \n\nMoran I statistic standard deviate = 20.043, p-value < 2.2e-16\nalternative hypothesis: greater\nsample estimates:\nMoran I statistic       Expectation          Variance \n      0.433932927      -0.001295337       0.000471516 \n\n\nComputing Monte Carlo Moran’s I\nThe code chunk below performs permutation test for Moran’s I statistic by using moran.mc() of spdep. A total of 1000 simulation will be performed.\n\nset.seed(1234)\nbperm= moran.mc(nigeria$nonfunctional, \n                listw=rswm_q, \n                nsim=999, \n                zero.policy = TRUE, \n                na.action=na.omit)\nbperm\n\n\n    Monte-Carlo simulation of Moran I\n\ndata:  nigeria$nonfunctional \nweights: rswm_q  \nnumber of simulations + 1: 1000 \n\nstatistic = 0.43393, observed rank = 1000, p-value = 0.001\nalternative hypothesis: greater\n\n\nVisualising Monte Carlo Moran’s I\nWe will use a density plot to visualise the output of the Monte Carlo Moran’s I. First, we need to extract the res value and convert it into a dataframe. We then visualise the test statistic result using geom_density from the ggplot package.\n\nmonte_carlo <- as.data.frame(bperm[7])\n\nggplot(monte_carlo, aes(x=res)) + \n  geom_density(fill=\"lightblue\") +\n  geom_vline(aes(xintercept=0.43393),\n            color=\"blue\", linetype=\"dashed\", size=1) +\n  labs(title = \"Density plot of Monte Carlo Simulation of Moran's I\", x = \"Test Statistic\", y = \"Density\") +\n  theme_minimal()"
  },
  {
    "objectID": "posts/Geo/Geospatial_Exercise/index.html#spatial-correlogram",
    "href": "posts/Geo/Geospatial_Exercise/index.html#spatial-correlogram",
    "title": "Geospatial Analytics for Social Good - Understanding Nigeria Water functional and non-functional water point rate",
    "section": "Spatial Correlogram",
    "text": "Spatial Correlogram\nA nonparametric spatial correlogram is another measure of overall spatial autocorrelation that does not rely on specifying a matrix of spatial weights. Instead, a local regression is fitted to the calculated covariances or correlations for all pairs of observations based on the distance between them. They show how correlated are pairs of spatial observations when you increase the distance (lag) between them - they are plots of some index of autocorrelation (Moran’s I or Geary’s c) against distance.Although correlograms are not as fundamental as variograms (a keystone concept of geostatistics), they are very useful as an exploratory and descriptive tool. For this purpose they actually provide richer information than variograms.\nCompute Moran’s I correlogram\nWe will utilise the sp.correlogram() function from the spdep package and compute a 6-lag spatial correlogram of GDPPC. We then use the plot() and print() function to visualise the output.\nWe will illustrate the Moran’s I correlogram.\n\nMI_corr <- sp.correlogram(wm_q, \n                          nigeria$nonfunctional, \n                          order=6, \n                          method=\"I\", \n                          style=\"W\")\nplot(MI_corr)\n\n\n\nprint(MI_corr)\n\nSpatial correlogram for nigeria$nonfunctional \nmethod: Moran's I\n           estimate expectation    variance standard deviate Pr(I) two sided\n1 (773)  4.3393e-01 -1.2953e-03  4.7152e-04          20.0433       < 2.2e-16\n2 (773)  2.6647e-01 -1.2953e-03  2.0206e-04          18.8374       < 2.2e-16\n3 (773)  1.9507e-01 -1.2953e-03  1.2189e-04          17.7863       < 2.2e-16\n4 (773)  1.4019e-01 -1.2953e-03  8.7589e-05          15.1181       < 2.2e-16\n5 (773)  6.3735e-02 -1.2953e-03  6.8779e-05           7.8413       4.459e-15\n6 (773)  2.1698e-02 -1.2953e-03  5.7380e-05           3.0354        0.002402\n           \n1 (773) ***\n2 (773) ***\n3 (773) ***\n4 (773) ***\n5 (773) ***\n6 (773) ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nComputing local Moran’s I\nTo compute local Moran’s I, the localmoran() function of spdep will be used. It computes Ii values, given a set of zi values and a listw object providing neighbour weighting information for the polygon associated with the zi values.\n\nfips <- order(nigeria$shapeName)\nlocalMI <- localmoran(nigeria$nonfunctional, rswm_q)\nhead(localMI)\n\n           Ii          E.Ii       Var.Ii        Z.Ii Pr(z != E(Ii))\n1 -0.32365786 -9.995243e-04 1.924638e-01 -0.73547576     0.46204980\n2  0.07000542 -4.092463e-05 1.053077e-02  0.68258288     0.49487045\n3  1.25819985 -1.627684e-03 4.181728e-01  1.94819847     0.05139122\n4 -0.03537489 -5.427505e-05 5.954304e-03 -0.45773361     0.64714384\n5  0.01201533 -2.590965e-04 3.988998e-02  0.06145673     0.95099547\n6  0.00768085 -1.538445e-07 1.687859e-05  1.86960486     0.06153871\n\n\nMapping the local Moran’s I\nWe have to combine the local Moran’s dataframe with the our exisiting Nigeria spatialdataframe before plotting. We will use the cbind() function.\n\nnigeria.localMI <- cbind(nigeria,localMI) %>%\n  rename(Pr.Ii = Pr.z....E.Ii..)\n\nMapping local Moran’s I values\nUsing choropleth mapping functions of tmap package, we can plot the local Moran’s I values by using the code chinks below.\n\ntm_shape(nigeria.localMI) +\n  tm_fill(col = \"Ii\", \n          style = \"pretty\",\n          palette = \"RdBu\",\n          title = \"Local Moran I value\") +\n  tm_borders(alpha = 0.5)\n\n\n\n\n\nnigeria.localMI.sub <- nigeria.localMI %>%\n  filter (Pr.Ii <= 0.05)\n\ntm_shape(nigeria.localMI) +\n  tm_fill(\"white\") +\n  tm_borders(\"grey\", lwd = 0.5, alpha = 0.5) +\n  tm_shape(nigeria.localMI.sub) +\n  tm_fill (col = \"Ii\",\n           style = \"pretty\",\n           palette = \"RdBu\",\n           title = \"Local Moran I value\") +\n  tm_borders(\"grey\", lwd = 0.5, alpha = 0.5) +\n  tm_layout(main.title = \"Local Moran I value for p < 0.05\",\n            main.title.size = 0.7,\n            main.title.fontface = \"bold\")"
  },
  {
    "objectID": "posts/Geo/Geospatial_Exercise/index.html#creating-a-lisa-cluster-map",
    "href": "posts/Geo/Geospatial_Exercise/index.html#creating-a-lisa-cluster-map",
    "title": "Geospatial Analytics for Social Good - Understanding Nigeria Water functional and non-functional water point rate",
    "section": "Creating a LISA Cluster Map",
    "text": "Creating a LISA Cluster Map\nThe LISA Cluster Map shows the significant locations color coded by type of spatial autocorrelation. The first step before we can generate the LISA cluster map is to plot the Moran scatterplot.\nPlotting Moran scatterplot\nThe Moran scatterplot is an illustration of the relationship between the values of the chosen attribute at each location and the average value of the same attribute at neighboring locations.\nThe code chunk below plots the Moran scatterplot of Non Functional Water Point by using moran.plot() of spdep.\n\nnci <- moran.plot(nigeria$nonfunctional, rswm_q,\n                  labels=as.character(nigeria$shapeName), \n                  xlab=\"Non Functional Water Point\", \n                  ylab=\"Spatially Lag Non Functional Water Point\")\n\n\n\n\nPreparing LISA map classes\nWe will now prepare the LISA map classes. We first need to retrieve the quadrant for each area. We will center the variable of interest and local Moran’s I around their mean. Subsequently, set a statistical significance level for our analysis and create the quadrant for categorising each area. Non-significant area with a p value of > 0.05 will not be categorised.\n\nquadrant <- vector(mode=\"numeric\",length=nrow(localMI))\nDV <- nigeria$nonfunctional - mean(nigeria$nonfunctional)\nC_mI <- localMI[,1] - mean(localMI[,1])\nsignif <- 0.05\nquadrant[DV >0 & C_mI>0] <- 4      \nquadrant[DV <0 & C_mI<0] <- 1      \nquadrant[DV <0 & C_mI>0] <- 2\nquadrant[DV >0 & C_mI<0] <- 3\nquadrant[localMI[,5]>signif] <- 0"
  }
]